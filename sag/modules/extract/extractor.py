"""
äº‹é¡¹æå–å™¨

ä¸»æ§åˆ¶å™¨ - åè°ƒå•ç¯‡æ–‡æ¡£çš„æå–æµç¨‹
"""

import asyncio
import uuid
from typing import Dict, List, Optional

from sqlalchemy import select
from sqlalchemy.orm import selectinload

from sag.core.ai.factory import get_embedding_client
from sag.core.ai.base import BaseLLMClient
from sag.core.prompt.manager import PromptManager
from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.entity_repository import EntityVectorRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import (
    SourceChunk,
    Entity,
    EntityType as DBEntityType,
    EventEntity,
    SourceEvent,
    get_session_factory,
)
from sag.exceptions import ExtractError
from sag.modules.extract.config import ExtractConfig
from sag.modules.extract.processor import EventProcessor
from sag.utils import estimate_tokens, get_logger

logger = get_logger("extract.extractor")


class EventExtractor:
    """äº‹é¡¹æå–å™¨ï¼ˆä¸»æ§åˆ¶å™¨ï¼‰"""

    def __init__(
        self,
        prompt_manager: PromptManager,
        model_config: Optional[Dict] = None,
    ):
        """
        åˆå§‹åŒ–äº‹é¡¹æå–å™¨

        Args:
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            model_config: LLMé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
                - å¦‚æœä¼ å…¥ï¼šä½¿ç”¨è¯¥é…ç½®
                - å¦‚æœä¸ä¼ ï¼šè‡ªåŠ¨ä»é…ç½®ç®¡ç†å™¨è·å– 'extract' åœºæ™¯é…ç½®
        """
        self.prompt_manager = prompt_manager
        self.model_config = model_config
        self._llm_client = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.session_factory = get_session_factory()
        self.logger = get_logger("extract.extractor")
        
        # ESç›¸å…³ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.es_client = None
        self.event_repo = None
        self.entity_repo = None
    
    async def _get_llm_client(self) -> BaseLLMClient:
        """è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._llm_client is None:
            from sag.core.ai.factory import create_llm_client
            
            self._llm_client = await create_llm_client(
                scenario='extract',
                model_config=self.model_config
            )
        
        return self._llm_client

    async def extract(self, config: ExtractConfig) -> List[SourceEvent]:
        """
        æå–äº‹é¡¹ï¼ˆç»Ÿä¸€å…¥å£ - æ–°æ¶æ„ï¼‰
        
        å·¥ä½œæµç¨‹ï¼š
        1. åŠ è½½æ‰€æœ‰chunks
        2. æŒ‰max_concurrencyå¹¶å‘å¤„ç†ï¼ˆSemaphoreæ§åˆ¶ï¼‰
        3. æ¯ä¸ªchunkç”±ä¸€ä¸ªExtractorAgentå¤„ç†
        4. åˆå¹¶æ‰€æœ‰ç»“æœ
        5. ä¿å­˜åˆ°æ•°æ®åº“ + Elasticsearch
        6. æ›´æ–°æºçŠ¶æ€ä¸ºå·²å®Œæˆ

        Args:
            config: æå–é…ç½®

        Returns:
            æ‰€æœ‰chunksæå–çš„äº‹é¡¹åˆ—è¡¨

        Example:
            config = ExtractConfig(
                source_config_id="source-uuid",
                chunk_ids=["chunk-1", "chunk-2", "chunk-3"],
                max_concurrency=3
            )
            events = await extractor.extract(config)
        """
        self.logger.info(
            f"å¼€å§‹æ‰¹é‡æå–: chunks={len(config.chunk_ids)}, "
            f"å¹¶å‘æ•°={config.max_concurrency}"
        )

        try:
            # 1. åŠ è½½æ‰€æœ‰chunks
            chunks = await self._load_chunks(config.chunk_ids)

            if not chunks:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„chunks")
                return []

            # 2. å¹¶å‘å¤„ç†chunksï¼ˆæ¯ä¸ªchunkä¸€ä¸ªAgentï¼‰
            all_events = await self._process_chunks_with_agents(chunks, config)
            
            self.logger.info(
                f"æ‰¹é‡æå–å®Œæˆ: chunks={len(chunks)}, events={len(all_events)}"
            )
            
            # 3. æŒ‰åŸæ–‡é¡ºåºé‡æ–°æ’åºå¹¶åˆ†é…å…¨å±€ rank
            if all_events:
                # åˆ›å»º chunk_id -> chunk.rank çš„æ˜ å°„
                chunk_rank_map = {chunk.id: chunk.rank for chunk in chunks}
                
                # æ’åºè§„åˆ™ï¼š
                # 1. å…ˆæŒ‰ chunk.rankï¼ˆä¿è¯ chunk ä¹‹é—´çš„é¡ºåºï¼‰
                # 2. å†æŒ‰äº‹é¡¹çš„æ—¶é—´ï¼ˆä¼šè¯ï¼‰æˆ– chunk å†… rankï¼ˆæ–‡æ¡£ï¼‰
                def sort_key(event):
                    chunk_order = chunk_rank_map.get(event.chunk_id, 9999)
                    
                    # ä¼šè¯ç±»å‹ï¼šæŒ‰æ—¶é—´æ’åº
                    if event.source_type == "CHAT" and event.start_time:
                        event_order = event.start_time
                    # æ–‡æ¡£ç±»å‹ï¼šæŒ‰ chunk å†… rank æ’åº
                    else:
                        event_order = event.rank or 0
                    
                    return (chunk_order, event_order)
                
                all_events.sort(key=sort_key)
                
                # é‡æ–°åˆ†é…å…¨å±€è¿ç»­ rank
                for i, event in enumerate(all_events):
                    event.rank = i
                
                self.logger.info(
                    f"äº‹é¡¹å·²æŒ‰åŸæ–‡é¡ºåºæ’åº: chunks={len(chunks)}, events={len(all_events)}"
                )
            
            # 4. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬ESï¼‰
            if all_events:
                await self._save_events(all_events, config)
                
                # 5. é‡æ–°ä»æ•°æ®åº“åŠ è½½äº‹é¡¹ï¼ˆå¸¦å®Œæ•´å…³ç³»æ•°æ®ï¼‰
                # è§£å†³è·¨ session é—®é¢˜ï¼šä¿å­˜åé‡æ–°æŸ¥è¯¢ï¼Œç¡®ä¿æ‰€æœ‰å…³ç³»æ­£ç¡®åŠ è½½
                event_ids = [e.id for e in all_events]
                all_events = await self._reload_events_with_relations(event_ids)
            else:
                self.logger.warning("æ²¡æœ‰æå–åˆ°ä»»ä½•äº‹é¡¹ï¼Œè·³è¿‡ä¿å­˜")
            
            # 6. æ›´æ–°æºçŠ¶æ€ä¸ºå·²å®Œæˆ
            if chunks:
                await self._update_source_status(chunks, status="COMPLETED")
            
            return all_events
            
        except Exception as e:
            # æå–å¤±è´¥æ—¶ï¼Œæ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            self.logger.error(f"æå–å¤±è´¥: {e}", exc_info=True)
            try:
                chunks = await self._load_chunks(config.chunk_ids)
                if chunks:
                    await self._update_source_status(chunks, status="FAILED", error=str(e))
            except Exception as update_error:
                self.logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {update_error}")
            
            raise ExtractError(f"æå–å¤±è´¥: {e}") from e
    
    async def _load_chunks(self, chunk_ids: List[str]) -> List[SourceChunk]:
        """æ‰¹é‡åŠ è½½chunksï¼ˆæŒ‰rankæ’åºï¼‰"""
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceChunk)
                .where(SourceChunk.id.in_(chunk_ids))
                .order_by(SourceChunk.rank)  # ğŸ†• æŒ‰ rank æ’åº
            )
            chunks = list(result.scalars().all())
            
            if len(chunks) != len(chunk_ids):
                missing = set(chunk_ids) - {c.id for c in chunks}
                self.logger.warning(f"éƒ¨åˆ†chunkä¸å­˜åœ¨: {missing}")
            
            return chunks
    
    async def _process_chunks_with_agents(
        self,
        chunks: List[SourceChunk],
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        å¹¶å‘å¤„ç†chunksï¼ˆæ¯ä¸ªchunkä¸€ä¸ªAgentï¼‰
        
        ä½¿ç”¨asyncio.Semaphoreæ§åˆ¶å¹¶å‘æ•°é‡ï¼š
        - åŒæ—¶æœ€å¤šæœ‰max_concurrencyä¸ªAgentåœ¨è¿è¡Œ
        - è¶…å‡ºçš„chunkä¼šè‡ªåŠ¨æ’é˜Ÿç­‰å¾…
        - ä¸€ä¸ªchunkå®Œæˆåï¼Œç«‹å³å¯åŠ¨ä¸‹ä¸€ä¸ª
        
        Args:
            chunks: chunkåˆ—è¡¨
            config: æå–é…ç½®
        
        Returns:
            åˆå¹¶åçš„æ‰€æœ‰äº‹é¡¹
        """
        semaphore = asyncio.Semaphore(config.max_concurrency)
        
        # è¿›åº¦è·Ÿè¸ª
        completed = 0
        success_count = 0
        failed_count = 0
        total = len(chunks)
        lock = asyncio.Lock()
        
        async def process_single_chunk(
            chunk: SourceChunk,
            index: int
        ) -> List[SourceEvent]:
            """å¤„ç†å•ä¸ªchunkï¼ˆå¸¦å¹¶å‘æ§åˆ¶å’Œè¿›åº¦ç»Ÿè®¡ï¼‰"""
            nonlocal completed, success_count, failed_count
            
            async with semaphore:  # ğŸ”’ è·å–å¹¶å‘æ§½ä½ï¼ˆæ²¡æœ‰å°±ç­‰å¾…ï¼‰
                try:
                    self.logger.info(
                        f"[{index+1}/{total}] å¼€å§‹å¤„ç†: chunk_id={chunk.id}, "
                        f"type={chunk.source_type}"
                    )
                    
                    # è°ƒç”¨chunkçº§æå–ï¼ˆä½¿ç”¨ExtractorAgentï¼‰
                    events = await self.extract_from_chunk(chunk, config)
                    
                    # æ›´æ–°è¿›åº¦
                    async with lock:
                        completed += 1
                        success_count += 1
                        progress = completed * 100 // total
                        
                    self.logger.info(
                        f"âœ… [{index+1}/{total}] å®Œæˆ ({progress}%): "
                        f"chunk_id={chunk.id}, events={len(events)}"
                    )
                    
                    return events
                    
                except Exception as e:
                    # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                    async with lock:
                        completed += 1
                        failed_count += 1
                        progress = completed * 100 // total
                        
                    self.logger.error(
                        f"âŒ [{index+1}/{total}] å¤±è´¥ ({progress}%): "
                        f"chunk_id={chunk.id}, error={e}",
                        exc_info=True
                    )
                    return []  # å¤±è´¥è¿”å›ç©ºï¼Œä¸ä¸­æ–­å…¶ä»–chunk
                # ğŸ”“ ç¦»å¼€æ—¶è‡ªåŠ¨é‡Šæ”¾æ§½ä½
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰chunk
        self.logger.info(
            f"ğŸš€ å¯åŠ¨å¹¶å‘æå–: total={total}, concurrency={config.max_concurrency}"
        )
        
        tasks = [
            process_single_chunk(chunk, i)
            for i, chunk in enumerate(chunks)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        
        # åˆå¹¶ç»“æœ
        all_events = []
        for events in results:
            all_events.extend(events)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info(
            f"ğŸ“Š æ‰¹é‡æå–ç»Ÿè®¡: æ€»æ•°={total}, æˆåŠŸ={success_count}, "
            f"å¤±è´¥={failed_count}, äº‹é¡¹={len(all_events)}"
        )
        
        return all_events

    async def _load_sections(self, config: ExtractConfig) -> List[SourceChunk]:
        """
        åŠ è½½æ¥æºç‰‡æ®µ

        Args:
            config: æå–é…ç½®

        Returns:
            æ¥æºç‰‡æ®µåˆ—è¡¨
        """
        self.logger.debug(f"åŠ è½½æ–‡ç« ç‰‡æ®µï¼šarticle_id={config.article_id}")

        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceChunk)
                .where(SourceChunk.source_type == 'article')
                .where(SourceChunk.source_id == config.article_id)
                .order_by(SourceChunk.rank)
            )
            sections = result.scalars().all()

        self.logger.info(f"åŠ è½½äº† {len(sections)} ä¸ªæ¥æºç‰‡æ®µ")

        return list(sections)

    async def _ensure_entity_types(self, config: ExtractConfig) -> None:
        """
        ç¡®ä¿å®ä½“ç±»å‹å·²åˆå§‹åŒ–åˆ°æ•°æ®åº“

        Args:
            config: æå–é…ç½®
        """
        if not config.custom_entity_types:
            return

        self.logger.debug(f"åˆå§‹åŒ–è‡ªå®šä¹‰å®ä½“ç±»å‹ï¼š{len(config.custom_entity_types)} ä¸ª")

        async with self.session_factory() as session:
            for custom_type in config.custom_entity_types:
                result = await session.execute(
                    select(DBEntityType)
                    .where(DBEntityType.source_config_id == config.source_config_id)
                    .where(DBEntityType.type == custom_type.type)
                )
                existing = result.scalar_one_or_none()

                if not existing:
                    entity_type = DBEntityType(
                        id=str(uuid.uuid4()),
                        source_config_id=config.source_config_id,
                        type=custom_type.type,
                        name=custom_type.name,
                        description=custom_type.description,
                        weight=custom_type.weight,
                        is_active=True,
                        is_default=False,
                    )
                    session.add(entity_type)
                    self.logger.debug(f"åˆ›å»ºè‡ªå®šä¹‰å®ä½“ç±»å‹ï¼š{custom_type.type}")

            await session.commit()

    def _create_batches(
        self, sections: List[SourceChunk], config: ExtractConfig
    ) -> List[List[SourceChunk]]:
        """
        åˆ›å»ºæ‰¹æ¬¡ï¼ˆæ™ºèƒ½åˆ†æ‰¹ï¼ŒåŸºäºtokené™åˆ¶ï¼‰

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡æ˜¯ç‰‡æ®µåˆ—è¡¨
        """
        batches = []
        current_batch = []
        current_tokens = 0

        for section in sections:
            section_text = f"{section.heading}\n{section.content}"
            section_tokens = estimate_tokens(section_text)

            will_exceed_token_limit = (
                current_tokens + section_tokens > config.max_tokens
            )
            will_exceed_section_limit = len(current_batch) >= config.max_sections

            if current_batch and (will_exceed_token_limit or will_exceed_section_limit):
                batches.append(current_batch)
                current_batch = []
                current_tokens = 0

            current_batch.append(section)
            current_tokens += section_tokens

        if current_batch:
            batches.append(current_batch)

        self.logger.info(
            f"åˆ›å»ºäº† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œ"
            f"å¹³å‡æ¯æ‰¹ {len(sections) / len(batches):.1f} ä¸ªç‰‡æ®µ"
        )

        return batches

    async def _process_batches_parallel(
        self,
        sections: List[SourceChunk],
        processor: EventProcessor,
        config: ExtractConfig,
    ) -> List[SourceEvent]:
        """
        å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡ï¼ˆä¸¤é˜¶æ®µå¤„ç†ï¼‰

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            processor: äº‹é¡¹å¤„ç†å™¨
            config: æå–é…ç½®

        Returns:
            æ‰€æœ‰æ‰¹æ¬¡æå–çš„äº‹é¡¹åˆ—è¡¨ï¼ˆåŒ…å«å®ä½“å…³è”ï¼‰
        """
        batches = self._create_batches(sections, config)

        if not batches:
            self.logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ‰¹æ¬¡")
            return []

        semaphore = asyncio.Semaphore(config.max_concurrency)
        
        # è¿›åº¦ç»Ÿè®¡
        completed_count = 0
        success_count = 0
        failed_count = 0
        progress_lock = asyncio.Lock()

        async def process_single_batch_extraction(
            batch_index: int, batch: List[SourceChunk]
        ) -> List[SourceEvent]:
            nonlocal completed_count, success_count, failed_count
            
            async with semaphore:
                try:
                    self.logger.debug(
                        f"æ‰¹æ¬¡ {batch_index + 1}/{len(batches)}: "
                        f"å¼€å§‹å¤„ç†ï¼ˆé˜¶æ®µ1ï¼‰ï¼Œç‰‡æ®µæ•°={len(batch)}"
                    )
                    # é˜¶æ®µ1ï¼šæå–äº‹é¡¹ï¼ˆä¸å«å®ä½“å…³è”ï¼‰
                    events = await processor.extract_events_without_entities(batch, batch_index)
                    
                    # æ›´æ–°è¿›åº¦ç»Ÿè®¡
                    async with progress_lock:
                        completed_count += 1
                        success_count += 1
                        progress_pct = completed_count * 100 // len(batches)
                        success_rate = success_count * 100 // completed_count
                        
                    self.logger.info(
                            f"ğŸ“Š è¿›åº¦: {completed_count}/{len(batches)} ({progress_pct}%) | "
                            f"æˆåŠŸç‡: {success_rate}% ({success_count}âœ“/{failed_count}âœ—) | "
                            f"æ‰¹æ¬¡ {batch_index + 1}: æå–äº† {len(events)} ä¸ªäº‹é¡¹"
                    )
                    
                    return events
                except Exception as e:
                    # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                    async with progress_lock:
                        completed_count += 1
                        failed_count += 1
                        progress_pct = completed_count * 100 // len(batches)
                        success_rate = success_count * 100 // completed_count if completed_count > 0 else 0
                        
                        self.logger.error(
                            f"âŒ è¿›åº¦: {completed_count}/{len(batches)} ({progress_pct}%) | "
                            f"æˆåŠŸç‡: {success_rate}% ({success_count}âœ“/{failed_count}âœ—) | "
                            f"æ‰¹æ¬¡ {batch_index + 1}: å¤„ç†å¤±è´¥: {e}"
                        )
                    
                    self.logger.error(
                        f"æ‰¹æ¬¡ {batch_index + 1} è¯¦ç»†é”™è¯¯",
                        exc_info=True,
                        extra={
                            "batch_index": batch_index,
                            "batch_size": len(batch),
                            "error_type": type(e).__name__,
                        },
                    )
                    # è¿”å›ç©ºåˆ—è¡¨ï¼Œä½†é”™è¯¯å·²è¢«è®°å½•
                    return []

        # é˜¶æ®µ1ï¼šå¹¶å‘æå–æ‰€æœ‰æ‰¹æ¬¡çš„äº‹é¡¹ï¼ˆä¸å«å®ä½“ï¼‰
        self.logger.info(
            f"ğŸš€ å¼€å§‹é˜¶æ®µ1ï¼šå¹¶å‘å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆäº‹é¡¹æå–ï¼‰ - "
            f"æœ€å¤§å¹¶å‘æ•°={config.max_concurrency}, LLMæ¨¡å‹={processor.llm_client.client.config.model}"
        )

        tasks = [process_single_batch_extraction(i, batch) for i, batch in enumerate(batches)]
        batch_results = await asyncio.gather(*tasks, return_exceptions=False)

        all_events = []
        for batch_events in batch_results:
            all_events.extend(batch_events)

        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        final_success_rate = success_count * 100 // len(batches) if len(batches) > 0 else 0

        self.logger.info(
            f"âœ… é˜¶æ®µ1å®Œæˆ | "
            f"æ€»æ‰¹æ¬¡: {len(batches)} | "
            f"æˆåŠŸ: {success_count}âœ“ | "
            f"å¤±è´¥: {failed_count}âœ— | "
            f"æˆåŠŸç‡: {final_success_rate}% | "
            f"æå–äº‹é¡¹: {len(all_events)} ä¸ª",
            extra={
                "total_batches": len(batches),
                "success_count": success_count,
                "failed_count": failed_count,
                "success_rate": final_success_rate,
                "total_sections": len(sections),
                "total_events": len(all_events),
            },
        )

        # é˜¶æ®µ2ï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰äº‹é¡¹çš„å®ä½“å…³è”
        if all_events:
            self.logger.info("å¼€å§‹é˜¶æ®µ2ï¼šç»Ÿä¸€å¤„ç†å®ä½“å…³è”")
            all_events = await processor.process_entity_associations(all_events)
            self.logger.info(f"é˜¶æ®µ2å®Œæˆï¼Œå·²å¤„ç† {len(all_events)} ä¸ªäº‹é¡¹çš„å®ä½“å…³è”")

        return all_events

    async def _extract_events(
        self, sections: List[SourceChunk], config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        æå–äº‹é¡¹

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            æå–çš„äº‹é¡¹åˆ—è¡¨
        """
        # è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰
        llm_client = await self._get_llm_client()
        
        processor = EventProcessor(
            llm_client=llm_client,
            prompt_manager=self.prompt_manager,
            config=config,
        )

        # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆåŠ è½½å®ä½“ç±»å‹é…ç½®ï¼‰
        await processor.initialize()

        if config.parallel:
            # å¹¶è¡Œå¤„ç†ç‰‡æ®µï¼ˆæ™ºèƒ½åˆ†æ‰¹ï¼‰
            events = await self._process_batches_parallel(sections, processor, config)
        else:
            # é¡ºåºå¤„ç†ç‰‡æ®µ
            events = []
            for i, section in enumerate(sections):
                batch_events = await processor.extract_from_sections([section], i)
                events.extend(batch_events)

        # ç»Ÿä¸€åˆ†é…å…¨å±€ rankï¼ˆç¡®ä¿åŒä¸€æ–‡ç« å†…çš„äº‹é¡¹æŒ‰é¡ºåºæ’åˆ—ï¼‰
        for i, event in enumerate(events):
            event.rank = i

        self.logger.info(f"æå–äº† {len(events)} ä¸ªäº‹é¡¹")

        return events

    async def _save_events(self, events: List[SourceEvent], config: ExtractConfig) -> None:
        """
        ä¿å­˜äº‹é¡¹åˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬å®ä½“å…³è”ï¼‰+ Elasticsearch

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        self.logger.info(f"ä¿å­˜ {len(events)} ä¸ªäº‹é¡¹åˆ°æ•°æ®åº“")

        # 1. ä¿å­˜åˆ° MySQL
        event_ids = []
        async with self.session_factory() as session:
            for event in events:
                event_ids.append(event.id)  # è®°å½• event ID
                # æ·»åŠ äº‹é¡¹
                session.add(event)
                
                # æ·»åŠ äº‹é¡¹çš„æ‰€æœ‰å®ä½“å…³è”
                if hasattr(event, 'event_associations') and event.event_associations:
                    for assoc in event.event_associations:
                        session.add(assoc)

            await session.commit()

        self.logger.info("äº‹é¡¹ä¿å­˜åˆ°MySQLå®Œæˆ")

        # 2. åŒæ­¥åˆ° Elasticsearchï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.auto_vector:
            try:
                # é‡æ–°ä»æ•°æ®åº“æŸ¥è¯¢äº‹é¡¹ï¼ˆå¸¦å…³ç³»æ•°æ®ï¼‰ï¼Œé¿å… detached instance é—®é¢˜
                fresh_events = await self._load_events_by_ids(event_ids)
                await self._sync_to_elasticsearch(fresh_events, config)
            except Exception as e:
                self.logger.error(f"åŒæ­¥åˆ°ESå¤±è´¥: {e}", exc_info=True)
                # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ

    async def _load_events_by_ids(self, event_ids: List[str]) -> List[SourceEvent]:
        """
        ä»æ•°æ®åº“åŠ è½½äº‹é¡¹åˆ—è¡¨ï¼ˆé¢„åŠ è½½å…³ç³»æ•°æ®ï¼‰
        
        Args:
            event_ids: äº‹é¡¹IDåˆ—è¡¨
            
        Returns:
            äº‹é¡¹åˆ—è¡¨
        """
        if not event_ids:
            return []
        
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.id.in_(event_ids))
                .options(selectinload(SourceEvent.event_associations))
            )
            events = result.scalars().all()
            
            # ç¡®ä¿æ•°æ®åœ¨ session å¤–å¯è®¿é—®
            # è®¾ç½® expire_on_commit=False æˆ–åœ¨è¿™é‡Œè§¦å‘åŠ è½½
            session.expire_on_commit = False
            
            # è§¦å‘åŠ è½½å…³ç³»æ•°æ®å’Œæ‰€æœ‰å­—æ®µ
            for event in events:
                # è§¦å‘æ‰€æœ‰å­—æ®µåŠ è½½ï¼ŒåŒ…æ‹¬ created_time ç­‰
                _ = event.created_time
                _ = event.updated_time
                # è§¦å‘å…³ç³»æ•°æ®åŠ è½½
                if hasattr(event, 'event_associations'):
                    _ = len(event.event_associations)
            
            return list(events)

    async def _sync_to_elasticsearch(
        self, events: List[SourceEvent], config: ExtractConfig
    ) -> None:
        """
        å°†äº‹é¡¹å’Œå®ä½“åŒæ­¥åˆ°Elasticsearch

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        self.logger.info(f"å¼€å§‹åŒæ­¥ {len(events)} ä¸ªäº‹é¡¹åˆ°Elasticsearch")
        
        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        if self.es_client is None:
            self.es_client = get_es_client()
            # Repository éœ€è¦ AsyncElasticsearch å¯¹è±¡ï¼Œè€Œä¸æ˜¯ ElasticsearchClient åŒ…è£…å¯¹è±¡
            self.event_repo = EventVectorRepository(self.es_client.client)
            self.entity_repo = EntityVectorRepository(self.es_client.client)
        
        # æ£€æŸ¥ ES è¿æ¥
        if not await self.es_client.ping():
            raise Exception("Elasticsearch è¿æ¥å¤±è´¥")
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å®ä½“
        unique_entities = {}  # key: entity_id, value: Entityå¯¹è±¡
        for event in events:
            if hasattr(event, 'event_associations') and event.event_associations:
                for assoc in event.event_associations:
                    entity_id = assoc.entity_id
                    if entity_id not in unique_entities:
                        entity = await self._load_entity_by_id(entity_id)
                        if entity:
                            unique_entities[entity_id] = entity
        
        # 1. å…ˆåŒæ­¥å®ä½“ï¼ˆå› ä¸ºäº‹é¡¹ä¼šå¼•ç”¨å®ä½“IDï¼‰
        if unique_entities:
            await self._sync_entities_to_es(list(unique_entities.values()), config)
        
        # 2. å†åŒæ­¥äº‹é¡¹
        await self._sync_events_to_es(events, config)
        
        self.logger.info(
            f"ESåŒæ­¥å®Œæˆ: {len(events)} ä¸ªäº‹é¡¹, {len(unique_entities)} ä¸ªå®ä½“"
        )


    async def _load_entity_by_id(self, entity_id: str):
        """
        ä»æ•°æ®åº“åŠ è½½å®ä½“

        Args:
            entity_id: å®ä½“ID

        Returns:
            å®ä½“å¯¹è±¡æˆ–None
        """
        async with self.session_factory() as session:
            result = await session.execute(
                select(Entity).where(Entity.id == entity_id)
            )
            return result.scalar_one_or_none()

    async def _sync_entities_to_es(
        self, entities: List[Entity], config: ExtractConfig
    ) -> None:
        """
        å°†å®ä½“æ‰¹é‡åŒæ­¥åˆ°Elasticsearch

        Args:
            entities: å®ä½“åˆ—è¡¨
            config: æå–é…ç½®
        """
        if not entities:
            return
        
        self.logger.debug(f"åŒæ­¥ {len(entities)} ä¸ªå®ä½“åˆ°ES")
        
        # é€ä¸ªç´¢å¼•å®ä½“ï¼ˆå› ä¸ºéœ€è¦ç”Ÿæˆå‘é‡ï¼‰
        success_count = 0
        error_count = 0
        
        # âœ… è·å– embedding å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ factory é…ç½®ç®¡ç†ï¼‰
        embedding_client = await get_embedding_client(scenario='general')
        
        for entity in entities:
            try:
                # ç”Ÿæˆå®ä½“åç§°çš„å‘é‡ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„embeddingæœåŠ¡ï¼‰
                vector = await embedding_client.generate(entity.name)
                
                # ä½¿ç”¨ Repository çš„ index_entity æ–¹æ³•ç´¢å¼•
                await self.entity_repo.index_entity(
                    entity_id=entity.id,
                    source_config_id=entity.source_config_id,
                    entity_type=entity.type,
                    name=entity.name,
                    vector=vector,
                    normalized_name=entity.normalized_name or "",
                    description=entity.description or "",
                    created_time=entity.created_time.isoformat() if entity.created_time else None,
                )
                success_count += 1
                
            except Exception as e:
                self.logger.error(f"å®ä½“ç´¢å¼•å¤±è´¥ {entity.id}: {e}")
                error_count += 1
        
        if error_count > 0:
            self.logger.warning(
                f"å®ä½“åŒæ­¥éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{error_count}"
            )
        else:
            self.logger.debug(f"å®ä½“åŒæ­¥æˆåŠŸ: {success_count} ä¸ª")

    async def _sync_events_to_es(
        self, events: List[SourceEvent], config: ExtractConfig
    ) -> None:
        """
        å°†äº‹é¡¹æ‰¹é‡åŒæ­¥åˆ°Elasticsearch

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        if not events:
            return
        
        self.logger.debug(f"åŒæ­¥ {len(events)} ä¸ªäº‹é¡¹åˆ°ES")
        
        # é€ä¸ªç´¢å¼•äº‹é¡¹ï¼ˆå› ä¸ºéœ€è¦ç”Ÿæˆå‘é‡ï¼‰
        success_count = 0
        error_count = 0
        
        # âœ… è·å– embedding å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ factory é…ç½®ç®¡ç†ï¼‰
        embedding_client = await get_embedding_client(scenario='general')
        
        for event in events:
            try:
                # ç”Ÿæˆæ ‡é¢˜å‘é‡ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„embeddingæœåŠ¡ï¼‰
                title_vector = await embedding_client.generate(event.title)
                
                # ç”Ÿæˆå†…å®¹å‘é‡ï¼ˆä½¿ç”¨æ ‡é¢˜+éƒ¨åˆ†å†…å®¹ï¼‰
                content_for_vector = f"{event.title}\n\n{event.content[:500]}"
                content_vector = await embedding_client.generate(content_for_vector)
                
                # æå–å…³è”çš„å®ä½“IDåˆ—è¡¨
                entity_ids = []
                if hasattr(event, 'event_associations') and event.event_associations:
                    entity_ids = [assoc.entity_id for assoc in event.event_associations]

                # å‡†å¤‡é¢å¤–å­—æ®µ
                extra_fields = {}
                if event.extra_data:
                    # categoryä¸å†ä»extra_dataè¯»å–
                    if "tags" in event.extra_data:
                        extra_fields["tags"] = event.extra_data["tags"]

                # categoryä»ç‹¬ç«‹å­—æ®µè¯»å–
                if event.category:
                    extra_fields["category"] = event.category

                # ä½¿ç”¨ Repository çš„ index_event æ–¹æ³•ç´¢å¼•
                await self.event_repo.index_event(
                    event_id=event.id,
                    source_config_id=event.source_config_id,
                    source_type=event.source_type,
                    source_id=event.source_id,
                    title=event.title,
                    summary=event.summary or "",
                    content=event.content,
                    title_vector=title_vector,
                    content_vector=content_vector,
                    entity_ids=entity_ids,
                    start_time=event.start_time.isoformat() if event.start_time else None,
                    end_time=event.end_time.isoformat() if event.end_time else None,
                    created_time=event.created_time.isoformat() if event.created_time else None,
                    **extra_fields,
                )
                success_count += 1
                
            except Exception as e:
                self.logger.error(f"äº‹é¡¹ç´¢å¼•å¤±è´¥ {event.id}: {e}")
                error_count += 1
        
        if error_count > 0:
            self.logger.warning(
                f"äº‹é¡¹åŒæ­¥éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{error_count}"
            )
        else:
            self.logger.debug(f"äº‹é¡¹åŒæ­¥æˆåŠŸ: {success_count} ä¸ª")

    async def _reload_events_with_entities(self, article_id: str) -> List[SourceEvent]:
        """
        é‡æ–°åŠ è½½äº‹é¡¹ï¼Œé¢„åŠ è½½å®ä½“å…³ç³»
        
        Args:
            article_id: æ–‡ç« ID
            
        Returns:
            åŒ…å«å®Œæ•´å®ä½“å…³ç³»çš„äº‹é¡¹åˆ—è¡¨
        """
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.article_id == article_id)
                .order_by(SourceEvent.rank)
                .options(
                    selectinload(SourceEvent.event_associations).selectinload(EventEntity.entity)
                )
            )
            events = list(result.scalars().all())
        
        self.logger.debug(f"é‡æ–°åŠ è½½äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆåŒ…å«å®ä½“å…³ç³»ï¼‰")
        return events

    # ============ æ–°å¢ï¼šåŸºäºAgentçš„chunkçº§æå– ============
    
    async def extract_from_chunk(
        self,
        chunk: SourceChunk,
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        ä»chunkæå–äº‹é¡¹ï¼ˆä½¿ç”¨Agentï¼‰
        
        æµç¨‹ï¼š
        1. åŠ è½½chunkå†…å®¹ï¼ˆsectionsæˆ–messagesï¼‰+ å…ƒæ•°æ®
        2. åŠ è½½å®ä½“ç±»å‹å®šä¹‰
        3. åˆ›å»ºExtractorAgentå¹¶æ‰§è¡Œ
        4. è½¬æ¢ç»“æœä¸ºSourceEventå¯¹è±¡
        
        Args:
            chunk: æ¥æºç‰‡æ®µå¯¹è±¡
            config: æå–é…ç½®
        
        Returns:
            æå–çš„äº‹é¡¹åˆ—è¡¨ï¼ˆåŒ…å«chunk_idï¼‰
        """
        try:
            self.logger.info(f"å¼€å§‹ä»chunkæå–: chunk_id={chunk.id}, type={chunk.source_type}")
            
            # 1. åŠ è½½å†…å®¹
            content_items, metadata = await self._load_chunk_content(chunk)
            if not content_items:
                self.logger.warning(f"Chunk {chunk.id} æ— å†…å®¹")
                return []
            
            # 2. åŠ è½½å®ä½“ç±»å‹ï¼ˆå¿…ä¼ ï¼Œä»æ•°æ®åº“åŠ è½½ï¼‰
            entity_types = await self._load_entity_types_for_chunk(config)
            
            # 3. åˆ›å»ºAgentå¹¶æå–
            from sag.core.agent.extractor import ExtractorAgent
            
            agent = ExtractorAgent(
                chunk_type=chunk.source_type,
                model_config=self.model_config
            )
            
            result = await agent.extract(
                content_items=content_items,
                metadata=metadata,
                entity_types=entity_types,
                chunk=chunk
            )
            
            # 4. è½¬æ¢ä¸ºSourceEvent
            events = await self._build_events_from_result(result, chunk, config)
            
            self.logger.info(f"Chunkæå–å®Œæˆ: chunk_id={chunk.id}, events={len(events)}")
            return events
            
        except Exception as e:
            self.logger.error(f"Chunkæå–å¤±è´¥: {e}", exc_info=True)
            raise ExtractError(f"Chunkæå–å¤±è´¥: {e}") from e
    
    async def _load_chunk_content(self, chunk: SourceChunk):
        """åŠ è½½chunkçš„å†…å®¹å’Œå…ƒæ•°æ®"""
        if chunk.source_type == 'ARTICLE':
            return await self._load_article_content(chunk)
        elif chunk.source_type == 'CHAT':
            return await self._load_conversation_content(chunk)
        else:
            raise ExtractError(f"ä¸æ”¯æŒçš„ç±»å‹: {chunk.source_type}")
    
    async def _load_article_content(self, chunk: SourceChunk):
        """åŠ è½½æ–‡ç« ç‰‡æ®µ + ä¸Šæ–‡chunkå†…å®¹ä½œä¸ºèƒŒæ™¯"""
        from sag.db import Article, ArticleSection, SourceChunk as SC
        
        async with self.session_factory() as session:
            # 1. åŠ è½½æ–‡ç« ï¼ˆæºèƒŒæ™¯ï¼‰
            article = await session.get(Article, chunk.source_id)
            if not article:
                raise ExtractError(f"æ–‡ç« ä¸å­˜åœ¨: {chunk.source_id}")
            
            # 2. åŠ è½½å½“å‰chunkçš„sectionsï¼ˆå¾…å¤„ç†å†…å®¹ï¼‰
            section_ids = chunk.references if chunk.references else []
            
            if section_ids:
                sections_result = await session.execute(
                    select(ArticleSection)
                    .where(ArticleSection.id.in_(section_ids))
                    .order_by(ArticleSection.rank)
                )
            else:
                sections_result = await session.execute(
                    select(ArticleSection)
                    .where(ArticleSection.article_id == chunk.source_id)
                    .order_by(ArticleSection.rank)
                )
            
            sections = list(sections_result.scalars().all())
            
            # 3. åŠ è½½ä¸Šä¸€ä¸ªchunkçš„å†…å®¹ï¼ˆä¸Šæ–‡èƒŒæ™¯ï¼‰
            previous_chunk = None
            if chunk.rank > 0:
                prev_result = await session.execute(
                    select(SC)
                    .where(SC.source_id == chunk.source_id)
                    .where(SC.source_type == 'ARTICLE')
                    .where(SC.rank == chunk.rank - 1)
                )
                previous_chunk = prev_result.scalar_one_or_none()
            
            return sections, {
                # Article è¡¨å­—æ®µï¼ˆæºèƒŒæ™¯ï¼‰
                "title": article.title,
                "summary": article.summary,
                "category": article.category,
                "tags": article.tags,
                
                # å½“å‰ Chunk ä¿¡æ¯
                "chunk_rank": chunk.rank,
                "chunk_heading": chunk.heading,
                
                # ä¸Šæ–‡ Chunkï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
                "previous_chunk": {
                    "heading": previous_chunk.heading,
                    "content": previous_chunk.content[:800] if len(previous_chunk.content or "") > 800 else previous_chunk.content
                } if previous_chunk and previous_chunk.content else None
            }
    
    async def _load_conversation_content(self, chunk: SourceChunk):
        """åŠ è½½å¯¹è¯æ¶ˆæ¯ + ä¸Šæ–‡chunkå†…å®¹ä½œä¸ºèƒŒæ™¯"""
        from sag.db import ChatConversation, ChatMessage, SourceChunk as SC
        
        async with self.session_factory() as session:
            # 1. åŠ è½½ä¼šè¯ï¼ˆæºèƒŒæ™¯ï¼‰
            conversation = await session.get(ChatConversation, chunk.source_id)
            if not conversation:
                raise ExtractError(f"ä¼šè¯ä¸å­˜åœ¨: {chunk.source_id}")
            
            # 2. åŠ è½½å½“å‰chunkçš„messagesï¼ˆå¾…å¤„ç†å†…å®¹ï¼‰
            message_ids = chunk.references if chunk.references else []
            
            if message_ids:
                messages_result = await session.execute(
                    select(ChatMessage)
                    .where(ChatMessage.id.in_(message_ids))
                    .order_by(ChatMessage.timestamp)
                )
            else:
                messages_result = await session.execute(
                    select(ChatMessage)
                    .where(ChatMessage.conversation_id == chunk.source_id)
                    .order_by(ChatMessage.timestamp)
                )
            
            messages = list(messages_result.scalars().all())
            
            # ä»å½“å‰messagesæå–å‚ä¸è€…å’Œæ—¶é—´ï¼ˆæ— éœ€é¢å¤–æŸ¥è¯¢ï¼‰
            participants = []
            seen_names = set()
            for msg in messages:
                if msg.sender_name and msg.sender_name not in seen_names:
                    participants.append({
                        "name": msg.sender_name,
                        "role": msg.sender_role
                    })
                    seen_names.add(msg.sender_name)
            
            time_range = ""
            if messages:
                start = messages[0].timestamp.strftime('%H:%M')
                end = messages[-1].timestamp.strftime('%H:%M')
                time_range = f"{start} ~ {end}"
            
            # 3. åŠ è½½ä¸Šä¸€ä¸ªchunkçš„å†…å®¹ï¼ˆä¸Šæ–‡èƒŒæ™¯ï¼‰
            previous_chunk = None
            if chunk.rank > 0:
                prev_result = await session.execute(
                    select(SC)
                    .where(SC.source_id == chunk.source_id)
                    .where(SC.source_type == 'CHAT')
                    .where(SC.rank == chunk.rank - 1)
                )
                previous_chunk = prev_result.scalar_one_or_none()
            
            return messages, {
                # ChatConversation è¡¨å­—æ®µï¼ˆæºèƒŒæ™¯ï¼‰
                "title": conversation.title,
                "messages_count": conversation.messages_count,
                
                # extra_data å­—æ®µ
                "platform": conversation.extra_data.get("platform") if conversation.extra_data else None,
                "scenario": conversation.extra_data.get("scenario") if conversation.extra_data else None,
                
                # ä»å½“å‰messagesæå–
                "participants": participants,
                "time_range": time_range,
                
                # å½“å‰ Chunk ä¿¡æ¯
                "chunk_rank": chunk.rank,
                "chunk_heading": chunk.heading,
                
                # ä¸Šæ–‡ Chunkï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
                "previous_chunk": {
                    "heading": previous_chunk.heading,
                    "content": previous_chunk.content[:800] if len(previous_chunk.content or "") > 800 else previous_chunk.content
                } if previous_chunk and previous_chunk.content else None
            }
    
    async def _load_entity_types_for_chunk(self, config: ExtractConfig) -> List[Dict]:
        """
        åŠ è½½å®ä½“ç±»å‹å®šä¹‰ï¼ˆå¿…å®šè¿”å›éç©ºåˆ—è¡¨ï¼‰
        
        ä¼˜å…ˆçº§ï¼š
        1. é»˜è®¤å…¨å±€ç±»å‹ï¼ˆis_default=Trueï¼‰
        2. sourceçº§åˆ«è‡ªå®šä¹‰ç±»å‹
        3. configä¸­çš„è¿è¡Œæ—¶ç±»å‹
        """
        entity_types = []
        
        async with self.session_factory() as session:
            # åŠ è½½é»˜è®¤ç±»å‹
            default_result = await session.execute(
                select(DBEntityType)
                .where(DBEntityType.is_default == True)
                .where(DBEntityType.is_active == True)
            )
            default_types = default_result.scalars().all()
            
            entity_types.extend([{
                "type": et.type,
                "name": et.name,
                "description": et.description or "",
                "weight": float(et.weight)
            } for et in default_types])
            
            # åŠ è½½sourceçº§åˆ«ç±»å‹
            if config.source_config_id:
                custom_result = await session.execute(
                    select(DBEntityType)
                    .where(DBEntityType.source_config_id == config.source_config_id)
                    .where(DBEntityType.is_active == True)
                )
                custom_types = custom_result.scalars().all()
                
                entity_types.extend([{
                    "type": et.type,
                    "name": et.name,
                    "description": et.description or "",
                    "weight": float(et.weight)
                } for et in custom_types])
        
        # è¿è¡Œæ—¶ç±»å‹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if config.custom_entity_types:
            entity_types.extend([{
                "type": et.type,
                "name": et.name,
                "description": et.description,
                "weight": et.weight
            } for et in config.custom_entity_types])
        
        return entity_types
    
    async def _build_events_from_result(
        self,
        result: Dict,
        chunk: SourceChunk,
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """è½¬æ¢Agentç»“æœä¸ºSourceEvent"""
        events = []
        
        for idx, evt in enumerate(result.get('events', [])):
            # å¤„ç† references æ ¼å¼ï¼ˆå…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼ï¼‰
            raw_references = evt.get('references', [])
            if raw_references and isinstance(raw_references[0], dict):
                # æ—§æ ¼å¼: [{"type": "section", "id": "xxx"}] -> æå– ID
                references = [ref['id'] for ref in raw_references if isinstance(ref, dict) and 'id' in ref]
            else:
                # æ–°æ ¼å¼: ["xxx", "yyy"] æˆ–ç©ºåˆ—è¡¨
                references = raw_references
            
            # ğŸ†• æ ¹æ®æ¥æºç±»å‹è®¾ç½®æ—¶é—´
            from datetime import datetime
            from sag.db import ChatMessage
            from sqlalchemy import select as sql_select
            
            start_time = None
            end_time = None
            
            if chunk.source_type == "ARTICLE":
                # æ–‡æ¡£ç±»å‹ï¼šä½¿ç”¨å½“å‰æ—¶é—´
                current_time = datetime.now()
                start_time = current_time
                end_time = current_time
                
            elif chunk.source_type == "CHAT":
                # ä¼šè¯ç±»å‹ï¼šä»äº‹é¡¹å¼•ç”¨çš„æ¶ˆæ¯ä¸­è·å–æ—¶é—´èŒƒå›´
                # âœ… ä½¿ç”¨äº‹é¡¹è‡ªå·±çš„ referencesï¼ˆä» LLM æå–ç»“æœæ¥çš„ï¼‰
                if references and isinstance(references, list):
                    async with self.session_factory() as session:
                        result_msgs = await session.execute(
                            sql_select(ChatMessage)
                            .where(ChatMessage.id.in_(references))
                            .order_by(ChatMessage.timestamp)
                        )
                        messages = list(result_msgs.scalars().all())
                        
                        if messages:
                            start_time = messages[0].timestamp
                            end_time = messages[-1].timestamp
            
            # åˆ›å»ºäº‹é¡¹
            event = SourceEvent(
                id=str(uuid.uuid4()),
                source_config_id=config.source_config_id,
                source_type=chunk.source_type,
                source_id=chunk.source_id,
                article_id=chunk.source_id if chunk.source_type == 'ARTICLE' else None,
                conversation_id=chunk.source_id if chunk.source_type == 'CHAT' else None,
                chunk_id=chunk.id,
                title=evt['title'],
                summary=evt['summary'],
                content=evt['content'],
                category=evt.get('category', ''),
                # ä¸šåŠ¡å­—æ®µï¼ˆå…¼å®¹ä¸»ç³»ç»Ÿï¼‰- typeä¸source_typeä¿æŒä¸€è‡´
                type=chunk.source_type,
                priority="UNKNOWN",  # é»˜è®¤å€¼
                status="UNKNOWN",  # é»˜è®¤å€¼
                rank=idx,
                start_time=start_time,  # ğŸ†•
                end_time=end_time,      # ğŸ†•
                references=references
            )
            
            # å…³è”å®ä½“
            event.event_associations = []
            
            for entity_data in evt.get('entities', []):
                entity = await self._get_or_create_entity(entity_data, config)
                
                # âœ… è·³è¿‡æ— æ•ˆå®ä½“ï¼ˆtypeä¸å­˜åœ¨æ—¶è¿”å›Noneï¼‰
                if entity is None:
                    continue
                
                assoc = EventEntity(
                    id=str(uuid.uuid4()),
                    event_id=event.id,
                    entity_id=entity.id,
                    description=entity_data.get('description', '')
                )
                # âœ… ä¸è®¾ç½® entity å…³ç³»ï¼Œé¿å…è·¨ session å†²çª
                # assoc.entity = entity  # ç§»é™¤ï¼Œä¼šåœ¨ä¿å­˜åé‡æ–°åŠ è½½
                
                event.event_associations.append(assoc)
            
            events.append(event)
        
        return events
    
    async def _get_or_create_entity(
        self,
        entity_data: Dict,
        config: ExtractConfig
    ) -> Optional[Entity]:
        """
        æŸ¥æ‰¾æˆ–åˆ›å»ºå®ä½“ï¼ˆå»é‡ï¼‰
        
        Returns:
            Entityå¯¹è±¡ï¼Œå¦‚æœå®ä½“ç±»å‹æ— æ•ˆåˆ™è¿”å›None
        """
        normalized_name = entity_data['name'].strip().lower()
        
        async with self.session_factory() as session:
            # æŸ¥æ‰¾å·²å­˜åœ¨
            existing_result = await session.execute(
                select(Entity)
                .where(Entity.source_config_id == config.source_config_id)
                .where(Entity.type == entity_data['type'])
                .where(Entity.normalized_name == normalized_name)
            )
            entity = existing_result.scalar_one_or_none()
            
            if entity:
                return entity
            
            # åˆ›å»ºæ–°å®ä½“ - è·å–entity_type_id
            entity_type_result = await session.execute(
                select(DBEntityType)
                .where(DBEntityType.type == entity_data['type'])
                .where(
                    (DBEntityType.source_config_id == config.source_config_id) |
                    (DBEntityType.is_default == True)
                )
                .where(DBEntityType.is_active == True)
            )
            entity_type = entity_type_result.scalar_one_or_none()
            
            if not entity_type:
                # âœ… è®°å½•è­¦å‘Šä½†ä¸æŠ›å¼‚å¸¸ï¼Œè¿”å› None
                self.logger.warning(
                    f"è·³è¿‡æ— æ•ˆå®ä½“ç±»å‹: type={entity_data['type']}, "
                    f"name={entity_data.get('name', 'N/A')}"
                )
                return None
            
            entity = Entity(
                id=str(uuid.uuid4()),
                source_config_id=config.source_config_id,
                entity_type_id=entity_type.id,
                type=entity_data['type'],
                name=entity_data['name'],
                normalized_name=normalized_name,
                description=entity_data.get('description', '')
            )
            
            session.add(entity)
            await session.commit()
            await session.refresh(entity)
            
            return entity
    
    async def _reload_events_with_relations(self, event_ids: List[str]) -> List[SourceEvent]:
        """
        é‡æ–°ä»æ•°æ®åº“åŠ è½½äº‹é¡¹åˆ—è¡¨ï¼ˆé¢„åŠ è½½å…³ç³»æ•°æ®ï¼‰
        
        è§£å†³è·¨ session é—®é¢˜ï¼šä¿å­˜åé‡æ–°æŸ¥è¯¢ï¼Œç¡®ä¿æ‰€æœ‰å…³ç³»æ­£ç¡®åŠ è½½
        
        Args:
            event_ids: äº‹é¡¹IDåˆ—è¡¨
            
        Returns:
            åŒ…å«å®Œæ•´å…³ç³»çš„äº‹é¡¹åˆ—è¡¨
        """
        if not event_ids:
            return []
        
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.id.in_(event_ids))
                .options(
                    selectinload(SourceEvent.event_associations).selectinload(EventEntity.entity)
                )
            )
            events = list(result.scalars().all())
            
            # è®¾ç½® expire_on_commit=Falseï¼Œç¡®ä¿æ•°æ®åœ¨ session å¤–å¯è®¿é—®
            session.expire_on_commit = False
            
            # è§¦å‘å…³ç³»æ•°æ®åŠ è½½ï¼ˆç¡®ä¿æ‰€æœ‰å­—æ®µåœ¨ session å¤–å¯è®¿é—®ï¼‰
            for event in events:
                # è§¦å‘äº‹é¡¹å­—æ®µåŠ è½½
                _ = event.title
                _ = event.created_time
                
                # è§¦å‘å…³è”å’Œå®ä½“åŠ è½½
                if hasattr(event, 'event_associations'):
                    for assoc in event.event_associations:
                        _ = assoc.id
                        if assoc.entity:
                            _ = assoc.entity.name
                            _ = assoc.entity.type
            
            self.logger.debug(f"é‡æ–°åŠ è½½äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆåŒ…å«å®Œæ•´å…³ç³»ï¼‰")
            return events
    
    async def _update_source_status(
        self,
        chunks: List[SourceChunk],
        status: str,
        error: Optional[str] = None
    ) -> None:
        """
        æ›´æ–°æºçŠ¶æ€ï¼ˆArticle æˆ– ChatConversationï¼‰
        
        Args:
            chunks: chunkåˆ—è¡¨ï¼ˆç”¨äºç¡®å®šæºç±»å‹å’ŒIDï¼‰
            status: çŠ¶æ€å€¼ï¼ˆPENDING/PROCESSING/COMPLETED/FAILEDï¼‰
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå¤±è´¥æ—¶æä¾›ï¼‰
        """
        if not chunks:
            return
        
        from sag.db import Article, ChatConversation
        
        # ç¡®å®šæºç±»å‹å’ŒIDï¼ˆåŒä¸€æ‰¹chunksåº”è¯¥æ¥è‡ªåŒä¸€ä¸ªæºï¼‰
        source_type = chunks[0].source_type
        source_id = chunks[0].source_id
        
        # éªŒè¯æ‰€æœ‰chunksæ¥è‡ªåŒä¸€ä¸ªæº
        if not all(c.source_type == source_type and c.source_id == source_id for c in chunks):
            self.logger.warning("Chunksæ¥è‡ªä¸åŒçš„æºï¼Œæ— æ³•ç»Ÿä¸€æ›´æ–°çŠ¶æ€")
            return
        
        async with self.session_factory() as session:
            try:
                if source_type == "ARTICLE":
                    result = await session.execute(
                        select(Article).where(Article.id == source_id)
                    )
                    source = result.scalar_one_or_none()
                    
                    if source:
                        source.status = status
                        if error:
                            source.error = error
                        await session.commit()
                        self.logger.info(f"âœ… å·²æ›´æ–°æ–‡ç« çŠ¶æ€: {source_id} -> {status}")
                    else:
                        self.logger.warning(f"æ–‡ç« ä¸å­˜åœ¨: {source_id}")
                
                elif source_type == "CHAT":
                    result = await session.execute(
                        select(ChatConversation).where(ChatConversation.id == source_id)
                    )
                    source = result.scalar_one_or_none()
                    
                    if source:
                        # ChatConversation æ²¡æœ‰ status å­—æ®µï¼Œè®°å½•åˆ° extra_data
                        if source.extra_data is None:
                            source.extra_data = {}
                        source.extra_data["extract_status"] = status
                        if error:
                            source.extra_data["extract_error"] = error
                        await session.commit()
                        self.logger.info(f"âœ… å·²æ›´æ–°ä¼šè¯æå–çŠ¶æ€: {source_id} -> {status}")
                    else:
                        self.logger.warning(f"ä¼šè¯ä¸å­˜åœ¨: {source_id}")
                
                else:
                    self.logger.warning(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
            
            except Exception as e:
                self.logger.error(f"æ›´æ–°æºçŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
    