"""
å®ä½“æ‰©å±•æ¨¡å—ï¼ˆExpandï¼‰

å®ç°å¤šè·³å¾ªç¯æœç´¢ç®—æ³•ï¼š
1. æ ¹æ®[key-final]ï¼Œç”¨sqlæŸ¥æ‰¾åˆ°æ‰€æœ‰å…³è”çš„eventï¼Œå¾—åˆ°æ–°çš„[Event-key-related-2]
2. è®¡ç®—åŸå§‹queryå’Œæ–°çš„[Event-key-related-2]çš„ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°ç›¸ä¼¼æ€§å‘é‡(event-query-2)
3. è®¡ç®—Event-key-related-2æƒé‡å‘é‡ï¼šæ ¹æ®æ¯ä¸ªeventåŒ…å«key-finalçš„æƒ…å†µï¼Œå°†å¯¹åº”keyçš„æƒé‡(key-final)ç›¸åŠ 
4. è®¡ç®—event-key-queryæƒé‡å‘é‡ï¼šå°†ï¼ˆevent-key-2ï¼‰*(event-query-2)ï¼Œå¾—åˆ°æ–°çš„ï¼ˆevent-jump-2ï¼‰
5. åå‘è®¡ç®—keyæƒé‡å‘é‡ï¼šæ ¹æ®eventæƒé‡åå‘å¾—å‡ºeventé‡Œæ‰€æœ‰çš„keyçš„é‡è¦æ€§

æ–°ç‰¹æ€§ï¼štopkey å»é‡æœºåˆ¶
- topkey é™åˆ¶æ¯ä¸€è·³çš„æœ€å¤§æ–° key æ•°é‡ï¼ˆä¸æ˜¯æœ€ç»ˆè¿”å›æ€»æ•°ï¼‰
- æ¯ä¸€è·³éƒ½ä¼šå»é‡ï¼Œå»æ‰å‰é¢å·²ç»å‡ºç°è¿‡çš„ key
- æœ€ç»ˆè¿”å›æ‰€æœ‰å‘ç°çš„å”¯ä¸€ keysï¼Œæœ€å¤§æ•°é‡ä¸º topkey * max_jumps
"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math

from sqlalchemy import select
from sqlalchemy.orm import selectinload

from sag.core.ai.base import BaseLLMClient
from sag.core.ai.models import LLMMessage, LLMRole
from sag.core.prompt.manager import PromptManager
from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.entity_repository import EntityVectorRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import SourceEvent, Entity, EventEntity, get_session_factory
from sag.exceptions import AIError
from sag.modules.load.processor import DocumentProcessor
from sag.modules.search.config import SearchConfig
from sag.modules.search.recall import RecallSearcher, RecallResult
from sag.modules.search.tracker import Tracker  # ğŸ†• ç»Ÿä¸€ä½¿ç”¨Tracker
from sag.utils import get_logger

logger = get_logger("search.expand")


@dataclass
class ExpandResult:
    """å®ä½“æ‰©å±•ç»“æœ"""
    # æœ€ç»ˆç»“æœ
    key_final: List[Dict[str, Any]]  # [{"key_id": str, "name": str, "weight": float, "steps": [int], "hop": int}, ...]
                                        # stepsåªåŒ…å«ä¸€ä¸ªæ•°å­—ï¼Œè¡¨ç¤ºkeyè¢«æœ€æ—©å‘ç°çš„æ­¥éª¤
                                        # steps=1: Recallä¸­å‘ç°, steps=2: Expandç¬¬1è·³å‘ç°, steps=3: Expandç¬¬2è·³å‘ç°, ä»¥æ­¤ç±»æ¨
                                        # hop: è·³æ•°ç¼–å·ï¼Œç”¨äºå‰ç«¯é¢œè‰²åŒºåˆ†
                                        #      hop=0: Recallé˜¶æ®µ (å»ºè®®æœ€æ·±è‰²)
                                        #      hop=1: ç¬¬1è·³ (å»ºè®®æ·±è‰²)
                                        #      hop=2: ç¬¬2è·³ (å»ºè®®ä¸­ç­‰è‰²)
                                        #      hop=N: ç¬¬Nè·³ (å»ºè®®ç”±æ·±åˆ°æµ…æ¸å˜)
                                        # æ³¨æ„ï¼šç°åœ¨è¿”å›æ‰€æœ‰å‘ç°çš„å”¯ä¸€keysï¼Œä¸å†å—top_n_keysæˆ–final_key_thresholdé™åˆ¶

    # å¤šè·³ç»“æœ
    jump_results: List[Dict[str, Any]]  # æ¯ä¸€è·³çš„ç»“æœ

    # èšåˆç»Ÿè®¡
    total_jumps: int  # å®é™…è·³è·ƒæ¬¡æ•°
    convergence_reached: bool  # æ˜¯å¦æ”¶æ•›

    # ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    all_events_by_jump: Dict[int, List[str]]  # æ¯è·³æ‰¾åˆ°çš„events
    all_keys_by_jump: Dict[int, List[str]]    # æ¯è·³è®¡ç®—çš„keysï¼ˆå»é‡åçš„æ–°keysï¼‰
    weight_evolution: Dict[int, Dict[str, float]]  # æƒé‡æ¼”åŒ–


class ExpandSearcher:
    """å®ä½“æ‰©å±•æœç´¢å™¨ - å®ç°å¤šè·³å¾ªç¯æœç´¢ç®—æ³•"""

    def __init__(
        self,
        llm_client: BaseLLMClient,
        prompt_manager: PromptManager,
        recall_searcher: RecallSearcher,
    ):
        """
        åˆå§‹åŒ–å®ä½“æ‰©å±•æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            recall_searcher: å®ä½“å¬å›æœç´¢å™¨å®ä¾‹
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.recall_searcher = recall_searcher
        self.session_factory = get_session_factory()
        self.logger = get_logger("search.expand")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.entity_repo = EntityVectorRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºç”Ÿæˆå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "å®ä½“æ‰©å±•æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    async def _calculate_cosine_similarity(self, vector1: List[float], vector2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vector1: ç¬¬ä¸€ä¸ªå‘é‡
            vector2: ç¬¬äºŒä¸ªå‘é‡

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´åœ¨[0, 1]ä¹‹é—´
        """
        if not vector1 or not vector2:
            return 0.0

        # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆä½¿ç”¨ float32 ä¼˜åŒ–ï¼‰
        v1 = np.array(vector1, dtype=np.float32)
        v2 = np.array(vector2, dtype=np.float32)

        # æ£€æŸ¥å‘é‡é•¿åº¦æ˜¯å¦ä¸€è‡´
        if len(v1) != len(v2):
            return 0.0

        try:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            # é¿å…é™¤é›¶é”™è¯¯
            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            # ç¡®ä¿ç»“æœåœ¨[0, 1]èŒƒå›´å†…
            return max(0.0, min(1.0, float(similarity)))

        except Exception as e:
            self.logger.warning(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
            return 0.0

    async def _batch_cosine_similarity(
        self,
        query_embedding: List[float],
        target_vectors: List[List[float]]
    ) -> np.ndarray:
        """
        æ‰¹é‡è®¡ç®— query å‘é‡ä¸å¤šä¸ªç›®æ ‡å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        ä¼˜åŒ–ï¼šä½¿ç”¨ numpy å‘é‡åŒ–æ“ä½œï¼Œå‡å°‘å¾ªç¯å¼€é”€

        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            target_vectors: ç›®æ ‡å‘é‡åˆ—è¡¨

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦æ•°ç»„
        """
        try:
            if not target_vectors:
                return np.array([])

            # è½¬æ¢ä¸º numpy æ•°ç»„ï¼ˆä½¿ç”¨ float32 å‡å°‘å†…å­˜å’Œè®¡ç®—é‡ï¼‰
            query_array = np.array(query_embedding, dtype=np.float32)
            target_array = np.array(target_vectors, dtype=np.float32)

            # è®¡ç®—ç‚¹ç§¯ï¼ˆçŸ©é˜µä¹˜æ³•ï¼Œæ¯”å¾ªç¯å¿«å¾—å¤šï¼‰
            dot_products = np.dot(target_array, query_array)

            # è®¡ç®—èŒƒæ•°
            query_norm = np.linalg.norm(query_array)
            target_norms = np.linalg.norm(target_array, axis=1)

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå‘é‡åŒ–æ“ä½œï¼Œé¿å…é™¤ä»¥é›¶ï¼‰
            denominators = target_norms * query_norm
            similarities = np.divide(
                dot_products,
                denominators,
                out=np.zeros_like(dot_products),
                where=denominators > 1e-8  # ä½¿ç”¨å°é˜ˆå€¼è€Œä¸æ˜¯0ï¼Œæ›´ç¨³å®š
            )

            # ç¡®ä¿ç»“æœåœ¨ [0, 1] èŒƒå›´å†…
            similarities = np.clip(similarities, 0.0, 1.0)

            return similarities

        except Exception as e:
            self.logger.error(f"æ‰¹é‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return np.zeros(len(target_vectors), dtype=np.float32)

    async def search(self, config: SearchConfig, recall_result: Optional[RecallResult] = None) -> ExpandResult:
        """
        æ‰§è¡Œå¤šè·³å¾ªç¯æœç´¢ç®—æ³•

        Args:
            config: æœç´¢é…ç½®
            recall_result: å®ä½“å¬å›ç»“æœï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä¼šè‡ªåŠ¨æ‰§è¡Œå¬å›ï¼‰

        Returns:
            å®ä½“æ‰©å±•ç»“æœ
        """
        try:
            self.logger.info(
                f"å¼€å§‹å®ä½“æ‰©å±•ï¼šsource_config_ids={config.source_config_ids}, query={config.query}, "
                f"max_jumps={config.expand.max_hops}"
            )

            # å¦‚æœæ²¡æœ‰æä¾›Recallç»“æœï¼Œå…ˆæ‰§è¡ŒRecall
            if recall_result is None:
                self.logger.info("æœªæä¾›Recallç»“æœï¼Œå…ˆæ‰§è¡ŒRecallæœç´¢")
                recall_result = await self.recall_searcher.search(config)

            # æå–Recallçš„æœ€ç»ˆkeysä½œä¸ºèµ·å§‹ç‚¹
            key_final_ids = [key["key_id"] for key in recall_result.key_final]
            key_final_weights = {key["key_id"]: key["weight"] for key in recall_result.key_final}

            if not key_final_ids:
                self.logger.warning("å®ä½“å¬å›æ²¡æœ‰äº§ç”Ÿæœ‰æ•ˆçš„keysï¼Œæ— æ³•è¿›è¡Œå®ä½“æ‰©å±•")
                return ExpandResult(
                    key_final=[],
                    jump_results=[],
                    total_jumps=0,
                    convergence_reached=False,
                    all_events_by_jump={},
                    all_keys_by_jump={},
                    weight_evolution={},
                )

            self.logger.info(f"ä»Recallè·å¾— {len(key_final_ids)} ä¸ªèµ·å§‹keys")

            # åˆå§‹åŒ–å¤šè·³å¾ªç¯å˜é‡
            jump_results = []
            all_events_by_jump = {}
            all_keys_by_jump = {}
            weight_evolution = {}

            # æ·»åŠ å…¨å±€å»é‡é›†åˆï¼Œè®°å½•æ‰€æœ‰å‘ç°è¿‡çš„keys
            all_discovered_keys = set(key_final_ids)  # åˆå§‹åŒ…å«Recallçš„æ‰€æœ‰keys

            # ===  çº¿ç´¢è¿½è¸ªï¼šè®°å½•parentå…³ç³» ===
            key_parent_map = {}  # {child_key_id: {"parent_id": str, "parent_name": str, "parent_type": str, "hop": int}}

            # ğŸ†• è®°å½•ç¬¬ä¸€è·³ä¸­æ²¡æœ‰æ‰©å±•å‡ºæ–°å®ä½“çš„ recall keys
            no_expansion_recall_keys = []  # List[str]: key_ids

            current_key_ids = key_final_ids.copy()
            current_key_weights = key_final_weights.copy()
            previous_total_weight = 0.0

            # å¼€å§‹å¤šè·³å¾ªç¯
            for jump in range(1, config.expand.max_hops + 1):
                self.logger.info(f"=== å¼€å§‹ç¬¬ {jump} è·³ ===")

                # 1. æ ¹æ®å½“å‰keysæŸ¥æ‰¾åˆ°æ‰€æœ‰å…³è”çš„events
                event_key_related_2 = await self._step1_keys_to_events(current_key_ids)
                all_events_by_jump[jump] = event_key_related_2

                if not event_key_related_2:
                    self.logger.warning(f"ç¬¬ {jump} è·³ï¼šæ²¡æœ‰æ‰¾åˆ°å…³è”çš„eventsï¼Œåœæ­¢è·³è·ƒ")
                    break

                self.logger.info(f"ç¬¬ {jump} è·³æ­¥éª¤1ï¼šæ‰¾åˆ° {len(event_key_related_2)} ä¸ªå…³è”events")

                # 2. è®¡ç®—åŸå§‹queryå’Œæ–°çš„eventsçš„ç›¸ä¼¼åº¦
                event_query_2, e2_weights = await self._step2_calculate_event_query_similarity(
                    config, event_key_related_2
                )

                if not event_query_2:
                    self.logger.warning(f"ç¬¬ {jump} è·³ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼eventsï¼Œåœæ­¢è·³è·ƒ")
                    break

                # ğŸ“Š è¯Šæ–­ï¼šç¬¬1è·³æ—¶æ£€æŸ¥ key-final å„ä¸ª key çš„ events è¿‡æ»¤æƒ…å†µ
                if jump == 1:
                    await self._diagnose_key_final_event_filtering(
                        current_key_ids,
                        current_key_weights,
                        event_key_related_2,
                        e2_weights
                    )

                self.logger.info(f"ç¬¬ {jump} è·³æ­¥éª¤2ï¼šè®¡ç®—äº† {len(event_query_2)} ä¸ªeventsçš„ç›¸ä¼¼åº¦")

                # ğŸ“Š æ—¥å¿—ï¼šæ˜¾ç¤ºæœ€ç»ˆé€‰å®šçš„äº‹é¡¹
                self.logger.info(f"ğŸ“Š ç¬¬ {jump} è·³æœ€ç»ˆé€‰å®šäº‹é¡¹åˆ—è¡¨ (å…±{len(event_query_2)}ä¸ª):")
                sorted_events = sorted(event_query_2, key=lambda x: x.get("similarity", 0), reverse=True)
                for i, event in enumerate(sorted_events[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                    event_id = event.get("event_id", "")
                    title = event.get("title", "")[:40]
                    similarity = event.get("similarity", 0)
                    self.logger.info(f"  {i}. [{event_id[:8]}] {title}... (ç›¸ä¼¼åº¦={similarity:.3f})")
                if len(event_query_2) > 5:
                    self.logger.info(f"  ... è¿˜æœ‰ {len(event_query_2) - 5} ä¸ªäº‹é¡¹")

                # 3. è®¡ç®—event-keyæƒé‡å‘é‡
                event_key_2 = await self._step3_calculate_event_key_weights(
                    event_key_related_2, current_key_ids, current_key_weights
                )

                self.logger.info(f"ç¬¬ {jump} è·³æ­¥éª¤3ï¼šè®¡ç®—äº† {len(event_key_2)} ä¸ªeventsçš„keyæƒé‡")

                # 4. è®¡ç®—event-key-queryæƒé‡å‘é‡
                event_jump_2 = await self._step4_calculate_event_key_query_weights(
                    event_key_2, e2_weights
                )

                self.logger.info(f"ç¬¬ {jump} è·³æ­¥éª¤4ï¼šè®¡ç®—äº† {len(event_jump_2)} ä¸ªeventsçš„å¤åˆæƒé‡")

                # 5. åå‘è®¡ç®—keyæƒé‡å‘é‡ + è¿½è¸ªæ‰©å±•å…³ç³»
                new_key_weights, key_expansion_trace = await self._step5_calculate_key_event_weights(
                    event_key_related_2, current_key_ids, event_jump_2
                )

                self.logger.info(f"ç¬¬ {jump} è·³æ­¥éª¤5ï¼šè®¡ç®—äº† {len(new_key_weights)} ä¸ªkeysçš„æ–°æƒé‡, è¿½è¸ªåˆ° {len(key_expansion_trace)} ä¸ªæ‰©å±•å…³ç³»")

                # ğŸ“Š è¯Šæ–­ï¼šç¬¬1è·³æ—¶æ£€æŸ¥æ¯ä¸ª parent key æ‰©å±•å‡ºçš„ child entities æƒ…å†µ
                if jump == 1:
                    # å…ˆè¿‡æ»¤å‡ºæ–°çš„ entitiesï¼ˆä¸åŒ…æ‹¬å·²çŸ¥çš„ parent keysï¼‰
                    new_unique_keys = [(key_id, weight) for key_id, weight in new_key_weights.items()
                                     if key_id not in all_discovered_keys]

                    # æŒ‰æƒé‡æ’åº
                    sorted_new_keys = sorted(new_unique_keys, key=lambda x: x[1], reverse=True)
                    top_new_keys_preview = sorted_new_keys[:config.expand.entities_per_hop]

                    no_expansion_recall_keys = await self._diagnose_key_expansion_success(
                        current_key_ids,
                        current_key_weights,
                        key_expansion_trace,
                        new_key_weights,
                        top_new_keys_preview,
                        all_discovered_keys,
                        config.expand.entities_per_hop
                    )

                # è®°å½•æƒé‡æ¼”åŒ–
                weight_evolution[jump] = new_key_weights.copy()

                # æ£€æŸ¥æ”¶æ•›æ€§
                current_total_weight = sum(new_key_weights.values())
                weight_change = abs(current_total_weight - previous_total_weight)
                previous_total_weight = current_total_weight

                # æ”¶é›†å½“å‰è·³çš„ç»“æœ
                jump_result = {
                    "jump": jump,
                    "events_found": len(event_key_related_2),
                    "events_similar": len(event_query_2),
                    "keys_count": len(new_key_weights),
                    "total_weight": current_total_weight,
                    "weight_change": weight_change,
                }
                jump_results.append(jump_result)

                self.logger.info(f"ç¬¬ {jump} è·³å®Œæˆï¼šæ€»æƒé‡={current_total_weight:.4f}, æƒé‡å˜åŒ–={weight_change:.4f}")

                # æ£€æŸ¥æ”¶æ•›æ¡ä»¶
                if weight_change < config.expand.weight_change_threshold:
                    self.logger.info(f"ç¬¬ {jump} è·³ï¼šæƒé‡å˜åŒ– {weight_change:.4f} å°äºæ”¶æ•›é˜ˆå€¼ {config.expand.weight_change_threshold}ï¼Œåœæ­¢è·³è·ƒ")
                    convergence_reached = True
                    break

                # æ›´æ–°å½“å‰keyså’Œæƒé‡ï¼Œä¸ºä¸‹ä¸€è·³å‡†å¤‡
                # é¦–å…ˆè¿‡æ»¤æ‰å·²ç»å‘ç°è¿‡çš„keysï¼Œå®ç°å»é‡
                new_unique_keys = [(key_id, weight) for key_id, weight in new_key_weights.items()
                                 if key_id not in all_discovered_keys]

                # æŒ‰æƒé‡æ’åºï¼Œé€‰æ‹©æƒé‡æœ€é«˜çš„topkeyä¸ªæ–°keys
                sorted_new_keys = sorted(new_unique_keys, key=lambda x: x[1], reverse=True)
                top_new_keys = sorted_new_keys[:config.expand.entities_per_hop]

                # ğŸ“Š æ—¥å¿—ï¼šæ˜¾ç¤ºæœ€ç»ˆé€‰å®šçš„æ–°å®ä½“
                if top_new_keys:
                    self.logger.info(f"ğŸ“Š ç¬¬ {jump} è·³æœ€ç»ˆé€‰å®šæ–°å®ä½“åˆ—è¡¨ (å…±{len(top_new_keys)}ä¸ª):")
                    try:
                        key_ids = [key_id for key_id, _ in top_new_keys]
                        async with self.session_factory() as session:
                            query = select(Entity).where(Entity.id.in_(key_ids))
                            result = await session.execute(query)
                            entities = {entity.id: entity for entity in result.scalars().all()}

                        for i, (key_id, weight) in enumerate(top_new_keys, 1):
                            entity = entities.get(key_id)
                            if entity:
                                self.logger.info(f"  {i}. [{entity.type}] {entity.name} (æƒé‡={weight:.3f})")
                            else:
                                self.logger.info(f"  {i}. {key_id[:12]}... (æƒé‡={weight:.3f})")
                    except Exception as e:
                        self.logger.warning(f"æŸ¥è¯¢å®ä½“åç§°å¤±è´¥: {e}")
                        for i, (key_id, weight) in enumerate(top_new_keys, 1):
                            self.logger.info(f"  {i}. {key_id[:12]}... (æƒé‡={weight:.3f})")
                else:
                    self.logger.info(f"ğŸ“Š ç¬¬ {jump} è·³æ²¡æœ‰é€‰å®šæ–°å®ä½“")

                # === è®°å½•parentå…³ç³»ï¼ˆä½¿ç”¨çœŸå®çš„æ‰©å±•è·¯å¾„ï¼‰ ===
                # ä¸ºæ¯ä¸ªæ–°å‘ç°çš„keyè®°å½•å®ƒæ˜¯ä»å“ªä¸ªparent keyé€šè¿‡å“ªä¸ªeventæ‰©å±•è€Œæ¥çš„
                for child_key_id, child_weight in top_new_keys:
                    # ä»key_expansion_traceä¸­è·å–çœŸå®çš„æ‰©å±•å…³ç³»
                    if child_key_id in key_expansion_trace:
                        expansion_paths = key_expansion_trace[child_key_id]
                        # é€‰æ‹©æƒé‡æœ€é«˜çš„æ‰©å±•è·¯å¾„ä½œä¸ºä¸»parent
                        # expansion_paths: [(parent_id, event_id, event_weight), ...]
                        best_path = max(expansion_paths, key=lambda x: x[2])  # x[2]æ˜¯event_weight
                        parent_id, event_id, event_weight = best_path

                        key_parent_map[child_key_id] = {
                            "parent_id": parent_id,
                            "event_id": event_id,  # è®°å½•æ‰©å±•æ‰€é€šè¿‡çš„event
                            "event_weight": event_weight,  # è®°å½•eventçš„æƒé‡
                            "hop": jump,  # è®°å½•åœ¨ç¬¬å‡ è·³å‘ç°çš„
                            "num_paths": len(expansion_paths),  # è®°å½•æ€»å…±æœ‰å¤šå°‘æ¡æ‰©å±•è·¯å¾„
                        }

                        self.logger.debug(
                            f"  âœ… è®°å½•æ‰©å±•å…³ç³»: {child_key_id[:8]} â† {parent_id[:8]} "
                            f"(via event {event_id[:8]}, weight={event_weight:.3f}, "
                            f"{len(expansion_paths)}æ¡å¯é€‰è·¯å¾„)"
                        )
                    else:
                        # Fallbackï¼šå¦‚æœæ²¡æœ‰è¿½è¸ªåˆ°æ‰©å±•å…³ç³»ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
                        self.logger.warning(
                            f"  âš ï¸  æœªè¿½è¸ªåˆ° {child_key_id[:8]} çš„æ‰©å±•å…³ç³»ï¼Œä½¿ç”¨fallbacké€»è¾‘"
                        )
                        if current_key_ids:
                            parent_id = max(current_key_ids, key=lambda k: current_key_weights.get(k, 0))
                            key_parent_map[child_key_id] = {
                                "parent_id": parent_id,
                                "hop": jump,
                                "is_fallback": True  # æ ‡è®°ä¸ºfallback
                            }

                # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè®°å½•parentå…³ç³»
                self.logger.info(
                    f"ğŸ” [Expandè¯Šæ–­] ç¬¬{jump}è·³parentå…³ç³»: "
                    f"ä¸º{len(top_new_keys)}ä¸ªæ–°keyè®°å½•parent, "
                    f"å…¶ä¸­{sum(1 for k in [k[0] for k in top_new_keys] if k in key_expansion_trace)}ä¸ªæ¥è‡ªçœŸå®æ‰©å±•è·¯å¾„, "
                    f"å½“å‰parent_mapæ€»æ•°={len(key_parent_map)}"
                )

                # æ›´æ–°å…¨å±€å»é‡é›†åˆï¼Œæ·»åŠ æ–°å‘ç°çš„keys
                for key_id, _ in top_new_keys:
                    all_discovered_keys.add(key_id)

                # è®¾ç½®å½“å‰è·³çš„keyså’Œæƒé‡
                current_key_ids = [key_id for key_id, _ in top_new_keys]
                current_key_weights = {key_id: weight for key_id, weight in top_new_keys}

                all_keys_by_jump[jump] = current_key_ids.copy()

                self.logger.info(f"ç¬¬ {jump} è·³ï¼šå‘ç° {len(new_key_weights)} ä¸ªkeysï¼Œå»é‡åé€‰æ‹© {len(current_key_ids)} ä¸ªæ–°keysè¿›å…¥ä¸‹ä¸€è·³")
                self.logger.info(f"ç¬¬ {jump} è·³ï¼šç´¯è®¡å·²å‘ç° {len(all_discovered_keys)} ä¸ªå”¯ä¸€keys")

            # æ±‡æ€»æœ€ç»ˆç»“æœ
            final_key_weights = await self._aggregate_key_weights(weight_evolution)
            key_final = await self._extract_final_keys(
                final_key_weights, config, recall_result.key_final, weight_evolution, all_discovered_keys, key_parent_map
            )

            # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šparent_entityå­—æ®µéªŒè¯
            keys_with_parent = sum(1 for k in key_final if "parent_entity" in k)
            expand_keys = sum(1 for k in key_final if k.get("steps", [0])[0] >= 2)
            self.logger.info(
                f"ğŸ” [Expandè¯Šæ–­] parent_entityå­—æ®µç»Ÿè®¡: "
                f"æ€»keys={len(key_final)}, "
                f"Expandå‘ç°={expand_keys}, "
                f"æœ‰parent_entity={keys_with_parent}"
            )
            if expand_keys != keys_with_parent:
                self.logger.warning(
                    f"âš ï¸ [Expandè¯Šæ–­] parent_entityç¼ºå¤±: "
                    f"Expandå‘ç°äº†{expand_keys}ä¸ªkeyï¼Œä½†åªæœ‰{keys_with_parent}ä¸ªæœ‰parent_entityå­—æ®µï¼"
                )

            # ğŸ¨ æ—¥å¿—ï¼šæŒ‰hopç»Ÿè®¡å®ä½“æ•°é‡ï¼ˆç”¨äºå‰ç«¯é¢œè‰²åˆ†å±‚ï¼‰
            hop_stats = {}
            for key in key_final:
                hop = key.get("hop", 0)
                hop_stats[hop] = hop_stats.get(hop, 0) + 1

            self.logger.info("ğŸ¨ [é¢œè‰²åˆ†å±‚] æŒ‰hopç»Ÿè®¡å®ä½“æ•°é‡ (hopè¶Šå¤§ï¼Œå»ºè®®é¢œè‰²è¶Šæµ…):")
            for hop in sorted(hop_stats.keys()):
                hop_name = "Recallé˜¶æ®µ" if hop == 0 else f"ç¬¬{hop}è·³"
                self.logger.info(f"  hop={hop} ({hop_name}): {hop_stats[hop]}ä¸ªå®ä½“")

            # === ğŸ†• ç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level="final") ===
            # ä¸ºæ‰€æœ‰key_finalä¸­çš„å®ä½“ç”Ÿæˆæœ€ç»ˆçº¿ç´¢
            # å‰ç«¯ç²¾ç®€æ¨¡å¼ï¼šåªæ˜¾ç¤ºè¿™äº› final çº¿ç´¢
            # å‰ç«¯å¯ä»¥æ ¹æ® final çº¿ç´¢åæ¨å®Œæ•´è·¯å¾„ï¼ˆåŒ…å« keyâ†’eventâ†’keyï¼‰

            # ğŸ†• åˆ›å»ºç»Ÿä¸€çš„ tracker å®ä¾‹ï¼Œç¡®ä¿æ•´ä¸ª expand é˜¶æ®µä½¿ç”¨åŒä¸€ä¸ªç¼“å­˜
            tracker = Tracker(config)

            if key_final:
                self.logger.info(f"ğŸ¯ [Expand Final] ç”Ÿæˆ {len(key_final)} æ¡æœ€ç»ˆçº¿ç´¢ (display_level=final)")

                # ğŸ†• æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰éœ€è¦çš„ event ä¿¡æ¯ï¼ˆé¿å… N+1 æŸ¥è¯¢ï¼‰
                event_ids_needed = set()
                for key in key_final:
                    steps = key.get("steps", [0])[0]
                    if steps >= 2:
                        parent_info = key_parent_map.get(key["key_id"])
                        if parent_info and "event_id" in parent_info:
                            event_ids_needed.add(parent_info["event_id"])

                # æ‰¹é‡æŸ¥è¯¢ events
                event_map = {}
                if event_ids_needed:
                    try:
                        async with self.session_factory() as session:
                            query = select(SourceEvent).where(SourceEvent.id.in_(list(event_ids_needed)))
                            result = await session.execute(query)
                            events = result.scalars().all()
                            event_map = {event.id: event for event in events}
                            self.logger.info(f"ğŸ“¦ [Expand Final] æ‰¹é‡æŸ¥è¯¢äº† {len(event_map)} ä¸ªeventç”¨äºæ„å»ºfinalçº¿ç´¢")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ [Expand Final] æ‰¹é‡æŸ¥è¯¢eventså¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–çº¿ç´¢")

                # ç»Ÿè®¡çº¿ç´¢ç”Ÿæˆæƒ…å†µ
                final_clues_count = 0
                recall_keys_count = 0
                expand_keys_count = 0
                expand_with_event_count = 0
                expand_without_event_count = 0

                for key in key_final:
                    steps = key.get("steps", [0])[0]

                    if steps == 1:
                        # Recall é˜¶æ®µçš„ keyï¼šç”Ÿæˆ query â†’ entity çº¿ç´¢
                        recall_keys_count += 1

                        entity_dict = {
                            "id": key["key_id"],
                            "key_id": key["key_id"],
                            "name": key["name"],
                            "type": key["type"],
                            "description": key.get("description", ""),
                            "hop": key.get("hop", 0)
                        }

                        # è·å–å®ä½“ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                        entity_similarity = entity_dict.get("similarity", 0.0)
                        # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        entity_weight = key.get("weight")
                        metadata = {
                            "method": "final_result",
                            "step": "recall",
                            "steps": key.get("steps", [1]),
                            "hop": key.get("hop", 0)
                        }
                        # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                        if entity_weight is not None:
                            metadata["weight"] = entity_weight

                        tracker.add_clue(
                            stage="expand",
                            from_node=Tracker.build_query_node(config),
                            to_node=Tracker.build_entity_node(entity_dict),
                            confidence=entity_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                            relation="å¬å›èµ·ç‚¹",
                            display_level="final",  # ğŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æœ
                            metadata=metadata
                        )
                        final_clues_count += 1

                    elif steps >= 2:
                        # Expand é˜¶æ®µçš„ keyï¼šç”Ÿæˆ parent_entity â†’ event â†’ child_entity çº¿ç´¢ï¼ˆä¸¤æ¡ï¼‰
                        expand_keys_count += 1

                        if "parent_entity" in key:
                            parent_entity = key["parent_entity"]

                            parent_entity_dict = {
                                "id": parent_entity["id"],
                                "key_id": parent_entity["id"],
                                "name": parent_entity["name"],
                                "type": parent_entity["type"],
                                "description": parent_entity.get("description", ""),
                                "hop": parent_entity.get("hop", 0)
                            }

                            child_entity_dict = {
                                "id": key["key_id"],
                                "key_id": key["key_id"],
                                "name": key["name"],
                                "type": key["type"],
                                "description": key.get("description", ""),
                                "hop": key.get("hop", 0)
                            }

                            # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰ event_idï¼ˆä» key_parent_map è·å–ï¼‰
                            parent_info = key_parent_map.get(key["key_id"])
                            if parent_info and "event_id" in parent_info:
                                # æœ‰ event_idï¼šç”Ÿæˆä¸¤æ¡çº¿ç´¢ parent_entity â†’ event, event â†’ child_entity
                                event_id = parent_info["event_id"]
                                event_weight = parent_info.get("event_weight", 1.0)
                                current_hop = key.get("hop", 1)  # ğŸ†• è·å–å½“å‰è·³æ•°

                                # ä»æ‰¹é‡æŸ¥è¯¢ç»“æœä¸­è·å– event ä¿¡æ¯
                                event_obj = event_map.get(event_id)
                                if event_obj:
                                    expand_with_event_count += 1

                                    parent_node = Tracker.build_entity_node(parent_entity_dict)
                                    # ğŸ†• ä½¿ç”¨ tracker å®ä¾‹æ–¹æ³•ï¼Œä¼ é€’ stage å’Œ hopï¼Œç¡®ä¿ä¸åŒè·³ç”Ÿæˆä¸åŒèŠ‚ç‚¹
                                    event_node = tracker.get_or_create_event_node(event_obj, stage="expand", hop=current_hop)
                                    child_node = Tracker.build_entity_node(child_entity_dict)

                                    # è·å–å®ä½“ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                                    parent_similarity = parent_entity_dict.get("similarity", 0.0)
                                    child_similarity = child_entity_dict.get("similarity", 0.0)

                                    # ç¬¬ä¸€æ¡çº¿ç´¢ï¼šparent_entity â†’ eventï¼ˆtoèŠ‚ç‚¹æ˜¯äº‹ä»¶ï¼Œä¸å­˜å‚¨weightï¼‰
                                    metadata1 = {
                                        "method": "final_result",
                                        "step": f"expand_hop{current_hop}",
                                        "steps": key.get("steps", [2]),
                                        "hop": current_hop
                                    }

                                    tracker.add_clue(
                                        stage="expand",
                                        from_node=parent_node,
                                        to_node=event_node,
                                        confidence=parent_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                                        relation="å…±ç°äº‹é¡¹",
                                        display_level="final",  # ğŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æœ
                                        metadata=metadata1
                                    )
                                    final_clues_count += 1

                                    # è·å–å­å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                                    child_entity_weight = key.get("weight")
                                    # ç¬¬äºŒæ¡çº¿ç´¢ï¼ševent â†’ child_entityï¼ˆtoèŠ‚ç‚¹æ˜¯å®ä½“ï¼Œéœ€è¦weightï¼‰
                                    metadata2 = {
                                        "method": "final_result",
                                        "step": f"expand_hop{key.get('hop', 1)}",
                                        "steps": key.get("steps", [2]),
                                        "hop": key.get("hop", 1)
                                    }
                                    # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                                    if child_entity_weight is not None:
                                        metadata2["weight"] = child_entity_weight

                                    tracker.add_clue(
                                        stage="expand",
                                        from_node=event_node,
                                        to_node=child_node,
                                        confidence=child_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                                        relation="æ‰©å±•å‘ç°",
                                        display_level="final",  # ğŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æœ
                                        metadata=metadata2
                                    )
                                    final_clues_count += 1
                                else:
                                    # Event ä¸å­˜åœ¨ï¼Œfallback åˆ°ç›´æ¥è¿æ¥
                                    expand_without_event_count += 1
                                    self.logger.warning(
                                        f"âš ï¸ [Expand Final] Event {event_id[:8]} æœªåœ¨æ‰¹é‡æŸ¥è¯¢ä¸­æ‰¾åˆ°ï¼Œä½¿ç”¨ç›´æ¥è¿æ¥"
                                    )
                                    # è·å–å®ä½“ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                                    parent_similarity = parent_entity_dict.get("similarity", 0.0)
                                    child_similarity = child_entity_dict.get("similarity", 0.0)
                                    # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                                    avg_similarity = (parent_similarity + child_similarity) / 2.0

                                    # è·å–å­å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                                    child_entity_weight = key.get("weight")
                                    metadata = {
                                        "method": "final_result",
                                        "step": f"expand_hop{key.get('hop', 1)}",
                                        "steps": key.get("steps", [2]),
                                        "hop": key.get("hop", 1)
                                    }
                                    # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                                    if child_entity_weight is not None:
                                        metadata["weight"] = child_entity_weight

                                    tracker.add_clue(
                                        stage="expand",
                                        from_node=Tracker.build_entity_node(parent_entity_dict),
                                        to_node=Tracker.build_entity_node(child_entity_dict),
                                        confidence=avg_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                                        relation="æ‰©å±•å‘ç°",
                                        display_level="final",
                                        metadata=metadata
                                    )
                                    final_clues_count += 1
                            else:
                                # æ²¡æœ‰ event_idï¼šç›´æ¥ç”Ÿæˆ parent_entity â†’ child_entity çº¿ç´¢
                                expand_without_event_count += 1
                                # è·å–å®ä½“ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                                parent_similarity = parent_entity_dict.get("similarity", 0.0)
                                child_similarity = child_entity_dict.get("similarity", 0.0)
                                # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                                avg_similarity = (parent_similarity + child_similarity) / 2.0

                                # è·å–å­å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                                child_entity_weight = key.get("weight")
                                metadata = {
                                    "method": "final_result",
                                    "step": f"expand_hop{key.get('hop', 1)}",
                                    "steps": key.get("steps", [2]),
                                    "hop": key.get("hop", 1)
                                }
                                # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                                if child_entity_weight is not None:
                                    metadata["weight"] = child_entity_weight

                                tracker.add_clue(
                                    stage="expand",
                                    from_node=Tracker.build_entity_node(parent_entity_dict),
                                    to_node=Tracker.build_entity_node(child_entity_dict),
                                    confidence=avg_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                                    relation="æ‰©å±•å‘ç°",
                                    display_level="final",  # ğŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æœ
                                    metadata=metadata
                                )
                                final_clues_count += 1
                        else:
                            self.logger.warning(
                                f"âš ï¸ [Expand Final] Expand key_id={key['key_id']} ç¼ºå°‘ parent_entityï¼Œæ— æ³•ç”Ÿæˆæœ€ç»ˆçº¿ç´¢"
                            )

                self.logger.info(
                    f"âœ… [Expand Final] æœ€ç»ˆçº¿ç´¢ç”Ÿæˆå®Œæˆ: "
                    f"å…± {final_clues_count} æ¡çº¿ç´¢ "
                    f"(Recall keys={recall_keys_count}, "
                    f"Expand keys={expand_keys_count}, "
                    f"åŒ…å«event={expand_with_event_count}, "
                    f"ä¸å«event={expand_without_event_count})"
                )

                # ğŸ†• ä¸ºç¬¬ä¸€è·³ä¸­æ²¡æœ‰æ‰©å±•å‡ºæ–°å®ä½“çš„ recall keys ä¹Ÿç”Ÿæˆ final çº¿ç´¢
                if no_expansion_recall_keys:
                    self.logger.info(
                        f"ğŸƒ [Expand Final] ä¸º {len(no_expansion_recall_keys)} ä¸ªæ²¡æœ‰æ‰©å±•çš„recall keyç”Ÿæˆfinalçº¿ç´¢"
                    )

                    # æŸ¥è¯¢è¿™äº› keys çš„å®ä½“ä¿¡æ¯
                    try:
                        async with self.session_factory() as session:
                            query = select(Entity).where(Entity.id.in_(no_expansion_recall_keys))
                            result = await session.execute(query)
                            no_expansion_entities = {entity.id: entity for entity in result.scalars().all()}

                        for key_id in no_expansion_recall_keys:
                            entity = no_expansion_entities.get(key_id)
                            if not entity:
                                self.logger.warning(f"âš ï¸ æ— æ³•æŸ¥è¯¢åˆ°å®ä½“ {key_id}ï¼Œè·³è¿‡")
                                continue

                            # ä» recall ç»“æœä¸­è·å–æƒé‡
                            weight = key_final_weights.get(key_id, 0.0)
                            # è·å–å®ä½“ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
                            entity_similarity = entity_dict.get("similarity", 0.0) if hasattr(entity, 'similarity') else 0.0

                            entity_dict = {
                                "id": key_id,
                                "key_id": key_id,
                                "name": entity.name,
                                "type": entity.type,
                                "description": entity.description or "",
                                "hop": 0,  # recall é˜¶æ®µ hop=0
                                "similarity": entity_similarity  # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯
                            }

                            # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                            entity_weight = weight  # weightå°±æ˜¯å®ä½“æƒé‡
                            metadata = {
                                "method": "final_result",
                                "step": "recall_no_expansion",
                                "steps": [1],
                                "hop": 0,
                                "is_leaf": True  # ğŸ†• æ ‡è®°ä¸ºå¶å­èŠ‚ç‚¹ï¼ˆæ²¡æœ‰æ‰©å±•ï¼‰
                            }
                            # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                            if entity_weight > 0:
                                metadata["weight"] = entity_weight

                            tracker.add_clue(
                                stage="expand",
                                from_node=Tracker.build_query_node(config),
                                to_node=Tracker.build_entity_node(entity_dict),
                                confidence=entity_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                                relation="å¬å›ç»ˆç‚¹",  # ğŸ†• æ ‡è®°ä¸ºæ²¡æœ‰ç»§ç»­æ‰©å±•çš„ç»ˆç‚¹
                                display_level="final",
                                metadata=metadata
                            )
                            final_clues_count += 1

                        self.logger.info(
                            f"âœ… [Expand Final] æ²¡æœ‰æ‰©å±•çš„recall keyå¤„ç†å®Œæˆï¼Œæ–°å¢ {len(no_expansion_recall_keys)} æ¡ç»ˆç‚¹çº¿ç´¢"
                        )

                    except Exception as e:
                        self.logger.error(f"âš ï¸ [Expand Final] ä¸ºæ²¡æœ‰æ‰©å±•çš„recall keyç”Ÿæˆçº¿ç´¢å¤±è´¥: {e}", exc_info=True)

            # === æ„å»ºExpandé˜¶æ®µçº¿ç´¢ ===
            expand_clues = await self._build_expand_clues(config, key_final, key_parent_map, tracker)
            config.expansion_clues = expand_clues
            self.logger.info(f"âœ¨ Expandçº¿ç´¢å·²æ„å»º (entityâ†’eventâ†’entityæ‹†åˆ†ä¸º2æ¡çº¿ç´¢)")

            self.logger.info(
                f"Expandæœç´¢å®Œæˆï¼šå®é™…è·³è·ƒ {len(jump_results)} æ¬¡ï¼Œæ€»å…±å‘ç° {len(key_final)} ä¸ªå”¯ä¸€keys"
            )

            return ExpandResult(
                key_final=key_final,
                jump_results=jump_results,
                total_jumps=len(jump_results),
                convergence_reached=weight_change < config.expand.weight_change_threshold if jump_results else False,
                all_events_by_jump=all_events_by_jump,
                all_keys_by_jump=all_keys_by_jump,
                weight_evolution=weight_evolution,
            )

        except Exception as e:
            self.logger.error(f"Expandæœç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    # === æ­¥éª¤å®ç°æ–¹æ³• ===

    async def _diagnose_key_final_event_filtering(
        self,
        key_ids: List[str],
        key_weights: Dict[str, float],
        all_event_ids: List[str],
        filtered_event_weights: Dict[str, float]
    ) -> None:
        """
        è¯Šæ–­ key-final å„ä¸ª key å¬å›çš„ events è¿‡æ»¤æƒ…å†µï¼ˆä»…ç¬¬1è·³è°ƒç”¨ï¼‰

        Args:
            key_ids: key-final çš„ key_ids
            key_weights: key æƒé‡å­—å…¸
            all_event_ids: æ­¥éª¤1å¬å›çš„å…¨éƒ¨ events
            filtered_event_weights: æ­¥éª¤2è¿‡æ»¤åçš„ events æƒé‡å­—å…¸
        """
        self.logger.info(f"ğŸ“Š [Key-Finalè¿‡æ»¤è¯Šæ–­] å¼€å§‹åˆ†æ {len(key_ids)} ä¸ªkeyçš„eventsè¿‡æ»¤æƒ…å†µ")

        try:
            async with self.session_factory() as session:
                # æŸ¥è¯¢ key-event å…³ç³»
                query = (
                    select(EventEntity.entity_id, EventEntity.event_id)
                    .where(EventEntity.entity_id.in_(key_ids))
                    .where(EventEntity.event_id.in_(all_event_ids))
                )
                result = await session.execute(query)
                relations = result.fetchall()

                # æ„å»ºæ˜ å°„: key_id -> [event_ids]
                key_to_events = {}
                for entity_id, event_id in relations:
                    if entity_id not in key_to_events:
                        key_to_events[entity_id] = []
                    key_to_events[entity_id].append(event_id)

                # æŸ¥è¯¢ key åç§°
                entity_query = select(Entity).where(Entity.id.in_(key_ids))
                entity_result = await session.execute(entity_query)
                entities = {entity.id: entity for entity in entity_result.scalars().all()}

            # ç»Ÿè®¡è¿‡æ»¤æƒ…å†µ
            filtered_event_ids = set(filtered_event_weights.keys())
            fully_filtered_keys = []  # å®Œå…¨è¢«è¿‡æ»¤çš„ keys
            partially_filtered_keys = []  # éƒ¨åˆ†è¢«è¿‡æ»¤çš„ keys
            no_events_keys = []  # æ²¡æœ‰å¬å› events çš„ keys

            for key_id in key_ids:
                entity = entities.get(key_id)
                key_name = entity.name if entity else key_id[:8]
                key_type = entity.type if entity else "unknown"
                weight = key_weights.get(key_id, 0)

                recalled_events = set(key_to_events.get(key_id, []))
                retained_events = recalled_events & filtered_event_ids

                recall_count = len(recalled_events)
                retain_count = len(retained_events)

                if recall_count == 0:
                    no_events_keys.append((key_name, key_type, weight))
                    self.logger.warning(
                        f"  âš ï¸  [{key_type}] {key_name}: æœªå¬å›ä»»ä½•events (weight={weight:.3f})"
                    )
                elif retain_count == 0:
                    fully_filtered_keys.append((key_name, key_type, weight, recall_count))
                    self.logger.warning(
                        f"  ğŸš« [{key_type}] {key_name}: å¬å›{recall_count}ä¸ªevents, å…¨éƒ¨è¢«è¿‡æ»¤ (ç›¸ä¼¼åº¦å‡<0.3, weight={weight:.3f})"
                    )
                else:
                    filter_rate = (recall_count - retain_count) / recall_count
                    if filter_rate > 0.5:
                        partially_filtered_keys.append((key_name, key_type, weight, recall_count, retain_count, filter_rate))
                        self.logger.info(
                            f"  âš ï¸  [{key_type}] {key_name}: å¬å›{recall_count}ä¸ª, ä¿ç•™{retain_count}ä¸ª, è¿‡æ»¤{filter_rate:.1%} (weight={weight:.3f})"
                        )
                    else:
                        self.logger.info(
                            f"  âœ… [{key_type}] {key_name}: å¬å›{recall_count}ä¸ª, ä¿ç•™{retain_count}ä¸ª, è¿‡æ»¤{filter_rate:.1%} (weight={weight:.3f})"
                        )

            # æ±‡æ€»æŠ¥å‘Š
            self.logger.info(f"ğŸ“Š [Key-Finalè¿‡æ»¤è¯Šæ–­] æ±‡æ€»:")
            self.logger.info(f"  â€¢ æ€»keyæ•°: {len(key_ids)}")
            self.logger.info(f"  â€¢ æœªå¬å›events: {len(no_events_keys)}ä¸ª")
            self.logger.info(f"  â€¢ eventså…¨éƒ¨è¢«è¿‡æ»¤: {len(fully_filtered_keys)}ä¸ª")
            self.logger.info(f"  â€¢ eventséƒ¨åˆ†è¢«è¿‡æ»¤(>50%): {len(partially_filtered_keys)}ä¸ª")

            if fully_filtered_keys:
                self.logger.warning(
                    f"âš ï¸ [Key-Finalè¿‡æ»¤è¯Šæ–­] {len(fully_filtered_keys)}ä¸ªkeyçš„eventså…¨éƒ¨è¢«è¿‡æ»¤ï¼Œæ— æ³•æ‰©å±•æ–°å®ä½“"
                )

        except Exception as e:
            self.logger.error(f"Key-Finalè¿‡æ»¤è¯Šæ–­å¤±è´¥: {e}", exc_info=True)

    async def _diagnose_key_expansion_success(
        self,
        parent_key_ids: List[str],
        parent_key_weights: Dict[str, float],
        key_expansion_trace: Dict[str, List[Tuple[str, str, float]]],
        all_new_key_weights: Dict[str, float],
        top_new_keys: List[Tuple[str, float]],
        already_discovered: set,
        entities_per_hop: int
    ) -> List[str]:
        """
        è¯Šæ–­æ¯ä¸ª parent key æ‰©å±•å‡ºçš„ child entities æƒ…å†µï¼ˆä»…ç¬¬1è·³è°ƒç”¨ï¼‰

        Args:
            parent_key_ids: parent keys åˆ—è¡¨
            parent_key_weights: parent keys æƒé‡
            key_expansion_trace: æ‰©å±•è¿½è¸ªä¿¡æ¯ {child_id: [(parent_id, event_id, weight), ...]}
            all_new_key_weights: æ‰€æœ‰æ–°å‘ç° entities çš„æƒé‡
            top_new_keys: é€‰ä¸­çš„ topkey åˆ—è¡¨
            already_discovered: å·²å‘ç°çš„ keys é›†åˆ
            entities_per_hop: æ¯è·³é€‰æ‹©çš„ entities æ•°é‡

        Returns:
            æ²¡æœ‰æ‰©å±•å‡ºæ–°å®ä½“çš„ parent key IDs åˆ—è¡¨
        """
        self.logger.info(f"ğŸ“Š [Keyæ‰©å±•è¯Šæ–­] å¼€å§‹åˆ†æ {len(parent_key_ids)} ä¸ªparent keyçš„æ‰©å±•æƒ…å†µ")

        try:
            # 1. åå‘æ„å»ºæ˜ å°„ï¼šparent_id -> [child_ids]
            parent_to_children = {}
            for child_id, expansion_paths in key_expansion_trace.items():
                for parent_id, event_id, weight in expansion_paths:
                    if parent_id not in parent_to_children:
                        parent_to_children[parent_id] = set()
                    parent_to_children[parent_id].add(child_id)

            # 2. topkey é›†åˆ
            top_key_ids = {key_id for key_id, _ in top_new_keys}

            # 3. æŸ¥è¯¢å®ä½“åç§°
            async with self.session_factory() as session:
                entity_query = select(Entity).where(Entity.id.in_(parent_key_ids))
                entity_result = await session.execute(entity_query)
                entities = {entity.id: entity for entity in entity_result.scalars().all()}

            # 4. ç»Ÿè®¡æ¯ä¸ª parent key çš„æ‰©å±•æƒ…å†µ
            no_expansion_keys = []  # æ²¡æœ‰æ‰©å±•å‡ºä»»ä½• child çš„ keys
            all_filtered_keys = []  # æ‰©å±•äº† child ä½†å…¨éƒ¨æœªè¿›å…¥ topkey çš„ keys
            partial_filtered_keys = []  # éƒ¨åˆ† child è¿›å…¥ topkey çš„ keys
            success_keys = []  # æˆåŠŸæ‰©å±•çš„ keys

            for parent_id in parent_key_ids:
                entity = entities.get(parent_id)
                key_name = entity.name if entity else parent_id[:8]
                key_type = entity.type if entity else "unknown"
                weight = parent_key_weights.get(parent_id, 0)

                # è·å–è¯¥ parent æ‰©å±•å‡ºçš„ children
                children = parent_to_children.get(parent_id, set())
                children_count = len(children)

                if children_count == 0:
                    # æ²¡æœ‰æ‰©å±•å‡ºä»»ä½• child
                    no_expansion_keys.append((key_name, key_type, weight))
                    self.logger.warning(
                        f"  ğŸš« [{key_type}] {key_name}: æœªæ‰©å±•å‡ºä»»ä½•æ–°å®ä½“ (weight={weight:.3f})"
                    )
                else:
                    # æ‰©å±•å‡ºäº† childrenï¼Œæ£€æŸ¥æœ‰å¤šå°‘è¿›å…¥ topkey
                    children_in_top = children & top_key_ids
                    children_not_in_top = children - top_key_ids

                    top_count = len(children_in_top)
                    not_top_count = len(children_not_in_top)

                    if top_count == 0:
                        # æ‰€æœ‰ children éƒ½æœªè¿›å…¥ topkey
                        all_filtered_keys.append((key_name, key_type, weight, children_count))

                        # æ˜¾ç¤ºè¢«è¿‡æ»¤çš„ children åŠå…¶æƒé‡ï¼ˆæ˜¾ç¤ºå‰3ä¸ªæƒé‡æœ€é«˜çš„ï¼‰
                        filtered_children_weights = [(cid, all_new_key_weights.get(cid, 0)) for cid in children_not_in_top]
                        filtered_children_weights.sort(key=lambda x: x[1], reverse=True)
                        top3_filtered = filtered_children_weights[:3]

                        # è®¡ç®—è¿™äº› children çš„æ’å
                        all_sorted = sorted(all_new_key_weights.items(), key=lambda x: x[1], reverse=True)
                        ranks = [i+1 for i, (kid, w) in enumerate(all_sorted) if kid in children_not_in_top]
                        min_rank = min(ranks) if ranks else "N/A"

                        self.logger.warning(
                            f"  âš ï¸  [{key_type}] {key_name}: æ‰©å±•äº†{children_count}ä¸ªæ–°å®ä½“, ä½†å…¨éƒ¨æœªè¿›å…¥Top{entities_per_hop} "
                            f"(æœ€é«˜æ’å={min_rank}, weight={weight:.3f})"
                        )

                        # æ˜¾ç¤ºè¢«è¿‡æ»¤çš„ top3 children
                        for i, (child_id, child_weight) in enumerate(top3_filtered, 1):
                            # æŸ¥æ‰¾è¯¥ child çš„æ’å
                            rank = next((i+1 for i, (kid, w) in enumerate(all_sorted) if kid == child_id), "N/A")
                            self.logger.debug(
                                f"    - è¢«è¿‡æ»¤çš„child#{i}: {child_id[:8]}... (weight={child_weight:.4f}, æ’å={rank})"
                            )
                    else:
                        # éƒ¨åˆ†æˆ–å…¨éƒ¨è¿›å…¥ topkey
                        if not_top_count > 0:
                            partial_filtered_keys.append((key_name, key_type, weight, children_count, top_count, not_top_count))
                            self.logger.info(
                                f"  âš ï¸  [{key_type}] {key_name}: æ‰©å±•äº†{children_count}ä¸ª, {top_count}ä¸ªè¿›å…¥Top{entities_per_hop}, "
                                f"{not_top_count}ä¸ªè¢«è¿‡æ»¤ (weight={weight:.3f})"
                            )
                            # æ˜¾ç¤ºè¿›å…¥Top10çš„å­å®ä½“ID
                            top_children_list = sorted(
                                [(cid, all_new_key_weights.get(cid, 0)) for cid in children_in_top],
                                key=lambda x: x[1],
                                reverse=True
                            )
                            self.logger.info(
                                f"    è¿›å…¥Top{entities_per_hop}çš„å­å®ä½“: {[cid[:8] for cid, _ in top_children_list]}"
                            )
                        else:
                            success_keys.append((key_name, key_type, weight, children_count))
                            self.logger.info(
                                f"  âœ… [{key_type}] {key_name}: æ‰©å±•äº†{children_count}ä¸ªæ–°å®ä½“, å…¨éƒ¨è¿›å…¥Top{entities_per_hop} (weight={weight:.3f})"
                            )
                            # æ˜¾ç¤ºå…¨éƒ¨è¿›å…¥Top10çš„å­å®ä½“ID
                            top_children_list = sorted(
                                [(cid, all_new_key_weights.get(cid, 0)) for cid in children_in_top],
                                key=lambda x: x[1],
                                reverse=True
                            )
                            self.logger.info(
                                f"    è¿›å…¥Top{entities_per_hop}çš„å­å®ä½“: {[cid[:8] for cid, _ in top_children_list]}"
                            )

            # 5. æ±‡æ€»æŠ¥å‘Š
            self.logger.info(f"ğŸ“Š [Keyæ‰©å±•è¯Šæ–­] æ±‡æ€»:")
            self.logger.info(f"  â€¢ æ€»parent keyæ•°: {len(parent_key_ids)}")
            self.logger.info(f"  â€¢ æœªæ‰©å±•å‡ºæ–°å®ä½“: {len(no_expansion_keys)}ä¸ª")
            self.logger.info(f"  â€¢ æ‰©å±•çš„æ–°å®ä½“å…¨éƒ¨è¢«è¿‡æ»¤: {len(all_filtered_keys)}ä¸ª")
            self.logger.info(f"  â€¢ æ‰©å±•çš„æ–°å®ä½“éƒ¨åˆ†è¢«è¿‡æ»¤: {len(partial_filtered_keys)}ä¸ª")
            self.logger.info(f"  â€¢ æˆåŠŸæ‰©å±•(å…¨éƒ¨è¿›å…¥topkey): {len(success_keys)}ä¸ª")

            if no_expansion_keys or all_filtered_keys:
                failed_count = len(no_expansion_keys) + len(all_filtered_keys)
                self.logger.warning(
                    f"âš ï¸ [Keyæ‰©å±•è¯Šæ–­] {failed_count}ä¸ªparent keyæœªæˆåŠŸæ‰©å±•æˆ–æ‰©å±•çš„å®ä½“å…¨éƒ¨è¢«è¿‡æ»¤ï¼Œ"
                    f"è¿™äº›keyä¸ä¼šå‡ºç°åœ¨Expandçº¿ç´¢ä¸­"
                )

            # è¿”å›æ²¡æœ‰æ‰©å±•å‡ºæ–°å®ä½“çš„ parent key IDs
            no_expansion_key_ids = [parent_id for parent_id in parent_key_ids
                                   if len(parent_to_children.get(parent_id, set())) == 0]
            return no_expansion_key_ids

        except Exception as e:
            self.logger.error(f"Keyæ‰©å±•è¯Šæ–­å¤±è´¥: {e}", exc_info=True)
            return []

    async def _step1_keys_to_events(self, key_ids: List[str]) -> List[str]:
        """
        æ­¥éª¤1: æ ¹æ®keysæŸ¥æ‰¾åˆ°æ‰€æœ‰å…³è”çš„events
        ç”¨sqlæ‰¾åˆ°æ‰€æœ‰å…³è”äº‹é¡¹ï¼Œå¾—åˆ°æ–°çš„[Event-key-related-2]
        """
        if not key_ids:
            return []

        self.logger.info(f"æ­¥éª¤1: æ ¹æ®keysæŸ¥æ‰¾å…³è”events")
        self.logger.info(f"  è¾“å…¥keysæ•°é‡: {len(key_ids)}")

        async with self.session_factory() as session:
            # æŸ¥è¯¢åŒ…å«è¿™äº›keyçš„æ‰€æœ‰event
            query = (
                select(EventEntity.event_id)
                .where(EventEntity.entity_id.in_(key_ids))
                .distinct()
            )

            result = await session.execute(query)
            event_ids = [row[0] for row in result.fetchall()]

        # è°ƒè¯•ï¼šè¾“å‡ºå…³è”çš„event_ids
        self.logger.info(f"æ­¥éª¤1å®Œæˆ:")
        self.logger.info(f"  â€¢ è¾“å…¥keysæ•°é‡: {len(key_ids)}")
        self.logger.info(f"  â€¢ å‘ç°å…³è”eventsæ•°é‡: {len(event_ids)}")
        events_preview = event_ids[:3] if len(event_ids) > 3 else event_ids
        events_suffix = "..." if len(event_ids) > 3 else ""
        self.logger.info(f"  â€¢ å…³è”events: {events_preview}{events_suffix}")

        return event_ids

    async def _step2_calculate_event_query_similarity(
        self, config: SearchConfig, event_ids: List[str]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        æ­¥éª¤2: è®¡ç®—åŸå§‹queryå’Œç»™å®ševentsçš„ç›¸ä¼¼åº¦
        å¾—åˆ°ç›¸ä¼¼æ€§å‘é‡(event-query-2)

        æ³¨æ„ï¼šè¿™é‡Œè®¡ç®—çš„æ˜¯ç»™å®ševentsä¸queryçš„ç›¸ä¼¼åº¦ï¼Œè€Œä¸æ˜¯é‡æ–°æœç´¢ç›¸ä¼¼events
        """
        if not event_ids:
            return [], {}

        self.logger.info(f"æ­¥éª¤2: è®¡ç®—åŸå§‹queryä¸ {len(event_ids)} ä¸ªç»™å®ševentsçš„ç›¸ä¼¼åº¦")

        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_embedding
            if config.has_query_embedding and config.query_embedding:
                query_embedding = config.query_embedding
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œé•¿åº¦: {len(query_embedding)}")
            else:
                # ç”ŸæˆåŸå§‹queryçš„å‘é‡
                query_embedding = await self.processor.generate_embedding(config.query)
                self.logger.debug(f"  Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(query_embedding)}")

                # ç¼“å­˜query_embeddingåˆ°config
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")
        except Exception as e:
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

        self.logger.debug(f"  Queryå‘é‡é•¿åº¦: {len(query_embedding)}")

        # å­˜å‚¨è®¡ç®—ç»“æœ
        event_query_related = []
        event_similarities = {}

        # ç»Ÿè®¡ä¿¡æ¯
        successful_calculations = 0
        failed_calculations = 0
        below_threshold_count = 0

        # åˆ†æ‰¹å¤„ç†eventsï¼Œé¿å…ä¸€æ¬¡æ€§æŸ¥è¯¢è¿‡å¤š
        batch_size = 50  # âœ… ä¼˜åŒ–ï¼šæé«˜æ‰¹é‡å¤§å°ï¼Œå‡å°‘æŸ¥è¯¢æ¬¡æ•°
        for i in range(0, len(event_ids), batch_size):
            batch_event_ids = event_ids[i:i + batch_size]

            self.logger.debug(f"  å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_event_ids)} ä¸ªevents")

            # è·å–è¿™æ‰¹äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«å‘é‡ï¼‰
            try:
                batch_events = await self.event_repo.get_events_by_ids(batch_event_ids)

                # ä¸ºæŸ¥æ‰¾æ–¹ä¾¿ï¼Œåˆ›å»ºevent_idåˆ°eventæ•°æ®çš„æ˜ å°„
                event_map = {}
                for event in batch_events:
                    if isinstance(event, dict) and "event_id" in event:
                        event_id = event["event_id"]
                        event_map[event_id] = event
                    else:
                        # è®°å½•æ ¼å¼é”™è¯¯çš„äº‹ä»¶æ•°æ®
                        self.logger.warning(f"äº‹ä»¶æ•°æ®æ ¼å¼é”™è¯¯æˆ–ç¼ºå°‘event_idå­—æ®µ: {type(event)}")
                        if isinstance(event, dict):
                            self.logger.debug(f"äº‹ä»¶å­—æ®µ: {list(event.keys())}")

            except Exception as e:
                self.logger.warning(f"è·å–æ‰¹æ¬¡äº‹ä»¶ä¿¡æ¯å¤±è´¥: {e}")
                event_map = {}

            # âœ… ä¼˜åŒ–ï¼šæ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„å‘é‡ï¼Œå‡†å¤‡æ‰¹é‡è®¡ç®—
            valid_event_data = []
            for event_id in batch_event_ids:
                try:
                    # è·å–äº‹ä»¶è¯¦ç»†ä¿¡æ¯
                    event_info = event_map.get(event_id, {})

                    # ç¡®ä¿event_infoæ˜¯å­—å…¸ç±»å‹
                    if not isinstance(event_info, dict):
                        event_info = {}

                    # âœ… ä¼˜åŒ–ï¼šç›´æ¥ä»æ‰¹é‡æŸ¥è¯¢ç»“æœä¸­è·å–å‘é‡ï¼Œä¸å†è°ƒç”¨ get_event_vector
                    content_vector = event_info.get("content_vector")
                    title_vector = event_info.get("title_vector")

                    # ä¼˜å…ˆä½¿ç”¨ content_vectorï¼Œå…¶æ¬¡ä½¿ç”¨ title_vector
                    event_vector = content_vector or title_vector
                    vector_type = "content_vector" if content_vector else "title_vector"

                    if event_vector is None:
                        self.logger.debug(f"    Event {event_id[:8]}: æ— å‘é‡æ•°æ®")
                        failed_calculations += 1
                        continue

                    # æ”¶é›†æœ‰æ•ˆçš„äº‹ä»¶æ•°æ®
                    valid_event_data.append({
                        'event_id': event_id,
                        'vector': event_vector,
                        'title': event_info.get("title", ""),
                        'summary': event_info.get("summary", ""),
                        'match_type': vector_type
                    })

                except Exception as e:
                    self.logger.warning(f"    âŒ Event {event_id[:8]}: å¤„ç†å¤±è´¥: {e}")
                    failed_calculations += 1
                    continue

            # âœ… ä¼˜åŒ–ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
            if valid_event_data:
                vectors = [item['vector'] for item in valid_event_data]
                similarities = await self._batch_cosine_similarity(query_embedding, vectors)

                # å¤„ç†æ‰¹é‡è®¡ç®—ç»“æœ
                for item, similarity in zip(valid_event_data, similarities):
                    event_data = {
                        "event_id": item['event_id'],
                        "title": item['title'],
                        "summary": item['summary'],
                        "similarity": float(similarity),
                        "match_type": item['match_type'],
                    }

                    event_query_related.append(event_data)
                    event_similarities[item['event_id']] = float(similarity)
                    successful_calculations += 1

                    # è°ƒè¯•ä¿¡æ¯
                    if similarity >= config.expand.event_similarity_threshold:
                        self.logger.debug(f"    âœ… Event {item['event_id'][:8]}: ç›¸ä¼¼åº¦={similarity:.4f} (è¶…è¿‡é˜ˆå€¼)")
                    else:
                        below_threshold_count += 1
                        self.logger.debug(f"    âš ï¸  Event {item['event_id'][:8]}: ç›¸ä¼¼åº¦={similarity:.4f} (ä½äºé˜ˆå€¼)")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"æ­¥éª¤2ç›¸ä¼¼åº¦è®¡ç®—ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ æ€»äº‹ä»¶æ•°: {len(event_ids)}")
        self.logger.info(f"  â€¢ æˆåŠŸè®¡ç®—: {successful_calculations}")
        self.logger.info(f"  â€¢ è®¡ç®—å¤±è´¥: {failed_calculations}")
        self.logger.info(f"  â€¢ ä½äºé˜ˆå€¼: {below_threshold_count}")

        if event_similarities:
            self.logger.info(f"  â€¢ ç›¸ä¼¼åº¦èŒƒå›´: {min(event_similarities.values()):.4f} - {max(event_similarities.values()):.4f}")

        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        before_threshold = len(event_query_related)
        event_query_related = [
            event for event in event_query_related
            if event["similarity"] >= config.expand.event_similarity_threshold
        ]
        event_similarities = {
            event_id: similarity
            for event_id, similarity in event_similarities.items()
            if similarity >= config.expand.event_similarity_threshold
        }

        self.logger.info(f"æ­¥éª¤2å®Œæˆ: æ‰¾åˆ° {len(event_query_related)} ä¸ªç›¸ä¼¼äº‹ä»¶ (é˜ˆå€¼è¿‡æ»¤å‰: {before_threshold})")

        return event_query_related, event_similarities

    async def _step3_calculate_event_key_weights(
        self,
        event_ids: List[str],
        key_ids: List[str],
        key_weights: Dict[str, float],
    ) -> Dict[str, float]:
        """
        æ­¥éª¤3: è®¡ç®—Event-key-related-2æƒé‡å‘é‡
        æ ¹æ®æ¯ä¸ªeventåŒ…å«keyçš„æƒ…å†µï¼Œå°†å¯¹åº”keyçš„æƒé‡ç›¸åŠ 

        ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ + å†…å­˜åˆ†ç»„ï¼Œé¿å…å¾ªç¯æŸ¥è¯¢æ•°æ®åº“
        """
        if not event_ids or not key_ids:
            return {}

        event_key_weights = {}

        try:
            async with self.session_factory() as session:
                # ğŸ”¥ ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰event-keyå…³ç³»
                query = (
                    select(EventEntity.event_id, EventEntity.entity_id)
                    .where(EventEntity.event_id.in_(event_ids))
                    .where(EventEntity.entity_id.in_(key_ids))
                )
                result = await session.execute(query)
                all_relations = result.fetchall()

                # ğŸ”¥ ä¼˜åŒ–ï¼šåœ¨å†…å­˜ä¸­æŒ‰event_idåˆ†ç»„
                event_to_keys = {}
                for event_id, entity_id in all_relations:
                    if event_id not in event_to_keys:
                        event_to_keys[event_id] = []
                    event_to_keys[event_id].append(entity_id)

                # è®¡ç®—æ¯ä¸ªeventçš„æƒé‡
                for event_id in event_ids:
                    event_keys = event_to_keys.get(event_id, [])

                    # è®¡ç®—æƒé‡ï¼šå°†å¯¹åº”keyçš„æƒé‡ç›¸åŠ 
                    total_weight = sum(key_weights.get(key_id, 0.0) for key_id in event_keys)
                    event_key_weights[event_id] = total_weight

        except Exception as e:
            self.logger.error(f"æ­¥éª¤3è®¡ç®—event-keyæƒé‡å¤±è´¥: {e}", exc_info=True)
            raise

        return event_key_weights

    async def _step4_calculate_event_key_query_weights(
        self,
        event_key_weights: Dict[str, float],
        event_query_weights: Dict[str, float],
    ) -> Dict[str, float]:
        """
        æ­¥éª¤4: è®¡ç®—event-key-queryæƒé‡å‘é‡
        å°†ï¼ˆevent-key-2ï¼‰*(event-query-2)ï¼Œå¾—åˆ°æ–°çš„ï¼ˆevent-jump-2ï¼‰
        """
        event_key_query_weights = {}

        # è°ƒè¯•ï¼šè¾“å‡ºæƒé‡è®¡ç®—çš„è¯¦ç»†ä¿¡æ¯
        self.logger.debug(f"æ­¥éª¤4è°ƒè¯•ä¿¡æ¯:")
        self.logger.debug(f"  event_key_weightsæ•°é‡: {len(event_key_weights)}")
        self.logger.debug(f"  event_query_weightsæ•°é‡: {len(event_query_weights)}")

        # ç»Ÿè®¡åŒ¹é…æƒ…å†µ
        matched_events = 0
        unmatched_events = 0
        zero_weight_events = 0

        for event_id in event_key_weights:
            key_weight = event_key_weights[event_id]
            query_weight = event_query_weights.get(event_id, 0.0)

            # å¤åˆæƒé‡ = keyæƒé‡ * queryç›¸ä¼¼åº¦æƒé‡
            combined_weight = key_weight * query_weight
            event_key_query_weights[event_id] = combined_weight

            # è°ƒè¯•ä¿¡æ¯
            if query_weight > 0:
                matched_events += 1
                self.logger.debug(f"  âœ… Event {event_id[:8]}: key_weight={key_weight:.4f}, query_weight={query_weight:.4f}, combined={combined_weight:.4f}")
            else:
                unmatched_events += 1
                if combined_weight == 0:
                    zero_weight_events += 1
                self.logger.debug(f"  âŒ Event {event_id[:8]}: key_weight={key_weight:.4f}, query_weight={query_weight:.4f}, combined={combined_weight:.4f}")

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.logger.info(f"æ­¥éª¤4æƒé‡è®¡ç®—ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ æ€»äº‹ä»¶æ•°: {len(event_key_weights)}")
        self.logger.info(f"  â€¢ åŒ¹é…æˆåŠŸçš„äº‹ä»¶: {matched_events}")
        self.logger.info(f"  â€¢ æœªåŒ¹é…çš„äº‹ä»¶: {unmatched_events}")
        self.logger.info(f"  â€¢ æƒé‡ä¸º0çš„äº‹ä»¶: {zero_weight_events}")

        # å¦‚æœæƒé‡ä¸º0çš„äº‹ä»¶è¿‡å¤šï¼Œå‘å‡ºè­¦å‘Š
        if zero_weight_events > len(event_key_weights) * 0.8:
            self.logger.warning(f"âš ï¸ æ­¥éª¤4ä¸­æœ‰ {zero_weight_events}/{len(event_key_weights)} ä¸ªäº‹ä»¶æƒé‡ä¸º0ï¼Œå¯èƒ½å½±å“æœç´¢æ•ˆæœ")

        # æƒé‡å½’ä¸€åŒ–å’Œå®¹é”™å¤„ç†
        if event_key_query_weights:
            # è¿‡æ»¤æ‰æƒé‡ä¸º0çš„äº‹ä»¶
            non_zero_weights = {k: v for k, v in event_key_query_weights.items() if v > 0}

            if non_zero_weights:
                # å¯¹æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œé¿å…æ•°å€¼è¿‡å°
                max_weight = max(non_zero_weights.values())
                if max_weight > 0:
                    normalized_weights = {
                        k: v / max_weight for k, v in non_zero_weights.items()
                    }

                    self.logger.debug(f"æƒé‡å½’ä¸€åŒ–:")
                    self.logger.debug(f"  å½’ä¸€åŒ–å‰æƒé‡èŒƒå›´: {min(non_zero_weights.values()):.6f} - {max(non_zero_weights.values()):.6f}")
                    self.logger.debug(f"  å½’ä¸€åŒ–åæƒé‡èŒƒå›´: {min(normalized_weights.values()):.6f} - 1.000000")

                    return normalized_weights
                else:
                    self.logger.warning("âš ï¸ æ‰€æœ‰äº‹ä»¶æƒé‡éƒ½ä¸º0ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
                    # ç»™æ‰€æœ‰äº‹ä»¶åˆ†é…ç›¸åŒçš„å°æƒé‡
                    fallback_weight = 0.1
                    return {k: fallback_weight for k in event_key_weights.keys()}
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰éé›¶æƒé‡äº‹ä»¶ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
                fallback_weight = 0.1
                return {k: fallback_weight for k in event_key_weights.keys()}

        return event_key_query_weights

    async def _step5_calculate_key_event_weights(
        self,
        event_ids: List[str],
        key_ids: List[str],
        event_weights: Dict[str, float],
    ) -> Tuple[Dict[str, float], Dict[str, List[Tuple[str, str, float]]]]:
        """
        æ­¥éª¤5: åå‘è®¡ç®—keyæƒé‡å‘é‡
        æ ¹æ®eventæƒé‡åå‘å¾—å‡ºeventé‡Œæ‰€æœ‰çš„keyçš„é‡è¦æ€§
        ä¿®æ­£ï¼šä»eventsä¸­æå–æ‰€æœ‰keysï¼Œä¸ä»…é™äºå·²çŸ¥keys

        ä¼˜åŒ–ï¼šåˆå¹¶ä¸¤æ¬¡æ•°æ®åº“æŸ¥è¯¢ä¸ºä¸€æ¬¡ï¼Œå‡å°‘ç½‘ç»œå¾€è¿”

        æ–°å¢ï¼šè¿½è¸ªæ‰©å±•å…³ç³»ï¼Œè®°å½•æ¯ä¸ªchild entityæ˜¯é€šè¿‡å“ªäº›(parent_id, event_id, weight)æ‰©å±•è€Œæ¥

        Returns:
            Tuple[key_event_weights, key_expansion_trace]
            - key_event_weights: {entity_id: total_weight}
            - key_expansion_trace: {child_entity_id: [(parent_entity_id, event_id, event_weight), ...]}
        """
        if not event_ids:
            return {}, {}

        key_event_weights = {}
        key_expansion_trace = {}  # æ–°å¢ï¼šè¿½è¸ªæ‰©å±•å…³ç³»

        # è°ƒè¯•ï¼šè¾“å‡ºæƒé‡è®¡ç®—ä¿¡æ¯
        self.logger.info(f"æ­¥éª¤5è°ƒè¯•ä¿¡æ¯:")
        self.logger.info(f"  è¾“å…¥event_idsæ•°é‡: {len(event_ids)}")
        self.logger.info(f"  è¾“å…¥key_idsæ•°é‡: {len(key_ids)}")
        self.logger.info(f"  è¾“å…¥event_weightsæ•°é‡: {len(event_weights)}")

        try:
            async with self.session_factory() as session:
                # âœ… ä¼˜åŒ–ï¼šä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ entity-event å…³ç³»
                entity_event_query = (
                    select(EventEntity.entity_id, EventEntity.event_id)
                    .where(EventEntity.event_id.in_(event_ids))
                )
                result = await session.execute(entity_event_query)
                all_relations = result.fetchall()

                # åœ¨å†…å­˜ä¸­å¤„ç†ï¼šæå–æ‰€æœ‰ entity_ids å¹¶åˆ†ç»„
                all_entity_ids = set()
                entity_to_events = {}
                event_to_entities = {}  # æ–°å¢ï¼šåå‘æ˜ å°„

                for entity_id, event_id in all_relations:
                    all_entity_ids.add(entity_id)
                    if entity_id not in entity_to_events:
                        entity_to_events[entity_id] = []
                    entity_to_events[entity_id].append(event_id)

                    # æ„å»ºåå‘æ˜ å°„ï¼ševent -> entities
                    if event_id not in event_to_entities:
                        event_to_entities[event_id] = []
                    event_to_entities[event_id].append(entity_id)

                # è½¬æ¢ä¸º list ä¿æŒä¸åŸå®ç°ç±»å‹ä¸€è‡´
                all_entity_ids = list(all_entity_ids)

                self.logger.info(f"  ä»eventsä¸­å‘ç°çš„æ€»entities: {len(all_entity_ids)}")

                # åŒºåˆ†å·²çŸ¥keyså’Œæ–°å‘ç°çš„keys
                known_keys = set(key_ids)
                new_keys = [eid for eid in all_entity_ids if eid not in known_keys]

                self.logger.info(f"  å·²çŸ¥keys: {len(known_keys)}")
                self.logger.info(f"  æ–°å‘ç°keys: {len(new_keys)}")

                # è®¡ç®—æ‰€æœ‰entitiesçš„æƒé‡ + è¿½è¸ªæ‰©å±•å…³ç³»
                for entity_id in all_entity_ids:
                    entity_events = entity_to_events.get(entity_id, [])

                    # è®¡ç®—æƒé‡ï¼šå°†åŒ…å«è¯¥entityçš„æ‰€æœ‰eventæƒé‡ç›¸åŠ 
                    total_weight = sum(
                        event_weights.get(event_id, 0.0) for event_id in entity_events
                    )
                    key_event_weights[entity_id] = total_weight

                    # ğŸ†• è¿½è¸ªæ‰©å±•å…³ç³»ï¼šè®°å½•child entityæ˜¯é€šè¿‡å“ªäº›(parent, event)æ‰©å±•è€Œæ¥
                    is_new = entity_id not in known_keys
                    if is_new:
                        expansion_paths = []
                        for event_id in entity_events:
                            event_weight = event_weights.get(event_id, 0.0)
                            if event_weight > 0:
                                # æ‰¾å‡ºè¯¥eventä¸­åŒ…å«çš„parent entitiesï¼ˆæ¥è‡ªknown_keysï¼‰
                                event_entities = event_to_entities.get(event_id, [])
                                parent_entities = [eid for eid in event_entities if eid in known_keys]

                                # ä¸ºæ¯ä¸ªparentè®°å½•ä¸€æ¡æ‰©å±•è·¯å¾„
                                for parent_id in parent_entities:
                                    expansion_paths.append((parent_id, event_id, event_weight))

                        if expansion_paths:
                            key_expansion_trace[entity_id] = expansion_paths

                    # è°ƒè¯•ä¿¡æ¯
                    marker = "ğŸ†•" if is_new else "ğŸ”„"
                    if total_weight > 0:
                        expansion_info = ""
                        if is_new and entity_id in key_expansion_trace:
                            num_paths = len(key_expansion_trace[entity_id])
                            num_parents = len(set(p[0] for p in key_expansion_trace[entity_id]))
                            expansion_info = f", æ‰©å±•è·¯å¾„={num_paths}æ¡(æ¥è‡ª{num_parents}ä¸ªparent)"
                        self.logger.debug(f"  {marker} Entity {entity_id[:8]} ({'æ–°' if is_new else 'å·²çŸ¥'}): å…³è”{len(entity_events)}ä¸ªevents, æ€»æƒé‡={total_weight:.4f}{expansion_info}")
                    else:
                        self.logger.debug(f"  {marker} Entity {entity_id[:8]} ({'æ–°' if is_new else 'å·²çŸ¥'}): å…³è”{len(entity_events)}ä¸ªevents, æ€»æƒé‡={total_weight:.4f}")

        except Exception as e:
            self.logger.error(f"æ­¥éª¤5è®¡ç®—key-eventæƒé‡å¤±è´¥: {e}", exc_info=True)
            raise

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        non_zero_keys = sum(1 for weight in key_event_weights.values() if weight > 0)
        self.logger.info(f"æ­¥éª¤5æƒé‡è®¡ç®—ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ æ€»entitiesæ•°: {len(key_event_weights)}")
        self.logger.info(f"  â€¢ æƒé‡>0çš„entitiesæ•°: {non_zero_keys}")
        self.logger.info(f"  â€¢ è¿½è¸ªåˆ°æ‰©å±•å…³ç³»çš„æ–°entitiesæ•°: {len(key_expansion_trace)}")

        if key_event_weights:
            weight_values = list(key_event_weights.values())
            self.logger.info(f"  â€¢ æƒé‡èŒƒå›´: {min(weight_values):.4f} - {max(weight_values):.4f}")

            # æ˜¾ç¤ºæƒé‡æœ€é«˜çš„å‡ ä¸ªentities
            sorted_entities = sorted(key_event_weights.items(), key=lambda x: x[1], reverse=True)[:5]
            top_entities_str = ", ".join([f"{eid[:8]}:{w:.3f}" for eid, w in sorted_entities])
            self.logger.info(f"  â€¢ Top5 entities: {top_entities_str}")

        return key_event_weights, key_expansion_trace

    async def _aggregate_key_weights(self, weight_evolution: Dict[int, Dict[str, float]]) -> Dict[str, float]:
        """
        èšåˆå¤šè·³çš„keyæƒé‡
        é‡‡ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼ï¼Œè¶Šåé¢çš„è·³è·ƒæƒé‡è¶Šé«˜
        """
        if not weight_evolution:
            return {}

        self.logger.debug(f"æƒé‡èšåˆè°ƒè¯•ä¿¡æ¯:")
        self.logger.debug(f"  è·³è·ƒæ¬¡æ•°: {len(weight_evolution)}")
        for jump, weights in weight_evolution.items():
            self.logger.debug(f"  ç¬¬{jump}è·³: {len(weights)}ä¸ªkeys, æƒé‡èŒƒå›´: {min(weights.values()) if weights else 0:.4f} - {max(weights.values()) if weights else 0:.4f}")

        aggregated_weights = {}
        total_jumps = len(weight_evolution)

        # å¯¹æ¯ä¸ªkeyï¼Œè®¡ç®—å…¶åœ¨æ‰€æœ‰è·³è·ƒä¸­çš„åŠ æƒå¹³å‡æƒé‡
        all_key_ids = set()
        for jump_weights in weight_evolution.values():
            all_key_ids.update(jump_weights.keys())

        self.logger.debug(f"  æ€»å…±æ¶‰åŠçš„keys: {len(all_key_ids)}")

        for key_id in all_key_ids:
            weighted_sum = 0.0
            weight_sum = 0.0
            jump_contributions = []

            for jump, jump_weights in weight_evolution.items():
                if key_id in jump_weights:
                    # è¶Šåé¢çš„è·³è·ƒæƒé‡è¶Šé«˜
                    jump_importance = jump / total_jumps
                    weighted_sum += jump_weights[key_id] * jump_importance
                    weight_sum += jump_importance
                    jump_contributions.append(f"è·³{jump}:{jump_weights[key_id]:.3f}")

            if weight_sum > 0:
                aggregated_weights[key_id] = weighted_sum / weight_sum
                self.logger.debug(f"  Key {key_id[:8]}: èšåˆæƒé‡={aggregated_weights[key_id]:.4f}, è´¡çŒ®={', '.join(jump_contributions)}")

        # è¾“å‡ºèšåˆç»“æœç»Ÿè®¡
        self.logger.info(f"æƒé‡èšåˆç»“æœ:")
        self.logger.info(f"  â€¢ èšåˆåkeyæ•°é‡: {len(aggregated_weights)}")
        if aggregated_weights:
            self.logger.info(f"  â€¢ èšåˆæƒé‡èŒƒå›´: {min(aggregated_weights.values()):.4f} - {max(aggregated_weights.values()):.4f}")

        return aggregated_weights

    async def _extract_final_keys(
        self,
        key_weights: Dict[str, float],
        config: SearchConfig,
        recall_keys: List[Dict[str, Any]],
        weight_evolution: Dict[int, Dict[str, float]],
        all_discovered_keys: set,
        key_parent_map: Dict[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æå–æœ€ç»ˆçš„keysï¼Œè®°å½•æœ€æ—©å‘ç°çš„æ­¥éª¤
        åŸºäºall_discovered_keysè¿”å›å®Œæ•´çš„å»é‡keylistï¼Œä¸å†ä½¿ç”¨é˜ˆå€¼æˆ–æ•°é‡é™åˆ¶
        """
        if not all_discovered_keys:
            return []

        # æ„å»ºå¬å›é˜¶æ®µçš„key IDé›†åˆ
        recall_key_ids = {key["key_id"] for key in recall_keys}

        # è®¡ç®—æ¯ä¸ªkeyçš„æœ€æ—©å‘ç°æ­¥éª¤
        key_discovery_steps = {}
        for key_id in all_discovered_keys:
            if key_id in recall_key_ids:
                # åœ¨Recallä¸­å‘ç°ï¼Œæ­¥éª¤è®°ä¸º1
                key_discovery_steps[key_id] = 1
            else:
                # åœ¨Expandä¸­å‘ç°ï¼Œæ‰¾åˆ°æœ€æ—©çš„è·³è·ƒæ­¥éª¤
                earliest_jump = None
                for jump in sorted(weight_evolution.keys()):
                    if key_id in weight_evolution[jump]:
                        earliest_jump = jump
                        break

                if earliest_jump is not None:
                    # Expandçš„è·³è·ƒæ­¥éª¤è½¬æ¢ä¸ºå…¨å±€æ­¥éª¤ç¼–å·
                    # Recallæ˜¯æ­¥éª¤1ï¼ŒExpandç¬¬1è·³æ˜¯æ­¥éª¤2ï¼Œä»¥æ­¤ç±»æ¨
                    key_discovery_steps[key_id] = earliest_jump + 1
                else:
                    # é»˜è®¤è®¾ä¸ºExpandç¬¬1è·³
                    key_discovery_steps[key_id] = 2

        # å¯¹all_discovered_keysä¸­çš„keysæŒ‰æƒé‡æ’åºï¼ˆå¦‚æœæœ‰æƒé‡ä¿¡æ¯çš„è¯ï¼‰
        sorted_keys = []
        for key_id in all_discovered_keys:
            weight = key_weights.get(key_id, 0.0)
            sorted_keys.append((key_id, weight))

        # æŒ‰æƒé‡æ’åº
        sorted_keys.sort(key=lambda x: x[1], reverse=True)

        # é€‰æ‹©æ‰€æœ‰å‘ç°çš„keysï¼Œä¸å†è¿›è¡Œé˜ˆå€¼æˆ–æ•°é‡ç­›é€‰
        selected_keys = sorted_keys

        # è·å–keyçš„è¯¦ç»†ä¿¡æ¯
        key_final = []
        if selected_keys:
            key_ids = [key_id for key_id, _ in selected_keys]

            try:
                async with self.session_factory() as session:
                    # æŸ¥è¯¢æ‰€æœ‰ selected keys çš„å®ä½“ä¿¡æ¯
                    query = select(Entity).where(Entity.id.in_(key_ids))
                    result = await session.execute(query)
                    entities = {entity.id: entity for entity in result.scalars().all()}

                    # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ parent entity IDs
                    parent_ids = set()
                    for key_id in key_ids:
                        if key_id in key_parent_map:
                            parent_ids.add(key_parent_map[key_id]["parent_id"])

                    # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰ parent entities
                    parent_entities = {}
                    if parent_ids:
                        parent_query = select(Entity).where(Entity.id.in_(list(parent_ids)))
                        parent_result = await session.execute(parent_query)
                        parent_entities = {entity.id: entity for entity in parent_result.scalars().all()}

                for key_id, weight in selected_keys:
                    entity = entities.get(key_id)
                    if entity:
                        key_info = {
                            "key_id": key_id,
                            "name": entity.name,
                            "type": entity.type,
                            "weight": weight,
                            "description": entity.description,
                            "steps": [key_discovery_steps[key_id]],
                            # è®°å½•æœ€æ—©å‘ç°çš„æ­¥éª¤
                            "hop": 0  # é»˜è®¤ä¸º0ï¼ˆRecallé˜¶æ®µï¼‰ï¼Œåé¢ä¼šæ ¹æ®å®é™…æƒ…å†µæ›´æ–°
                        }

                        # å¦‚æœè¿™ä¸ªkeyæ˜¯åœ¨Expandä¸­é€šè¿‡parentæ‰©å±•å‘ç°çš„ï¼Œæ·»åŠ parent_entityä¿¡æ¯
                        if key_id in key_parent_map:
                            parent_info = key_parent_map[key_id]
                            parent_id = parent_info["parent_id"]
                            parent_entity = parent_entities.get(parent_id)

                            if parent_entity:
                                # è®¡ç®—parentçš„hopï¼ˆæ¯”childçš„hopå°1ï¼‰
                                parent_hop = parent_info["hop"] - 1 if parent_info["hop"] > 0 else 0

                                key_info["parent_entity"] = {
                                    "id": parent_id,
                                    "name": parent_entity.name,
                                    "type": parent_entity.type,
                                    "hop": parent_hop  # ğŸ¨ æ·»åŠ parentçš„hop
                                }
                                key_info["hop"] = parent_info["hop"]
                                self.logger.debug(
                                    f"Key {entity.name} (step{key_discovery_steps[key_id]}) "
                                    f"ç”±parent {parent_entity.name} æ‰©å±•å‘ç° (hop={parent_info['hop']})"
                                )
                            else:
                                # ğŸ” Parent entityæœªæ‰¾åˆ°ï¼Œè®°å½•è­¦å‘Šå¹¶å°è¯•ä½¿ç”¨ç®€åŒ–ä¿¡æ¯
                                self.logger.warning(
                                    f"âš ï¸ Key '{entity.name}' åœ¨key_parent_mapä¸­ï¼Œä½†parent_id={parent_id[:8]}... "
                                    f"æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°ã€‚å°è¯•ä½¿ç”¨ç®€åŒ–parentä¿¡æ¯ã€‚"
                                )
                                # è®¡ç®—parentçš„hop
                                parent_hop = parent_info["hop"] - 1 if parent_info["hop"] > 0 else 0

                                # æä¾›ç®€åŒ–çš„parentä¿¡æ¯ï¼ˆè‡³å°‘åŒ…å«IDï¼‰
                                key_info["parent_entity"] = {
                                    "id": parent_id,
                                    "name": f"Unknown-{parent_id[:8]}",
                                    "type": "unknown",
                                    "hop": parent_hop  # ğŸ¨ æ·»åŠ parentçš„hop
                                }
                                key_info["hop"] = parent_info["hop"]
                        else:
                            # ğŸ” æ£€æŸ¥æ˜¯å¦åº”è¯¥æœ‰parentä½†key_parent_mapä¸­ç¼ºå¤±
                            if key_discovery_steps[key_id] >= 2:
                                self.logger.warning(
                                    f"âš ï¸ Key '{entity.name}' (step{key_discovery_steps[key_id]}) "
                                    f"ä¸åœ¨key_parent_mapä¸­ï¼å¯èƒ½æ˜¯stepåˆ¤æ–­é”™è¯¯æˆ–parentè®°å½•ç¼ºå¤±ã€‚"
                                )

                        key_final.append(key_info)

            except Exception as e:
                self.logger.error(f"æå–æœ€ç»ˆkeyså¤±è´¥: {e}", exc_info=True)
                raise

        return key_final
    async def _build_expand_clues(
        self,
        config: SearchConfig,
        key_final: List[Dict[str, Any]],
        key_parent_map: Dict[str, Dict[str, Any]],
        tracker: Tracker  # ğŸ†• æ¥æ”¶ tracker å®ä¾‹ï¼Œé¿å…åˆ›å»ºå¤šä¸ªå®ä¾‹
    ) -> List[Dict[str, Any]]:
        """
        æ„å»ºExpandé˜¶æ®µçš„çº¿ç´¢ï¼ˆentity â†’ event â†’ entityï¼‰

        ğŸ†• ä¿®æ”¹ï¼šä¸å†ä½¿ç”¨å•æ¡entityâ†’entityçº¿ç´¢ï¼Œæ”¹ä¸ºæ‹†åˆ†æˆä¸¤æ¡ï¼š
        1. parent_entity â†’ event
        2. event â†’ child_entity

        è¿™æ ·ç¡®ä¿ä¸­é—´èŠ‚ç‚¹ï¼ˆeventï¼‰ä¸ä¼šè¢«çœç•¥ï¼Œå‰ç«¯å¯ä»¥æ„å»ºå®Œæ•´çŸ¥è¯†å›¾è°±

        Args:
            config: æœç´¢é…ç½®
            key_final: æœ€ç»ˆçš„keyåˆ—è¡¨
            key_parent_map: parentå…³ç³»æ˜ å°„
            tracker: ç»Ÿä¸€çš„ Tracker å®ä¾‹ï¼ˆç¡®ä¿åŒä¸€è·³å†… event èŠ‚ç‚¹å»é‡ï¼‰

        Returns:
            Expandé˜¶æ®µçš„çº¿ç´¢åˆ—è¡¨ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼Œå®é™…çº¿ç´¢å·²è¿½åŠ åˆ°config.all_cluesï¼‰
        """
        # ğŸ†• ä½¿ç”¨ä¼ å…¥çš„ tracker å®ä¾‹ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„
        # tracker = Tracker(config)  # âŒ åˆ é™¤è¿™è¡Œ

        clues = []

        # ğŸ†• æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰éœ€è¦çš„eventä¿¡æ¯ï¼ˆé¿å…N+1æŸ¥è¯¢ï¼‰
        event_ids_needed = set()
        for key in key_final:
            steps = key.get("steps", [0])[0]
            if steps >= 2 and key["key_id"] in key_parent_map:
                parent_info = key_parent_map[key["key_id"]]
                if "event_id" in parent_info:
                    event_ids_needed.add(parent_info["event_id"])

        # æ‰¹é‡æŸ¥è¯¢events
        event_map = {}
        if event_ids_needed:
            try:
                async with self.session_factory() as session:
                    query = select(SourceEvent).where(SourceEvent.id.in_(list(event_ids_needed)))
                    result = await session.execute(query)
                    events = result.scalars().all()
                    event_map = {event.id: event for event in events}
                    self.logger.info(f"ğŸ“¦ æ‰¹é‡æŸ¥è¯¢äº† {len(event_map)} ä¸ªeventç”¨äºæ„å»ºçº¿ç´¢")
            except Exception as e:
                self.logger.warning(f"æ‰¹é‡æŸ¥è¯¢eventså¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„eventèŠ‚ç‚¹")

        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šç»Ÿè®¡çº¿ç´¢æ„å»ºæƒ…å†µ
        total_keys = len(key_final)
        expand_keys = 0
        keys_with_parent_entity = 0
        keys_without_parent_entity = []

        # ğŸ†• ç»Ÿè®¡ parent entities çš„æƒ…å†µ
        key_final_ids = set(k["key_id"] for k in key_final)
        parent_ids_in_clues = set()
        parent_ids_not_in_key_final = set()

        # ğŸ” ç»Ÿè®¡ä» Recall ä¼ å…¥çš„ keysï¼ˆä½œä¸º Expand çš„èµ·ç‚¹ï¼‰
        recall_keys = [k for k in key_final if k.get("steps", [0])[0] == 1]
        recall_key_ids = set(k["key_id"] for k in recall_keys)

        self.logger.info(
            f"ğŸ” [Expandè¯Šæ–­] Recallä¼ å…¥çš„keys: {len(recall_keys)}ä¸ª, "
            f"Expandæ‰©å±•çš„keys: {len([k for k in key_final if k.get('steps', [0])[0] >= 2])}ä¸ª"
        )

        # åªä¸ºåœ¨Expandä¸­å‘ç°çš„keysï¼ˆstepsåŒ…å«2æˆ–æ›´å¤§ï¼‰æ„å»ºexpandçº¿ç´¢
        for key in key_final:
            steps = key.get("steps", [0])[0]

            # ç»Ÿè®¡Expandå‘ç°çš„keys
            if steps >= 2:
                expand_keys += 1

                # æ£€æŸ¥parent_entityå­—æ®µ
                if "parent_entity" in key:
                    keys_with_parent_entity += 1
                else:
                    keys_without_parent_entity.append({
                        "key_id": key["key_id"],
                        "name": key["name"],
                        "step": steps
                    })

            # ğŸ†• æ‹†åˆ†æˆä¸¤æ¡çº¿ç´¢ï¼šparent_entity â†’ event, event â†’ child_entity
            if "parent_entity" in key and steps >= 2:
                parent_entity = key["parent_entity"]

                # æ„å»ºparentå®ä½“å­—å…¸ï¼ˆå®Œæ•´ä¿¡æ¯ç”¨äºæ ‡å‡†èŠ‚ç‚¹ï¼‰
                parent_entity_dict = {
                    "id": parent_entity["id"],
                    "key_id": parent_entity["id"],
                    "name": parent_entity["name"],
                    "type": parent_entity["type"],
                    "description": parent_entity.get("description", ""),
                    "hop": parent_entity.get("hop", 0)  # ğŸ¨ ä¼ é€’hopå­—æ®µ
                }

                # æ„å»ºchildå®ä½“å­—å…¸ï¼ˆå®Œæ•´ä¿¡æ¯ç”¨äºæ ‡å‡†èŠ‚ç‚¹ï¼‰
                child_entity_dict = {
                    "id": key["key_id"],
                    "key_id": key["key_id"],
                    "name": key["name"],
                    "type": key["type"],
                    "description": key.get("description", ""),
                    "hop": key.get("hop", 0)  # ğŸ¨ ä¼ é€’hopå­—æ®µ
                }

                # è·å–eventä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if key["key_id"] in key_parent_map:
                    parent_info = key_parent_map[key["key_id"]]
                    event_id = parent_info.get("event_id")
                    event_weight = parent_info.get("event_weight", 1.0)
                    hop = parent_info.get("hop", 1)

                    if event_id:
                        # æ„å»ºæ ‡å‡†èŠ‚ç‚¹
                        parent_node = Tracker.build_entity_node(parent_entity_dict)
                        child_node = Tracker.build_entity_node(child_entity_dict)

                        event_obj = event_map.get(event_id)
                        if event_obj:
                            # ğŸ†• ä½¿ç”¨ tracker å®ä¾‹æ–¹æ³•ï¼Œä¼ é€’ stage å’Œ hop
                            event_node = tracker.get_or_create_event_node(event_obj, stage="expand", hop=hop)
                        else:
                            # Fallbackï¼šåˆ›å»ºç®€åŒ–çš„event dict
                            self.logger.warning(f"Event {event_id[:8]} æœªåœ¨æ‰¹é‡æŸ¥è¯¢ä¸­æ‰¾åˆ°")
                            event_fallback_dict = {
                                "id": event_id,
                                "title": f"Event-{event_id[:8]}",
                                "content": "",
                                "category": "",
                                "summary": ""
                            }
                            # éœ€è¦å°†dictè½¬ä¸ºSourceEventå¯¹è±¡æ¨¡æ‹Ÿ
                            # ç”±äºæˆ‘ä»¬æ²¡æœ‰SourceEventå¯¹è±¡ï¼Œä½¿ç”¨ç‰¹æ®Šå¤„ç†
                            event_node = {
                                "id": event_id,
                                "type": "event",
                                "category": "",
                                "content": f"Event-{event_id[:8]}",
                                "description": ""
                            }

                        # è·å–å®ä½“ç›¸ä¼¼åº¦
                        parent_similarity = parent_entity_dict.get("similarity", 0.0)
                        child_similarity = child_entity_dict.get("similarity", 0.0)

                        # ğŸ†• ç¬¬ä¸€æ¡çº¿ç´¢ï¼šparent_entity â†’ eventï¼ˆtoèŠ‚ç‚¹æ˜¯äº‹ä»¶ï¼Œä¸å­˜å‚¨weightï¼‰
                        metadata1 = {"hop": hop, "method": "cooccurrence"}

                        tracker.add_clue(
                            stage="expand",
                            from_node=parent_node,
                            to_node=event_node,
                            confidence=parent_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                            metadata=metadata1
                        )

                        # ğŸ†• ç¬¬äºŒæ¡çº¿ç´¢ï¼ševent â†’ child_entityï¼ˆtoèŠ‚ç‚¹æ˜¯å®ä½“ï¼Œéœ€è¦weightï¼‰
                        child_entity_weight = key.get("weight")
                        metadata2 = {"hop": hop, "method": "cooccurrence"}
                        # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                        if child_entity_weight is not None:
                            metadata2["weight"] = child_entity_weight

                        tracker.add_clue(
                            stage="expand",
                            from_node=event_node,
                            to_node=child_node,
                            confidence=child_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                            metadata=metadata2
                        )

                        # ğŸ” è®°å½•parent_idï¼ˆç”¨äºç»Ÿè®¡ï¼‰
                        parent_ids_in_clues.add(parent_entity["id"])

                        self.logger.debug(
                            f"  âœ… æ‹†åˆ†çº¿ç´¢: {parent_node['content'][:10]} â†’ "
                            f"{event_node['content'][:10]} â†’ {child_node['content'][:10]} "
                            f"(hop={hop})"
                        )
                    else:
                        # æ²¡æœ‰event_idï¼Œç›´æ¥åˆ›å»ºentityâ†’entityçº¿ç´¢ï¼ˆfallbackï¼‰
                        parent_node = Tracker.build_entity_node(parent_entity_dict)
                        child_node = Tracker.build_entity_node(child_entity_dict)

                        # è·å–å®ä½“ç›¸ä¼¼åº¦
                        parent_similarity = parent_entity_dict.get("similarity", 0.0)
                        child_similarity = child_entity_dict.get("similarity", 0.0)
                        # ä½¿ç”¨å¹³å‡ç›¸ä¼¼åº¦ä½œä¸ºconfidence
                        avg_similarity = (parent_similarity + child_similarity) / 2.0

                        # è·å–å­å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        child_entity_weight = key.get("weight")
                        metadata = {"hop": hop, "method": "cooccurrence"}
                        # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                        if child_entity_weight is not None:
                            metadata["weight"] = child_entity_weight

                        tracker.add_clue(
                            stage="expand",
                            from_node=parent_node,
                            to_node=child_node,
                            confidence=avg_similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                            metadata=metadata
                        )
                        self.logger.warning(
                            f"  âš ï¸  ç¼ºå°‘event_idï¼Œä½¿ç”¨ç›´æ¥entityâ†’entityçº¿ç´¢: "
                            f"{parent_node['content'][:10]} â†’ {child_node['content'][:10]}"
                        )
                else:
                    # æ²¡æœ‰åœ¨key_parent_mapä¸­æ‰¾åˆ°ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
                    self.logger.warning(
                        f"  âš ï¸  key {key['key_id'][:8]} ä¸åœ¨key_parent_mapä¸­ï¼Œè·³è¿‡"
                    )

        # ğŸ” è¯Šæ–­æ—¥å¿—ï¼šè¾“å‡ºçº¿ç´¢æ„å»ºç»Ÿè®¡
        self.logger.info(
            f"ğŸ” [Expandè¯Šæ–­] çº¿ç´¢æ„å»ºç»Ÿè®¡: "
            f"æ€»keys={total_keys}, "
            f"Expand keys={expand_keys}, "
            f"æœ‰parent_entity={keys_with_parent_entity}"
        )

        # ğŸ” ç»Ÿè®¡å“ªäº› Recall çš„ key å‡ºç°åœ¨äº† Expand çº¿ç´¢ä¸­
        recall_keys_in_expand = recall_key_ids & parent_ids_in_clues
        recall_keys_not_in_expand = recall_key_ids - parent_ids_in_clues

        self.logger.info(
            f"ğŸ” [Expandè¯Šæ–­] Recall keysåœ¨Expandçº¿ç´¢ä¸­çš„æƒ…å†µ: "
            f"å‡ºç°={len(recall_keys_in_expand)}ä¸ª, "
            f"æœªå‡ºç°={len(recall_keys_not_in_expand)}ä¸ª"
        )

        if recall_keys_not_in_expand:
            # æ˜¾ç¤ºæœªå‡ºç°çš„ Recall keys
            missing_recall_keys = [k for k in recall_keys if k["key_id"] in recall_keys_not_in_expand]
            self.logger.warning(
                f"âš ï¸ [Expandè¯Šæ–­] {len(recall_keys_not_in_expand)}ä¸ªRecall keyæœªå‡ºç°åœ¨Expandçº¿ç´¢ä¸­ï¼š"
            )
            for k in missing_recall_keys[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                self.logger.warning(
                    f"  â€¢ [{k.get('type', 'unknown')}] {k.get('name', 'unknown')} "
                    f"(key_id={k['key_id'][:8]}..., weight={k.get('weight', 0):.3f})"
                )

        # å¦‚æœæœ‰ç¼ºå¤±parent_entityçš„Expand keysï¼Œå‘å‡ºè­¦å‘Š
        if keys_without_parent_entity:
            missing_keys_info = [f"{k['name']}(step{k['step']})" for k in keys_without_parent_entity[:3]]
            self.logger.warning(
                f"âš ï¸ [Expandè¯Šæ–­] {len(keys_without_parent_entity)}ä¸ªExpand keyç¼ºå¤±parent_entity: "
                f"{missing_keys_info}"
            )

        # è¿”å›ç©ºåˆ—è¡¨ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼Œå®é™…çº¿ç´¢å·²é€šè¿‡trackerè¿½åŠ åˆ°config.all_cluesï¼‰
        self.logger.info(
            f"âœ¨ Expandçº¿ç´¢å·²é€šè¿‡trackerè¿½åŠ åˆ°config.all_clues "
            f"(æ¯ä¸ªæ‰©å±•æ‹†åˆ†ä¸º2æ¡: entityâ†’event, eventâ†’entity)"
        )
        return []
