"""
æœç´¢ Rerank æ¨¡å—

å®žçŽ°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦æ®µè½çš„æ–¹æ³•ï¼š
1. keyæ‰¾contentï¼šæ ¹æ®[key-final]ä»Žsqlä¸­æå–åŽŸæ–‡å—[content-key-related]ï¼Œä»ŽESèŽ·å–é¢„å­˜å‘é‡å¹¶è®¡ç®—å’Œqueryçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè®°å½•event_idï¼‰
2. queryæ‰¾contentï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼ˆKNN+ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°åŽŸæ–‡å—[content-query-related]ï¼ˆevent_idsä¸ºç©ºï¼‰
3. contentåˆå¹¶+åŽ»é‡ï¼šåˆå¹¶[content-key-related]å’Œ[content-query-related]ï¼Œå¦‚æžœåŒä¸€æ®µè½ï¼ˆarticle_id+section_idç›¸åŒï¼‰åŒæ—¶å‡ºçŽ°åœ¨SQLå’ŒEmbeddingç»“æžœä¸­ï¼Œåªä¿ç•™SQLçš„ç»“æžœ
4. åˆ¶ä½œ[content-related]æƒé‡å‘é‡ï¼šä½¿ç”¨å…¬å¼ weight = 0.5*ç›¸ä¼¼åº¦ + ln(1 + Î£(keyæƒé‡ Ã— ln(1+å‡ºçŽ°æ¬¡æ•°) / step))
5. PageRanké‡æŽ’åºï¼šæ ¹æ®æƒé‡å¯¹æ®µè½æŽ’åºï¼ˆä»Žå¤§åˆ°å°ï¼‰
6. å–Top-Nå¹¶è¿”å›žï¼šä»ŽTop-Næ®µè½ä¸­æå–å…³è”çš„äº‹é¡¹åˆ—è¡¨

è¿”å›žæ ¼å¼ï¼š
Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
    - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰æ®µè½ PageRank é¡ºåºæŽ’åˆ—ï¼Œå·²åŽ»é‡ï¼‰
    - clues (Dict): å¬å›žçº¿ç´¢ä¿¡æ¯
        - origin_query (str): åŽŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
        - final_query (str): LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆé‡å†™åŽï¼‰
        - query_entities (List[Dict]): æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
        - recall_entities (List[Dict]): å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æŽ‰query_entitiesä¸­çš„å€¼ï¼‰

æ³¨æ„ï¼šstep1 å’Œ step2 éƒ½ä½¿ç”¨ ES é¢„å­˜çš„ content_vectorï¼ˆç”Ÿæˆæ—¶ä½¿ç”¨"æ ‡é¢˜ + å†…å®¹[:500]"ï¼‰ï¼Œç¡®ä¿å‘é‡ä¸€è‡´æ€§

"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math
import re
import time
import asyncio
from collections import Counter, defaultdict


from sqlalchemy import select, and_, or_
from sqlalchemy.orm import selectinload

from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import SourceEvent, Entity, EventEntity, ArticleSection, Article, get_session_factory
from sag.exceptions import AIError
from sag.modules.load.processor import DocumentProcessor
from sag.modules.search.config import SearchConfig
from sag.modules.search.tracker import Tracker  # ðŸ†• æ·»åŠ çº¿ç´¢è¿½è¸ªå™¨
from sag.utils import get_logger

logger = get_logger("search.rerank.pagerank")


@dataclass
class ContentSearchResult:
    """
    æœç´¢ç»“æžœçš„ç»Ÿä¸€è¿”å›žæ ¼å¼

    ç”¨äºŽè¡¨ç¤ºä»ŽSQLæ•°æ®åº“æˆ–Embeddingå‘é‡æ•°æ®åº“æœç´¢åˆ°çš„å†…å®¹
    """
    # å¿…éœ€å­—æ®µ
    search_type: str      # "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
    source_id: str        # æ•°æ®æºID (UUID)
    article_id: str       # æ–‡ç« ID (UUID)
    section_id: str       # æ®µè½ID (UUID)
    rank: int             # æ®µè½åœ¨æ–‡ç« ä¸­çš„æŽ’åº
    heading: str          # æ®µè½æ ‡é¢˜
    content: str          # æ®µè½å†…å®¹
    score: float = 0.0    # ç›¸å…³æ€§å¾—åˆ†
    weight: float = 0.0   # æƒé‡å€¼ï¼ˆstep4è®¡ç®—åŽèµ‹å€¼ï¼‰
    event_ids: List[str] = None  # å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
    event: str = ""  # èšåˆåŽçš„äº‹é¡¹æ‘˜è¦ï¼ˆå¤šä¸ªsummaryåˆå¹¶ï¼‰
    clues: List[Dict[str, Any]] = None  # å¬å›žè¯¥æ®µè½çš„çº¿ç´¢åˆ—è¡¨ï¼ˆæ¥è‡ª key_final æˆ– queryï¼‰

    def __post_init__(self):
        """åˆå§‹åŒ–åŽéªŒè¯"""
        # åˆå§‹åŒ– event_ids ä¸ºç©ºåˆ—è¡¨
        if self.event_ids is None:
            self.event_ids = []

        # åˆå§‹åŒ– clues ä¸ºç©ºåˆ—è¡¨
        if self.clues is None:
            self.clues = []

        # å…è®¸ "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
        valid_types = ["sql", "embedding"]
        is_valid = (
            self.search_type in valid_types or
            self.search_type.startswith("SQL-") or
            self.search_type.startswith("embedding-")
        )

        if not is_valid:
            raise ValueError(
                f"search_type å¿…é¡»æ˜¯ 'sql', 'embedding' æˆ–å¸¦ç¼–å·æ ¼å¼(å¦‚ 'SQL-1', 'embedding-1')ï¼Œ"
                f"å½“å‰å€¼: {self.search_type}"
            )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "search_type": self.search_type,
            "source_id": self.source_id,
            "article_id": self.article_id,
            "section_id": self.section_id,
            "rank": self.rank,
            "heading": self.heading,
            "content": self.content,
            "score": self.score,
            "weight": self.weight,
            "event_ids": self.event_ids,
            "event": self.event,
            "clues": self.clues,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ContentSearchResult":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        return cls(
            search_type=data.get("search_type", "sql"),
            source_id=data["source_id"],
            article_id=data["article_id"],
            section_id=data["section_id"],
            rank=data.get("rank", 0),
            heading=data.get("heading", ""),
            content=data.get("content", ""),
            score=data.get("score", 0.0),
            weight=data.get("weight", 0.0),
            event_ids=data.get("event_ids", []),
            event=data.get("event", ""),
            clues=data.get("clues", []),
        )

    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"ContentSearchResult(type={self.search_type}, "
            f"section_id={self.section_id}, "
            f"heading='{self.heading[:30]}...', "
            f"score={self.score:.3f})"
        )


class RerankPageRankSearcher:
    """Rerankæ®µè½æœç´¢å™¨ - å®žçŽ°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦æ®µè½çš„æ–¹æ³•"""

    def __init__(
        self,
        llm_client=None
    ):
        """
        åˆå§‹åŒ–Rerankæ®µè½æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """

        self.session_factory = get_session_factory()
        self.logger = get_logger("search.rerank.pagerank")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.content_repo = SourceChunkRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)  # æ·»åŠ äº‹é¡¹å‘é‡ä»“åº“

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºŽç”Ÿæˆembedingå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "Rerankæ®µè½æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    async def search(
        self,
        key_final: List[Dict[str, Any]],
        config: SearchConfig
    ) -> Dict[str, Any]:
        """
        Rerank æœç´¢ä¸»æ–¹æ³•

        æ•´åˆæ­¥éª¤1-6ï¼Œç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼Œé¿å…é‡å¤è®¡ç®—

        æ­¥éª¤æµç¨‹ï¼š
          1. keyæ‰¾content (SQL) - è®°å½•event_id
          2. queryæ‰¾content (KNN + ä½™å¼¦ç›¸ä¼¼åº¦) - event_idsä¸ºç©º
          3. åˆå¹¶ç»“æžœ+åŽ»é‡ï¼ˆä¼˜å…ˆä¿ç•™SQLç»“æžœï¼‰
          4. è®¡ç®—æƒé‡å‘é‡
          5. PageRankæŽ’åº
          6. å–Top-Næ®µè½å¹¶æå–å…³è”çš„äº‹é¡¹åˆ—è¡¨

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„å…³é”®å®žä½“åˆ—è¡¨
            config: Rerankæœç´¢é…ç½®

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰æ®µè½ PageRank é¡ºåºæŽ’åˆ—ï¼Œå·²åŽ»é‡ï¼‰
                - clues (Dict): å¬å›žçº¿ç´¢ä¿¡æ¯
                    - origin_query (str): åŽŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
                    - final_query (str): LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆé‡å†™åŽï¼‰
                    - query_entities (List[Dict]): æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities (List[Dict]): å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æŽ‰query_entitiesä¸­çš„å€¼ï¼‰
        """
        try:
            # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
            overall_start = time.perf_counter()

            self.logger.info(
                f"Rerankæœç´¢å¼€å§‹: query='{config.query}', source_config_ids={config.get_source_config_ids()}")

            # ç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼ˆé¿å…åœ¨step1å’Œstep2ä¸­é‡å¤è®¡ç®—ï¼‰
            vector_start = time.perf_counter()
            query_vector = await self._generate_query_vector(config.query, config)
            vector_time = time.perf_counter() - vector_start
            if config.has_query_embedding:
                self.logger.info(
                    f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")
            else:
                self.logger.info(
                    f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")

            # ç”¨äºŽè®°å½•å„æ­¥éª¤è€—æ—¶
            step_times = {}

            self.logger.info("=" * 80)
            self.logger.info("ã€Rerank æœç´¢ã€‘è€—æ—¶ç»Ÿè®¡")
            self.logger.info("=" * 80)

            # æ®µè½æ¨¡å¼ï¼šæ‰§è¡Œå®Œæ•´çš„step1-step6æµç¨‹
            # æ­¥éª¤1å’Œ2å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆäº’ä¸ä¾èµ–ï¼‰
            self.logger.info("æ­¥éª¤1å’Œ2å¹¶è¡Œå¼€å§‹...")
            parallel_start = time.perf_counter()

            # å¹¶è¡Œæ‰§è¡Œ step1 å’Œ step2
            step1_task = self._step1_keys_to_contents(
                key_final=key_final,
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                query_vector=query_vector,
                config=config  # ä¼ å…¥configç”¨äºŽç¼“å­˜
            )

            step2_task = self._step2_query_to_contents(
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                k=config.rerank.pagerank_section_top_k,
                query_vector=query_vector,
                config=config  # ä¼ å…¥configç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤
            )

            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆ
            step1_results, step2_results = await asyncio.gather(step1_task, step2_task)

            parallel_time = time.perf_counter() - parallel_start
            step_times['step1_2å¹¶è¡Œæ‰§è¡Œ'] = parallel_time

            self.logger.info(
                f"æ­¥éª¤1å’Œ2å¹¶è¡Œå®Œæˆ: "
                f"step1æ‰¾åˆ° {len(step1_results)} ä¸ªæ®µè½, "
                f"step2æ‰¾åˆ° {len(step2_results)} ä¸ªæ®µè½, "
                f"æ€»è€—æ—¶: {parallel_time:.3f}ç§’"
            )

            # æ­¥éª¤3: åˆå¹¶ç»“æžœï¼ˆä¸åŽ»é‡ï¼Œç›´æŽ¥åˆå¹¶ï¼‰
            step3_start = time.perf_counter()
            merged_results = await self._step3_merge_result(step1_results, step2_results, config)
            step3_time = time.perf_counter() - step3_start
            step_times['step3_åˆå¹¶åŽ»é‡'] = step3_time
            self.logger.info(
                f"æ­¥éª¤3å®Œæˆï¼Œåˆå¹¶åŽæ€»å…± {len(merged_results)} ä¸ªæ®µè½, è€—æ—¶: {step3_time:.3f}ç§’")

            # æ­¥éª¤4: è®¡ç®—æ®µè½æƒé‡
            step4_start = time.perf_counter()
            weighted_results = await self._step4_calculate_weight_of_contents(
                key_final=key_final,
                content_related=merged_results
            )
            step4_time = time.perf_counter() - step4_start
            step_times['step4_æƒé‡è®¡ç®—'] = step4_time
            self.logger.info(
                f"æ­¥éª¤4å®Œæˆï¼Œè®¡ç®—äº† {len(weighted_results)} ä¸ªæ®µè½çš„æƒé‡, è€—æ—¶: {step4_time:.3f}ç§’")

            # æ­¥éª¤5: PageRanké‡æŽ’åºï¼ˆä½¿ç”¨PageRankç®—æ³•ï¼‰
            step5_start = time.perf_counter()
            sorted_results = await self._step5_pageRank_of_contents(
                content_related=weighted_results,
                key_final=key_final  # ä¼ å…¥key_finalç”¨äºŽå®žä½“å…³è”
            )
            step5_time = time.perf_counter() - step5_start
            step_times['step5_PageRankæŽ’åº'] = step5_time
            self.logger.info(
                f"æ­¥éª¤5å®Œæˆï¼ŒæŽ’åºäº† {len(sorted_results)} ä¸ªæ®µè½, è€—æ—¶: {step5_time:.3f}ç§’")

            # æ­¥éª¤6: æå–å…³è”çš„äº‹é¡¹åˆ—è¡¨å¹¶è®¡ç®—ç›¸ä¼¼åº¦
            step6_start = time.perf_counter()
            results, event_to_clues = await self._step6_get_topn_of_contents(
                sorted_contents=sorted_results,
                top_k=config.rerank.max_results * 2,
                source_config_ids=config.get_source_config_ids(),
                query_vector=query_vector,
                config=config
            )
            step6_time = time.perf_counter() - step6_start
            step_times['step6_äº‹é¡¹å¬å›žä¸Žè¿‡æ»¤'] = step6_time
            self.logger.info(
                f"æ­¥éª¤6å®Œæˆï¼Œæœ€ç»ˆè¿”å›ž {len(results)} ä¸ªäº‹é¡¹, è€—æ—¶: {step6_time:.3f}ç§’"
            )

            # è®¡ç®—æ€»è€—æ—¶
            overall_time = time.perf_counter() - overall_start

            # è¾“å‡ºè€—æ—¶ç»Ÿè®¡æ±‡æ€»
            self.logger.info("=" * 80)
            self.logger.info("ã€Rerank æœç´¢ã€‘å„æ­¥éª¤è€—æ—¶æ±‡æ€»:")
            self.logger.info("-" * 80)
            self.logger.info(
                f"æŸ¥è¯¢å‘é‡ç”Ÿæˆ: {vector_time:.3f}ç§’ ({vector_time/overall_time*100:.1f}%)")
            for step_name, step_time in step_times.items():
                self.logger.info(
                    f"{step_name}: {step_time:.3f}ç§’ ({step_time/overall_time*100:.1f}%)")
            self.logger.info("-" * 80)
            self.logger.info(f"æ€»è€—æ—¶: {overall_time:.3f}ç§’")
            self.logger.info("=" * 80)

            # === æž„å»ºReranké˜¶æ®µçº¿ç´¢ ===
            # rerank_clues = self._build_rerank_clues(config, key_final, results, event_to_clues, sorted_results)
            # config.rerank_clues = rerank_clues
            self.logger.info(f"âœ¨ Rerankçº¿ç´¢å·²æž„å»º (entityâ†’sectionâ†’eventæ‹†åˆ†ä¸º2æ¡çº¿ç´¢)")

            # æž„å»ºå¹¶è¿”å›žæ–°çš„å“åº”æ ¼å¼
            return self._build_response(config, key_final, results, event_to_clues)

        except Exception as e:
            self.logger.error(f"Rerankæœç´¢å¤±è´¥: {e}", exc_info=True)
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿”å›ž final_query
            # å¦‚æžœå¯ç”¨äº†queryé‡å†™åŠŸèƒ½ï¼ˆenable_query_rewrite=Trueï¼‰ï¼Œåˆ™è¿”å›žé‡å†™åŽçš„query
            # å¦åˆ™è¿”å›ž None
            final_query = config.query if config.enable_query_rewrite else None
            return {
                "events": [],
                "clues": {
                    "origin_query": config.original_query,
                    "final_query": final_query,
                    "query_entities": [],
                    "recall_entities": []
                }
            }  # å¤±è´¥æ—¶è¿”å›žç©ºå­—å…¸

    async def _step1_keys_to_contents(
        self,
        key_final: List[Dict[str, Any]],
        query: str,
        source_config_ids: List[str],
        query_vector: Optional[List[float]] = None,  # å¯é€‰çš„æŸ¥è¯¢å‘é‡
        config: Optional[SearchConfig] = None  # æ·»åŠ configå‚æ•°ç”¨äºŽç¼“å­˜
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤1: keyæ‰¾content
        æ ¹æ®[key-final]ä»Žsqlä¸­æå–åŽŸæ–‡å—[content-key-related]ï¼Œå¹¶è®¡ç®—å’Œqueryçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        1. å®žä½“åŒ¹é…ï¼šæ ¹æ®ä¼ å…¥çš„ key_final ä¸­çš„å®žä½“åç§°å’Œç±»åž‹ï¼Œåœ¨ Entity è¡¨ä¸­æŸ¥æ‰¾åŒ¹é…çš„å®žä½“
        2. äº‹ä»¶å…³è”ï¼šé€šè¿‡ EventEntity è¡¨æ‰¾åˆ°ä¸Žè¿™äº›å®žä½“ç›¸å…³çš„äº‹ä»¶
        3. æ®µè½æŸ¥æ‰¾ï¼šé€šè¿‡ SourceEvent å’Œ Article è¡¨çš„å…³è”ï¼Œæ‰¾åˆ°å¯¹åº”çš„æ–‡ç« 
        4. æ®µè½è¿‡æ»¤ï¼šæ£€æŸ¥äº‹ä»¶çš„ references å­—æ®µï¼Œåªè¿”å›žçœŸæ­£è¢«äº‹ä»¶å¼•ç”¨çš„æ®µè½
        5. å‘é‡èŽ·å–ï¼šä»Ž ES æ‰¹é‡èŽ·å–æ®µè½çš„é¢„å­˜å‘é‡ï¼ˆcontent_vector = æ ‡é¢˜ + å†…å®¹[:500]ï¼‰
        6. ç›¸ä¼¼åº¦è®¡ç®—ï¼šè®¡ç®—æ¯ä¸ªæ®µè½å‘é‡ä¸Ž query çš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆå¾—åˆ†

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„key_finalæ•°æ®
            query: æŸ¥è¯¢æ–‡æœ¬
            source_id: æ•°æ®æºID
            k: è¿”å›žç»“æžœæ•°é‡
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            ç›¸å…³æ®µè½åˆ—è¡¨ï¼ˆContentSearchResult.to_dict()æ ¼å¼ï¼‰ï¼ŒæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦é™åºæŽ’åº
        """
        try:
            self.logger.info(
                f"æ­¥éª¤1å¼€å§‹: å¤„ç† {len(key_final)} ä¸ªkey, query='{query}'")

            if not key_final:
                return []

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æžœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
                if config and config.has_query_embedding:
                    self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
                else:
                    self.logger.debug(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.debug(f"ä½¿ç”¨ä¼ å…¥çš„æŸ¥è¯¢å‘é‡ï¼Œç»´åº¦: {len(query_vector)}")

            # 2. æå–å®žä½“IDå’Œæƒé‡
            # key_id å°±æ˜¯å®žä½“çš„ IDï¼Œæ— éœ€å†æŸ¥è¯¢ Entity è¡¨
            entity_ids = [key.get("key_id") or key.get("id")
                          for key in key_final]
            entity_weight_map = {
                key.get("key_id") or key.get("id"): key["weight"]
                for key in key_final
            }

            # è¿‡æ»¤æŽ‰å¯èƒ½ä¸º None çš„ ID
            entity_ids = [eid for eid in entity_ids if eid]

            if not entity_ids:
                self.logger.warning("key_final ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®žä½“ID")
                return []

            self.logger.info(
                f"ä»Ž {len(key_final)} ä¸ªkeyä¸­æå–åˆ° {len(entity_ids)} ä¸ªå®žä½“ID")

            async with self.session_factory() as session:
                # ðŸ†• ä¼˜åŒ–ï¼šç›´æŽ¥é€šè¿‡å®žä½“IDæŸ¥è¯¢ Entity è¡¨ï¼ˆä»…ç”¨äºŽèŽ·å–å®žä½“è¯¦æƒ…ï¼‰
                entity_query = (
                    select(Entity)
                    .where(
                        and_(
                            Entity.source_id.in_(source_config_ids),
                            Entity.id.in_(entity_ids)
                        )
                    )
                )

                entity_result = await session.execute(entity_query)
                found_entities = entity_result.scalars().all()

                if not found_entities:
                    self.logger.warning("æœªæ‰¾åˆ°åŒ¹é…çš„å®žä½“")
                    return []

                self.logger.info(f"æ‰¾åˆ° {len(found_entities)} ä¸ªåŒ¹é…å®žä½“")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ª key å¬å›žçš„å®žä½“ï¼ˆæŒ‰ key_id ä¸€å¯¹ä¸€æ˜¾ç¤ºï¼‰
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Key â†’ Entity æ˜ å°„ (ä¸€å¯¹ä¸€):")
                self.logger.info("-" * 80)

                for key in key_final:
                    key_id = key.get("key_id") or key.get("id")
                    key_display = f"{key['name']}({key['type']})"

                    # æ‰¾åˆ°å¯¹åº”çš„ entity
                    entity = next(
                        (e for e in found_entities if e.id == key_id), None)
                    if entity:
                        entity_display = f"{entity.name}({entity.type})"
                        self.logger.info(
                            f"  Key '{key_display}' [id={key_id[:8]}...] â†’ Entity '{entity_display}'")
                    else:
                        self.logger.warning(
                            f"  Key '{key_display}' [id={key_id[:8]}...] â†’ âŒ æœªæ‰¾åˆ°å¯¹åº”å®žä½“")

                self.logger.info("-" * 80)
                self.logger.info(
                    f"  æ€»è®¡: {len(key_final)} ä¸ªKey â†’ {len(found_entities)} ä¸ªEntity")
                self.logger.info("=" * 80)

                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¯ä¸ªåŒ¹é…çš„å®žä½“
                for entity in found_entities:
                    self.logger.debug(
                        f"  å®žä½“: {entity.name} (type={entity.type}, id={entity.id[:8]}...)"
                    )

                # 4. é€šè¿‡EventEntityæŸ¥æ‰¾ç›¸å…³äº‹ä»¶ï¼ˆé™åˆ¶åœ¨æŒ‡å®šsource_config_idså†…ï¼‰
                event_entity_query = (
                    select(EventEntity.event_id,
                           EventEntity.entity_id, EventEntity.weight)
                    .join(SourceEvent, EventEntity.event_id == SourceEvent.id)
                    .where(
                        and_(
                            SourceEvent.source_id.in_(source_config_ids),
                            EventEntity.entity_id.in_(entity_ids)
                        )
                    )
                    .distinct()
                )

                event_result = await session.execute(event_entity_query)
                event_entities = event_result.fetchall()

                if not event_entities:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³äº‹ä»¶")
                    return []

                # 5. è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„æƒé‡ï¼Œå¹¶è®°å½•äº‹ä»¶åˆ°å®žä½“çš„æ˜ å°„
                event_weights = {}
                event_to_entities = {}  # äº‹ä»¶ID -> å®žä½“IDåˆ—è¡¨çš„æ˜ å°„

                # åˆ›å»º entity_id -> key å¯¹è±¡çš„æ˜ å°„ï¼ˆç”¨äºŽåŽç»­æž„å»º cluesï¼‰
                entity_to_key = {}
                for key in key_final:
                    key_id = key.get("key_id") or key.get("id")
                    if key_id:
                        # åˆ›å»º key çš„å‰¯æœ¬ï¼Œå°† key_id é‡å‘½åä¸º id
                        key_copy = key.copy()
                        if "key_id" in key_copy:
                            key_copy["id"] = key_copy.pop("key_id")
                        elif "id" not in key_copy:
                            key_copy["id"] = key_id
                        entity_to_key[key_id] = key_copy

                for event_entity in event_entities:
                    event_id = event_entity.event_id
                    entity_id = event_entity.entity_id
                    event_entity_weight = event_entity.weight or 1.0
                    entity_weight = entity_weight_map.get(
                        entity_id, 1.0)  # ðŸ†• ç›´æŽ¥ç”¨ entity_id æŸ¥æ‰¾

                    # ç»¼åˆæƒé‡ = å®žä½“æƒé‡ Ã— å…³è”æƒé‡
                    combined_weight = float(
                        entity_weight) * float(event_entity_weight)
                    event_weights[event_id] = event_weights.get(
                        event_id, 0) + combined_weight

                    # è®°å½•äº‹ä»¶åˆ°å®žä½“çš„æ˜ å°„
                    if event_id not in event_to_entities:
                        event_to_entities[event_id] = []
                    event_to_entities[event_id].append(entity_id)

                event_ids = list(event_weights.keys())
                self.logger.info(f"æ‰¾åˆ° {len(event_ids)} ä¸ªç›¸å…³äº‹ä»¶")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤º Entity â†’ Event æ˜ å°„ï¼ˆé€šè¿‡keyå…³è”ï¼‰
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Entity â†’ Event æ˜ å°„:")
                self.logger.info("-" * 80)

                # æž„å»º entity_id â†’ entity_name æ˜ å°„
                entity_id_to_name = {
                    e.id: f"{e.name}({e.type})" for e in found_entities}

                # æŒ‰ entity åˆ†ç»„æ˜¾ç¤ºå…³è”çš„ events
                for entity_id, entity_name in entity_id_to_name.items():
                    related_events = [
                        event_id for event_id, entity_ids in event_to_entities.items()
                        if entity_id in entity_ids
                    ]
                    if related_events:
                        self.logger.info(
                            f"  Entity '{entity_name}' â†’ {len(related_events)} ä¸ªäº‹é¡¹: "
                            f"{', '.join([eid[:8]+'...' for eid in related_events[:5]])}"
                            f"{' ...' if len(related_events) > 5 else ''}"
                        )

                self.logger.info("=" * 80)

                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¯ä¸ªäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯
                for event_id in event_ids:
                    self.logger.debug(
                        f"  äº‹ä»¶ {event_id[:8]}... æƒé‡={event_weights[event_id]:.3f}"
                    )

                # 6. é€šè¿‡äº‹ä»¶æŸ¥æ‰¾å¯¹åº”çš„æ®µè½
                # æ–°æ–¹æ³•ï¼šç›´æŽ¥ä½¿ç”¨äº‹ä»¶çš„ references å­—æ®µæ¥æŸ¥æ‰¾æ®µè½

                # é¦–å…ˆèŽ·å–æ‰€æœ‰äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ references å­—æ®µï¼‰
                event_detail_query = (
                    select(SourceEvent)
                    .where(
                        and_(
                            SourceEvent.source_id.in_(source_config_ids),
                            SourceEvent.id.in_(event_ids)
                        )
                    )
                )
                event_detail_result = await session.execute(event_detail_query)
                events = event_detail_result.scalars().all()

                # æ”¶é›†æ‰€æœ‰äº‹ä»¶å¼•ç”¨çš„æ®µè½ID
                all_referenced_section_ids = set()
                event_to_sections = {}  # äº‹ä»¶ID -> æ®µè½IDåˆ—è¡¨çš„æ˜ å°„

                for event in events:
                    if event.references:
                        event_to_sections[event.id] = event.references
                        all_referenced_section_ids.update(event.references)
                        self.logger.debug(
                            f"äº‹ä»¶ {event.id}... å¼•ç”¨äº† {len(event.references)} ä¸ªæ®µè½"
                        )
                    else:
                        # å¦‚æžœæ²¡æœ‰ referencesï¼Œä½¿ç”¨æ–‡ç« çš„æ‰€æœ‰æ®µè½ï¼ˆå‘åŽå…¼å®¹ï¼‰
                        event_to_sections[event.id] = None
                        self.logger.warning(
                            f"äº‹ä»¶ {event.id}... æ²¡æœ‰ references å­—æ®µ"
                        )

                if not all_referenced_section_ids:
                    self.logger.warning("æ‰€æœ‰äº‹ä»¶éƒ½æ²¡æœ‰å¼•ç”¨ä»»ä½•æ®µè½")
                    return []

                self.logger.info(
                    f"æ”¶é›†åˆ° {len(all_referenced_section_ids)} ä¸ªè¢«å¼•ç”¨çš„æ®µè½ID")

                # æŸ¥è¯¢è¿™äº›æ®µè½çš„è¯¦ç»†ä¿¡æ¯
                article_section_query = (
                    select(ArticleSection, Article)
                    .join(Article, ArticleSection.article_id == Article.id)
                    .where(
                        and_(
                            Article.source_id.in_(source_config_ids),
                            ArticleSection.id.in_(
                                list(all_referenced_section_ids))
                        )
                    )
                    .order_by(ArticleSection.rank)
                )

                section_result = await session.execute(article_section_query)
                sections_data = section_result.fetchall()

                if not sections_data:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³æ®µè½")
                    return []

                self.logger.info(f"æ‰¾åˆ° {len(sections_data)} ä¸ªè¢«å¼•ç”¨çš„æ®µè½")

                # 7. æž„å»ºæ®µè½æ•°æ®å¹¶è®¡ç®—ç›¸ä¼¼åº¦
                # ä½¿ç”¨å­—å…¸æ¥åˆå¹¶åŒä¸€æ®µè½çš„å¤šä¸ªäº‹ä»¶ID
                paragraphs_dict = {}  # key: section_id, value: paragraph data

                # åå‘æ˜ å°„ï¼šsection_id -> [event_ids]
                section_to_events = {}
                for event_id, section_ids in event_to_sections.items():
                    if section_ids:  # å¦‚æžœæœ‰ references
                        for section_id in section_ids:
                            if section_id not in section_to_events:
                                section_to_events[section_id] = []
                            section_to_events[section_id].append(event_id)

                self.logger.debug(f"æž„å»ºäº† {len(section_to_events)} ä¸ªæ®µè½åˆ°äº‹ä»¶çš„æ˜ å°„")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤º Event â†’ Section æ˜ å°„
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Event â†’ Section æ˜ å°„:")
                self.logger.info("-" * 80)

                # æ˜¾ç¤ºæ¯ä¸ªäº‹é¡¹å¼•ç”¨çš„æ®µè½
                for event in events[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªäº‹é¡¹
                    if event.references:
                        section_count = len(event.references)
                        section_preview = ', '.join(
                            [sid[:8]+'...' for sid in event.references[:3]])
                        if section_count > 3:
                            section_preview += f' ... (å…±{section_count}ä¸ª)'
                        self.logger.info(
                            f"  Event {event.id[:8]}... ('{event.title[:30]}') â†’ {section_count} ä¸ªæ®µè½: {section_preview}"
                        )

                if len(events) > 10:
                    self.logger.info(f"  ... (è¿˜æœ‰ {len(events) - 10} ä¸ªäº‹é¡¹æœªæ˜¾ç¤º)")

                self.logger.info("=" * 80)

                # éåŽ†æ‰€æœ‰æ®µè½ï¼Œæž„å»ºæ®µè½æ•°æ®
                for section, article in sections_data:
                    section_id = section.id

                    # æ‰¾åˆ°å¼•ç”¨è¯¥æ®µè½çš„æ‰€æœ‰äº‹ä»¶
                    related_event_ids = section_to_events.get(section_id, [])

                    if not related_event_ids:
                        self.logger.warning(
                            f"æ®µè½ {section_id[:8]}... æ²¡æœ‰æ‰¾åˆ°å…³è”çš„äº‹ä»¶")
                        continue

                    # è®¡ç®—è¯¥æ®µè½çš„ç»¼åˆæƒé‡ï¼ˆæ‰€æœ‰å…³è”äº‹ä»¶çš„æƒé‡ä¹‹å’Œï¼‰
                    total_event_weight = sum(event_weights.get(
                        eid, 1.0) for eid in related_event_ids)

                    # æ”¶é›†è¯¥æ®µè½çš„ cluesï¼ˆä»Žå…³è”çš„äº‹ä»¶ä¸­æ”¶é›†å…³è”çš„å®žä½“ï¼‰
                    paragraph_clues = []
                    seen_entity_ids = set()
                    for event_id in related_event_ids:
                        # èŽ·å–è¯¥äº‹ä»¶å…³è”çš„æ‰€æœ‰å®žä½“
                        entity_ids = event_to_entities.get(event_id, [])
                        for entity_id in entity_ids:
                            if entity_id not in seen_entity_ids:
                                # èŽ·å–å¯¹åº”çš„ key å¯¹è±¡
                                key = entity_to_key.get(entity_id)
                                if key:
                                    paragraph_clues.append(key)
                                    seen_entity_ids.add(entity_id)

                    # åˆ›å»ºæ®µè½æ•°æ®
                    paragraph = {
                        "section_id": section_id,
                        "article_id": article.id,
                        "event_ids": related_event_ids,  # æ‰€æœ‰å…³è”çš„äº‹ä»¶ID
                        "article_title": article.title,
                        "article_category": article.category,
                        "section_rank": section.rank,
                        "section_heading": section.heading,
                        "section_content": section.content,
                        "content_vector": None,  # å°†åœ¨åŽé¢èŽ·å–
                        "entity_weight": total_event_weight,
                        "created_time": section.created_time,
                        "extra_data": section.extra_data or {},
                        "clues": paragraph_clues  # å¬å›žè¯¥æ®µè½çš„ key åˆ—è¡¨
                    }
                    paragraphs_dict[section_id] = paragraph

                    self.logger.debug(
                        f"æ®µè½ {section_id[:8]}... å…³è”äº† {len(related_event_ids)} ä¸ªäº‹ä»¶ï¼Œ"
                        f"ç»¼åˆæƒé‡={total_event_weight:.3f}ï¼Œcluesæ•°={len(paragraph_clues)}"
                    )

                # è½¬æ¢ä¸ºåˆ—è¡¨
                paragraphs_data = list(paragraphs_dict.values())

                # ç»Ÿè®¡ä¿¡æ¯
                multi_event_sections = [
                    p for p in paragraphs_data if len(p["event_ids"]) > 1]
                self.logger.info(
                    f"æž„å»ºäº† {len(paragraphs_data)} ä¸ªå”¯ä¸€æ®µè½æ•°æ®"
                )
                if multi_event_sections:
                    self.logger.info(
                        f"å…¶ä¸­ {len(multi_event_sections)} ä¸ªæ®µè½å…³è”äº†å¤šä¸ªäº‹ä»¶"
                    )

                if not paragraphs_data:
                    self.logger.warning("è¿‡æ»¤åŽæ²¡æœ‰æ‰¾åˆ°çœŸæ­£å…³è”çš„æ®µè½")
                    return []

                # ðŸ†• æ—¥å¿—ï¼šæ±‡æ€»æ˜¾ç¤ºå®Œæ•´å¬å›žè·¯å¾„ç»Ÿè®¡
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„æ±‡æ€»ã€‘å®Œæ•´å¬å›žé“¾:")
                self.logger.info("-" * 80)
                self.logger.info(f"  Keyæ•°é‡: {len(key_final)}")
                self.logger.info(
                    f"  â†’ Entityæ•°é‡: {len(found_entities)} (é€šè¿‡KeyåŒ¹é…)")
                self.logger.info(f"  â†’ Eventæ•°é‡: {len(events)} (é€šè¿‡Entityå…³è”)")
                self.logger.info(
                    f"  â†’ Sectionæ•°é‡: {len(paragraphs_data)} (é€šè¿‡Event.references)")
                self.logger.info("-" * 80)
                self.logger.info(f"  å¬å›žè·¯å¾„: Key â†’ Entity â†’ Event â†’ Section")
                self.logger.info("=" * 80)

                # 7.5. ä»Ž ES æ‰¹é‡èŽ·å–æ®µè½çš„é¢„å­˜å‘é‡
                section_ids_list = list(paragraphs_dict.keys())
                self.logger.info(
                    f"ä»Ž ES æ‰¹é‡èŽ·å– {len(section_ids_list)} ä¸ªæ®µè½çš„é¢„å­˜å‘é‡...")

                es_sections_data = await self.content_repo.get_chunks_by_ids(
                    chunk_ids=section_ids_list,
                    include_vectors=True
                )

                # æž„å»º section_id -> content_vector çš„æ˜ å°„
                section_vector_map = {}
                for es_section in es_sections_data:
                    chunk_id = es_section.get('chunk_id')
                    content_vector = es_section.get('content_vector')
                    if chunk_id and content_vector:
                        section_vector_map[chunk_id] = content_vector

                self.logger.info(
                    f"ä»Ž ES èŽ·å–åˆ° {len(section_vector_map)} ä¸ªæ®µè½çš„é¢„å­˜å‘é‡ "
                    f"(è¯·æ±‚äº† {len(section_ids_list)} ä¸ª)"
                )

                # å°†èŽ·å–åˆ°çš„å‘é‡å¡«å……åˆ°æ®µè½æ•°æ®ä¸­
                filled_count = 0
                for paragraph in paragraphs_data:
                    section_id = paragraph['section_id']
                    if section_id in section_vector_map:
                        paragraph['content_vector'] = section_vector_map[section_id]
                        filled_count += 1
                    else:
                        self.logger.warning(
                            f"æ®µè½ {section_id[:8]}... åœ¨ ES ä¸­æœªæ‰¾åˆ°é¢„å­˜å‘é‡ï¼Œå°†çŽ°åœºç”Ÿæˆ"
                        )

                self.logger.info(
                    f"æˆåŠŸå¡«å…… {filled_count}/{len(paragraphs_data)} ä¸ªæ®µè½çš„é¢„å­˜å‘é‡"
                )

                # 8. è®¡ç®—å‘é‡ç›¸ä¼¼åº¦å¾—åˆ†
                similarity_scores = await self._calculate_cosine_scores(
                    query_vector=query_vector,
                    paragraphs=paragraphs_data
                )
                self.logger.debug(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")

                # 9. ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆå¾—åˆ†
                content_results = []

                # æ·»åŠ è¯¦ç»†å¾—åˆ†æ—¥å¿—ï¼ˆä¸Žæ­¥éª¤2å¯¹åº”ï¼‰
                self.logger.info("=" * 80)
                self.logger.info("æ­¥éª¤1 å¾—åˆ†è¯¦æƒ…ï¼ˆçº¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨ESé¢„å­˜å‘é‡ï¼‰ï¼š")
                self.logger.info("-" * 80)

                for idx, paragraph in enumerate(paragraphs_data, start=1):
                    section_id = paragraph["section_id"]

                    # ç›´æŽ¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†ï¼ˆèŒƒå›´é€šå¸¸åœ¨[0, 1]ä¹‹é—´ï¼‰
                    cosine_score = similarity_scores.get(section_id, 0.0)

                    # èŽ·å–è¯¥æ®µè½å…³è”çš„æ‰€æœ‰äº‹ä»¶ID
                    event_ids = paragraph["event_ids"]

                    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ªæ®µè½çš„å¾—åˆ†å’Œå…³è”çš„äº‹ä»¶æ•°
                    heading_preview = paragraph.get("section_heading", "")[:40]
                    self.logger.info(
                        f"æ®µè½ {section_id[:8]}... | "
                        f"Cosine={cosine_score:.4f} | "
                        f"å…³è”äº‹ä»¶æ•°={len(event_ids)} | "
                        f"æ ‡é¢˜: {heading_preview}"
                    )

                    # DEBUGçº§åˆ«ï¼šæ˜¾ç¤ºæ‰€æœ‰å…³è”çš„äº‹ä»¶ID
                    if len(event_ids) > 1:
                        event_ids_preview = [
                            eid[:8] + "..." for eid in event_ids]
                        self.logger.debug(f"  äº‹ä»¶IDåˆ—è¡¨: {event_ids_preview}")

                    # åˆ›å»º ContentSearchResult å¯¹è±¡ï¼Œæ·»åŠ ç¼–å·å’Œ clues
                    result = ContentSearchResult(
                        search_type=f"SQL-{idx}",  # æ·»åŠ ç¼–å·
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªsource_idä½œä¸ºä¸»source_id
                        source_id=source_config_ids[0] if source_config_ids else "",
                        article_id=paragraph["article_id"],
                        section_id=section_id,
                        rank=paragraph["section_rank"],
                        heading=paragraph["section_heading"],
                        content=paragraph["section_content"],
                        score=cosine_score,
                        event_ids=event_ids,  # è®°å½•æ‰€æœ‰å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
                        clues=paragraph.get("clues", []),  # è®°å½•å¬å›žè¯¥æ®µè½çš„ key åˆ—è¡¨
                    )

                    content_results.append(result)

                self.logger.info("=" * 80)

                # 10. æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æŽ’åº
                content_results.sort(key=lambda x: x.score, reverse=True)

                # 11. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦ç»“æžœ
                original_count = len(content_results)
                if config and config.rerank.score_threshold:
                    filtered_results = [
                        r for r in content_results if r.score >= config.rerank.score_threshold]

                    if len(filtered_results) < original_count:
                        self.logger.info(
                            f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªæ®µè½ "
                            f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                        )

                        # å±•ç¤ºè¿‡æ»¤åŽä¿ç•™çš„æ®µè½ä¿¡æ¯
                        if filtered_results:
                            self.logger.info("=" * 80)
                            self.logger.info(
                                f"è¿‡æ»¤åŽä¿ç•™çš„ {len(filtered_results)} ä¸ªæ®µè½ï¼š")
                            self.logger.info("-" * 80)
                            for result in filtered_results:
                                heading_preview = result.heading[:
                                                                 40] if result.heading else "æ— æ ‡é¢˜"
                                self.logger.info(
                                    f"æ®µè½ {result.section_id[:8]}... | "
                                    f"Cosine={result.score:.4f} | "
                                    f"æ ‡é¢˜: {heading_preview}"
                                )
                            self.logger.info("=" * 80)

                    content_results = filtered_results
                else:
                    self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤ºè¿‡æ»¤åŽçš„æœ‰æ•ˆæ˜ å°„å…³ç³»
                if content_results:
                    # æ”¶é›†è¿‡æ»¤åŽçš„æ‰€æœ‰ section_ids å’Œ event_ids
                    filtered_section_ids = {
                        r.section_id for r in content_results}
                    filtered_event_ids = set()
                    for r in content_results:
                        filtered_event_ids.update(r.event_ids)

                    self.logger.info("=" * 80)
                    self.logger.info("ã€Step1 å¬å›žè·¯å¾„è¿‡æ»¤åŽã€‘æœ‰æ•ˆæ˜ å°„å…³ç³»:")
                    self.logger.info("-" * 80)

                    # 1. Entity â†’ Event æ˜ å°„ï¼ˆåªæ˜¾ç¤ºæœ‰ä¿ç•™æ®µè½çš„äº‹é¡¹ï¼‰
                    self.logger.info("  1ï¸âƒ£ Entity â†’ Event (ä»…ä¿ç•™æœ‰æ•ˆäº‹é¡¹):")
                    entity_event_count = {}

                    # ðŸ†• åˆ›å»º event_id â†’ event_title çš„æ˜ å°„
                    event_title_map = {
                        event.id: event.title for event in events}

                    for entity_id, entity_name in entity_id_to_name.items():
                        # æ‰¾åˆ°è¯¥å®žä½“å…³è”çš„ã€ä¸”æœ‰ä¿ç•™æ®µè½çš„äº‹é¡¹
                        valid_events = [
                            event_id for event_id, entity_ids in event_to_entities.items()
                            if entity_id in entity_ids and event_id in filtered_event_ids
                        ]
                        if valid_events:
                            entity_event_count[entity_name] = len(valid_events)

                            # ðŸ†• æ˜¾ç¤ºäº‹é¡¹IDå’Œæ ‡é¢˜
                            events_preview_parts = []
                            for eid in valid_events[:3]:
                                event_title = event_title_map.get(eid, "")
                                title_preview = event_title[:30] if event_title else "æ— æ ‡é¢˜"
                                events_preview_parts.append(
                                    f"{eid[:8]}...({title_preview})")

                            events_preview = ', '.join(events_preview_parts)
                            if len(valid_events) > 3:
                                events_preview += f' ... (å…±{len(valid_events)}ä¸ª)'

                            self.logger.info(
                                f"     {entity_name} â†’ {len(valid_events)} ä¸ªäº‹é¡¹: {events_preview}")

                    # 2. Event â†’ Section æ˜ å°„ï¼ˆåªæ˜¾ç¤ºä¿ç•™çš„æ®µè½ï¼‰
                    self.logger.info("")
                    self.logger.info("  2ï¸âƒ£ Event â†’ Section (ä»…ä¿ç•™æ®µè½):")
                    event_section_count = {}
                    displayed_count = 0

                    # ðŸ†• åˆ›å»º section_id â†’ heading çš„æ˜ å°„
                    section_heading_map = {}
                    for content in content_results:
                        section_heading_map[content.section_id] = content.heading or ""

                    # ðŸ†• åªéåŽ†æœ‰æ•ˆçš„äº‹é¡¹ï¼ˆåœ¨ filtered_event_ids ä¸­çš„ï¼‰
                    for event in events:
                        if event.id in filtered_event_ids:
                            # æ‰¾åˆ°è¯¥äº‹é¡¹å…³è”çš„ã€ä¸”è¢«ä¿ç•™çš„æ®µè½
                            valid_sections = [
                                sid for sid in (event.references or [])
                                if sid in filtered_section_ids
                            ]
                            if valid_sections:
                                event_section_count[event.id] = len(
                                    valid_sections)

                                # ðŸ†• æ˜¾ç¤ºæ®µè½IDå’Œæ ‡é¢˜
                                sections_preview_parts = []
                                for sid in valid_sections[:3]:
                                    section_heading = section_heading_map.get(
                                        sid, "")
                                    heading_preview = section_heading[:
                                                                      30] if section_heading else "æ— æ ‡é¢˜"
                                    sections_preview_parts.append(
                                        f"{sid[:8]}...({heading_preview})")

                                sections_preview = ', '.join(
                                    sections_preview_parts)
                                if len(valid_sections) > 3:
                                    sections_preview += f' ... (å…±{len(valid_sections)}ä¸ª)'

                                self.logger.info(
                                    f"     Event {event.id[:8]}... ('{event.title[:30]}') "
                                    f"â†’ {len(valid_sections)} ä¸ªæ®µè½: {sections_preview}"
                                )
                                displayed_count += 1

                                # é™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼ˆé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                                if displayed_count >= 10:
                                    break

                    # ç»Ÿè®¡æ‰€æœ‰æœ‰æ•ˆäº‹é¡¹
                    total_valid_events = sum(
                        1 for e in events if e.id in filtered_event_ids)
                    if displayed_count < total_valid_events:
                        self.logger.info(
                            f"     ... (è¿˜æœ‰ {total_valid_events - displayed_count} ä¸ªæœ‰æ•ˆäº‹é¡¹æœªæ˜¾ç¤º)")

                    # 3. ç»Ÿè®¡æ±‡æ€»
                    self.logger.info("")
                    self.logger.info("  ðŸ“Š è¿‡æ»¤æ•ˆæžœç»Ÿè®¡:")
                    self.logger.info(
                        f"     è¿‡æ»¤å‰: {len(found_entities)} ä¸ªEntity â†’ {len(events)} ä¸ªEvent â†’ {len(paragraphs_data)} ä¸ªSection")
                    self.logger.info(
                        f"     è¿‡æ»¤åŽ: {len(entity_event_count)} ä¸ªæœ‰æ•ˆEntity â†’ {len(filtered_event_ids)} ä¸ªæœ‰æ•ˆEvent â†’ {len(filtered_section_ids)} ä¸ªæœ‰æ•ˆSection")
                    self.logger.info(
                        f"     è¿‡æ»¤çŽ‡: Entity={len(entity_event_count)/len(found_entities)*100:.1f}%, Event={len(filtered_event_ids)/len(events)*100:.1f}%, Section={len(filtered_section_ids)/len(paragraphs_data)*100:.1f}%")

                    self.logger.info("=" * 80)

                    # ðŸ†• æž„å»º Step1 é˜¶æ®µçš„çº¿ç´¢
                    from sag.modules.search.tracker import Tracker
                    tracker = Tracker(config)

                    # å‡†å¤‡æ•°æ®æ˜ å°„
                    # 1. entity_id â†’ entity_weight æ˜ å°„
                    entity_weight_map = {key["key_id"]: key.get(
                        "weight", 0.0) for key in key_final}

                    # 2. section_id â†’ section_data æ˜ å°„ï¼ˆåŒ…å« scoreï¼‰
                    section_data_map = {}
                    for content in content_results:
                        section_data_map[content.section_id] = {
                            "section_id": content.section_id,
                            "id": content.section_id,
                            "heading": content.heading or "",
                            "content": content.content or "",
                            "summary": "",
                            "section_type": getattr(content, 'search_type', ''),
                            "score": content.score  # ä½™å¼¦ç›¸ä¼¼åº¦
                        }

                    # A. æž„å»º Entity â†’ Event çº¿ç´¢
                    entity_event_clue_count = 0
                    for entity in found_entities:
                        entity_id = entity.id
                        entity_weight = entity_weight_map.get(entity_id, 0.0)

                        # æ‰¾åˆ°è¿™ä¸ªå®žä½“å…³è”çš„æ‰€æœ‰æœ‰æ•ˆäº‹é¡¹
                        for event in events:
                            if event.id in filtered_event_ids:
                                # æ£€æŸ¥è¿™ä¸ªäº‹é¡¹æ˜¯å¦å…³è”è¿™ä¸ªå®žä½“
                                if entity_id in event_to_entities.get(event.id, []):
                                    # æž„å»ºèŠ‚ç‚¹
                                    entity_node = Tracker.build_entity_node({
                                        "key_id": entity_id,
                                        "id": entity_id,
                                        "name": entity.name,
                                        "type": entity.type,
                                        "description": entity.description or ""
                                    })
                                    # ðŸ†• ä½¿ç”¨ tracker å®žä¾‹æ–¹æ³•ï¼ŒæŒ‡å®šå¬å›žæ–¹å¼ä¸º "entity"
                                    event_node = tracker.get_or_create_event_node(
                                        event, "rerank", recall_method="entity")

                                    # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨å®žä½“æƒé‡ï¼‰
                                    tracker.add_clue(
                                        stage="rerank",
                                        from_node=entity_node,
                                        to_node=event_node,
                                        confidence=entity_weight,
                                        relation="å®žä½“å¬å›ž",
                                        metadata={
                                            "method": "entity_recall",
                                            "entity_weight": entity_weight,
                                            "step": "step1"
                                        }
                                    )
                                    entity_event_clue_count += 1

                    # B. æž„å»º Event â†’ Section çº¿ç´¢
                    event_section_clue_count = 0
                    for event in events:
                        if event.id in filtered_event_ids:
                            # éåŽ†è¿™ä¸ªäº‹é¡¹å¼•ç”¨çš„æ‰€æœ‰æ®µè½
                            for section_id in (event.references or []):
                                if section_id in filtered_section_ids:
                                    # èŽ·å–æ®µè½æ•°æ®
                                    section_data = section_data_map.get(
                                        section_id)
                                    if section_data:
                                        # æž„å»ºèŠ‚ç‚¹
                                        # ðŸ†• ä½¿ç”¨ tracker å®žä¾‹æ–¹æ³•ï¼ŒæŒ‡å®šå¬å›žæ–¹å¼ä¸º "entity"ï¼ˆå› ä¸ºæ˜¯é€šè¿‡entityæ‰¾åˆ°çš„eventï¼‰
                                        event_node = tracker.get_or_create_event_node(
                                            event, "rerank", recall_method="entity")
                                        section_node = Tracker.build_section_node(
                                            section_data)

                                        # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                                        tracker.add_clue(
                                            stage="rerank",
                                            from_node=event_node,
                                            to_node=section_node,
                                            confidence=section_data["score"],
                                            relation="æ®µè½å¬å›ž",
                                            metadata={
                                                "method": "section_recall",
                                                "section_score": section_data["score"],
                                                "step": "step1"
                                            }
                                        )
                                        event_section_clue_count += 1

                    self.logger.info("=" * 80)
                    self.logger.info(
                        f"ðŸ”— [Step1 çº¿ç´¢æž„å»º] Entityâ†’Event={entity_event_clue_count}æ¡, "
                        f"Eventâ†’Section={event_section_clue_count}æ¡"
                    )
                    self.logger.info("=" * 80)

                self.logger.info(
                    f"æ­¥éª¤1å®Œæˆ: å¤„ç†äº† {len(content_results)} ä¸ªæ®µè½",
                    extra={
                        "avg_cosine_score": np.mean([r.score for r in content_results]) if content_results else 0.0
                    }
                )

                # æ˜¾ç¤ºTop 5ç»“æžœ
                top_results = content_results[:5]
                for i, result in enumerate(top_results, 1):
                    self.logger.debug(
                        f"Top {i}: {result.heading[:50]} - "
                        f"Cosine:{result.score:.3f}"
                    )

                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨è¿”å›ž
                return [r.to_dict() for r in content_results]

        except Exception as e:
            self.logger.error(f"æ­¥éª¤1æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step2_query_to_contents(
        self,
        query: str,
        source_config_ids: List[str],
        k: int = 20,
        query_vector: Optional[List[float]] = None,  # å¯é€‰çš„æŸ¥è¯¢å‘é‡
        config: Optional[SearchConfig] = None  # æ·»åŠ configå‚æ•°ç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤2: queryæ‰¾contentï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°åŽŸæ–‡å—[content-query-related]ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_id: æ•°æ®æºID
            k: è¿”å›žç»“æžœæ•°é‡
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤ï¼‰

        Returns:
            ç›¸å…³æ®µè½åˆ—è¡¨ï¼ˆContentSearchResult.to_dict()æ ¼å¼ï¼‰
        """
        try:
            self.logger.info(
                f"æ­¥éª¤2å¼€å§‹: query='{query}', source_config_ids={source_config_ids}")

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æžœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
            if config and config.has_query_embedding:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # 2. ä½¿ç”¨KNNæœç´¢æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ
            similar_paragraphs = await self._search_similar_paragraphs(
                query_vector=query_vector,
                source_config_ids=source_config_ids,
                k=k
            )
            self.logger.info(f"KNNæœç´¢æ‰¾åˆ° {len(similar_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")

            if not similar_paragraphs:
                self.logger.warning("æœªæ‰¾åˆ°ç›¸ä¼¼æ®µè½")
                return []

            # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
            cosine_scores = await self._calculate_cosine_scores(
                query_vector=query_vector,
                paragraphs=similar_paragraphs
            )

            # 4. ä¸ºæ¯ä¸ªæ®µè½æ·»åŠ å¾—åˆ†ä¿¡æ¯å¹¶åˆ›å»º ContentSearchResult å¯¹è±¡
            content_results = []

            # æ·»åŠ è¯¦ç»†å¾—åˆ†æ—¥å¿—
            self.logger.info("=" * 80)
            self.logger.info("æ­¥éª¤2 å¾—åˆ†è¯¦æƒ…ï¼ˆçº¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼š")
            self.logger.info("-" * 80)

            for idx, paragraph in enumerate(similar_paragraphs, start=1):
                section_id = paragraph.get("section_id")
                cosine_score = cosine_scores.get(section_id, 0.0)

                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ªæ®µè½çš„å¾—åˆ†
                heading_preview = paragraph.get("heading", "")[:40]
                self.logger.info(
                    f"æ®µè½ {section_id[:8]}... | "
                    f"Cosine={cosine_score:.4f} | "
                    f"æ ‡é¢˜: {heading_preview}"
                )

                # åˆ›å»º ContentSearchResult å¯¹è±¡ï¼Œæ·»åŠ ç¼–å·å’Œ clues
                # Embedding æœç´¢ä½¿ç”¨ query ä½œä¸º clue
                query_clue = {
                    "type": "query",
                    "name": query,
                    "weight": 1.0,
                    "source": "embedding"
                }

                result = ContentSearchResult(
                    search_type=f"embedding-{idx}",  # æ·»åŠ ç¼–å·
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªsource_idä½œä¸ºä¸»source_id
                    source_id=source_config_ids[0] if source_config_ids else "",
                    article_id=paragraph.get("article_id", ""),
                    section_id=section_id,
                    rank=paragraph.get("rank", 0),
                    heading=paragraph.get("heading", ""),
                    content=paragraph.get("content", ""),
                    score=cosine_score,  # ç›´æŽ¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†
                    event_ids=[],  # embeddingæœç´¢æ²¡æœ‰ç›´æŽ¥å…³è”çš„event_id
                    clues=[query_clue],  # ä½¿ç”¨ query ä½œä¸ºå¬å›žçº¿ç´¢
                )
                content_results.append(result)

            self.logger.info("=" * 80)

            # 5. æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æŽ’åº
            content_results.sort(key=lambda x: x.score, reverse=True)

            # 6. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦ç»“æžœ
            original_count = len(content_results)
            if config and config.rerank.score_threshold:
                filtered_results = [
                    r for r in content_results if r.score >= config.rerank.score_threshold]

                if len(filtered_results) < original_count:
                    self.logger.info(
                        f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªæ®µè½ "
                        f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                    )

                    # å±•ç¤ºè¿‡æ»¤åŽä¿ç•™çš„æ®µè½ä¿¡æ¯
                    if filtered_results:
                        self.logger.info("=" * 80)
                        self.logger.info(
                            f"è¿‡æ»¤åŽä¿ç•™çš„ {len(filtered_results)} ä¸ªæ®µè½ï¼š")
                        self.logger.info("-" * 80)
                        for result in filtered_results:
                            heading_preview = result.heading[:
                                                             40] if result.heading else "æ— æ ‡é¢˜"
                            self.logger.info(
                                f"æ®µè½ {result.section_id[:8]}... | "
                                f"Cosine={result.score:.4f} | "
                                f"æ ‡é¢˜: {heading_preview}"
                            )
                        self.logger.info("=" * 80)

                content_results = filtered_results
            else:
                self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

            self.logger.info(
                f"æ­¥éª¤2å®Œæˆ: å¤„ç†äº† {len(content_results)} ä¸ªæ®µè½",
                extra={
                    "avg_cosine_score": np.mean([r.score for r in content_results]) if content_results else 0.0
                }
            )

            # æ˜¾ç¤ºTop 5ç»“æžœ
            top_results = content_results[:5]
            for i, result in enumerate(top_results, 1):
                self.logger.debug(
                    f"Top {i}: {result.heading[:50]} - "
                    f"Cosine:{result.score:.3f}"
                )

            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨è¿”å›ž
            return [r.to_dict() for r in content_results]

        except Exception as e:
            self.logger.error(f"æ­¥éª¤2æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _generate_query_vector(self, query: str, config: SearchConfig = None) -> List[float]:
        """
        å°†queryè½¬åŒ–æˆå‘é‡

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽç¼“å­˜query_vectorï¼‰

        Returns:
            æŸ¥è¯¢å‘é‡
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_vectorï¼ˆå¦‚æžœæœ‰configä¼ å…¥ï¼‰
            if config and config.has_query_embedding and config.query_embedding:
                self.logger.debug(
                    f"ðŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(config.query_embedding)}")
                return config.query_embedding

            # ä½¿ç”¨processorç”Ÿæˆå‘é‡
            query_vector = await self.processor.generate_embedding(query)
            self.logger.debug(f"Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # å¦‚æžœæœ‰configï¼Œç¼“å­˜query_vector
            if config:
                config.query_embedding = query_vector
                config.has_query_embedding = True
                self.logger.debug("ðŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")

            return query_vector
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

    async def _search_similar_paragraphs(
        self,
        query_vector: List[float],
        source_config_ids: List[str],
        k: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨KNNæœç´¢æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ

        éåŽ†æ‰€æœ‰æ•°æ®æºï¼Œåˆå¹¶æœç´¢ç»“æžœ

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: æ¯ä¸ªæ•°æ®æºè¿”å›žçš„æ•°é‡

        Returns:
            ç›¸ä¼¼æ®µè½åˆ—è¡¨ï¼ˆåˆå¹¶æ‰€æœ‰æ•°æ®æºçš„ç»“æžœï¼‰
        """
        try:
            all_paragraphs = []

            # å¦‚æžœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œåˆ™æœç´¢æ‰€æœ‰æ•°æ®æº
            if not source_config_ids:
                self.logger.info("æœªæŒ‡å®šæ•°æ®æºï¼Œæœç´¢æ‰€æœ‰æ•°æ®æº")
                similar_paragraphs = await self.content_repo.search_similar_by_content(
                    query_vector=query_vector,
                    k=k,
                    source_id=None
                )
                all_paragraphs.extend(similar_paragraphs)
            else:
                # éåŽ†æ¯ä¸ªæ•°æ®æºè¿›è¡Œæœç´¢
                self.logger.info(f"éåŽ† {len(source_config_ids)} ä¸ªæ•°æ®æºè¿›è¡ŒKNNæœç´¢")
                for source_id in source_config_ids:
                    try:
                        similar_paragraphs = await self.content_repo.search_similar_by_content(
                            query_vector=query_vector,
                            k=k,
                            source_id=source_id  # ä½¿ç”¨å•ä¸ª source_id
                        )
                        all_paragraphs.extend(similar_paragraphs)
                        self.logger.debug(
                            f"æ•°æ®æº {source_id[:8]}... æ‰¾åˆ° {len(similar_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
                    except Exception as e:
                        self.logger.warning(
                            f"æ•°æ®æº {source_id[:8]}... æœç´¢å¤±è´¥: {e}")
                        continue

            self.logger.info(f"KNNæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
            return all_paragraphs

        except Exception as e:
            self.logger.error(f"KNNæœç´¢å¤±è´¥: {e}")
            raise AIError(f"KNNæœç´¢å¤±è´¥: {e}") from e

    async def _calculate_cosine_scores(
        self,
        query_vector: List[float],
        paragraphs: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            paragraphs: æ®µè½åˆ—è¡¨

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å­—å…¸ {section_id: score}
        """
        try:
            if not paragraphs:
                return {}

            cosine_scores = {}

            for paragraph in paragraphs:
                section_id = paragraph.get("section_id")

                # ä»Žæ®µè½ä¸­èŽ·å–é¢„å­˜çš„å‘é‡
                content_vector = paragraph.get("content_vector")

                if content_vector:
                    # ç›´æŽ¥ä½¿ç”¨ESè¿”å›žçš„é¢„å­˜å‘é‡
                    pass
                else:
                    # å¦‚æžœæ²¡æœ‰é¢„å­˜å‘é‡ï¼ŒçŽ°åœºç”Ÿæˆï¼ˆåªå¯¹contentç”Ÿæˆï¼Œä¸åŒ…å«æ ‡é¢˜ï¼‰
                    content = paragraph.get(
                        'section_content') or paragraph.get('content', '')
                    if not content.strip():
                        # å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡
                        self.logger.warning(f"æ®µè½ {section_id} å†…å®¹ä¸ºç©ºä¸”æ— é¢„å­˜å‘é‡ï¼Œè·³è¿‡")
                        continue

                    content_vector = await self.processor.generate_embedding(content)

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = await self._cosine_similarity(query_vector, content_vector)
                cosine_scores[section_id] = similarity

            self.logger.info(
                f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ - å…± {len(cosine_scores)} ä¸ªæ®µè½, "
                f"å¹³å‡ç›¸ä¼¼åº¦: {np.mean(list(cosine_scores.values())):.4f}"
            )

            return cosine_scores

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

    async def _tokenize_text(self, text: str) -> List[str]:
        """
        ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åˆ†è¯åˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        # è½¬æ¢ä¸ºå°å†™å¹¶ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text.lower())

        # ä½¿ç”¨jiebaåˆ†è¯ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
        try:
            import jieba
            tokens = list(jieba.cut(text))
        except ImportError:
            # å¦‚æžœjiebaæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„ç©ºæ ¼åˆ†è¯
            self.logger.warning("jiebaæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•åˆ†è¯")
            tokens = text.split()

        # åªè¿‡æ»¤ç©ºç™½tokenï¼Œä¿ç•™å•å­—ç¬¦ï¼ˆä¸­æ–‡å•å­—ä¹Ÿå¯èƒ½æœ‰æ„ä¹‰ï¼‰
        tokens = [t.strip() for t in tokens if t.strip()]

        return tokens

    async def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            v1 = np.array(vec1)
            v2 = np.array(vec2)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(v1, v2)
            norm_v1 = np.linalg.norm(v1)
            norm_v2 = np.linalg.norm(v2)

            if norm_v1 == 0 or norm_v2 == 0:
                return 0.0

            similarity = dot_product / (norm_v1 * norm_v2)
            return float(similarity)

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def _batch_cosine_similarity(
        self,
        query_vector: List[float],
        target_vectors: List[List[float]]
    ) -> np.ndarray:
        """
        æ‰¹é‡è®¡ç®—queryå‘é‡ä¸Žå¤šä¸ªç›®æ ‡å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            target_vectors: ç›®æ ‡å‘é‡åˆ—è¡¨

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦æ•°ç»„
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            query_array = np.array(query_vector)
            target_array = np.array(target_vectors)

            # è®¡ç®—ç‚¹ç§¯
            dot_products = np.dot(target_array, query_array)

            # è®¡ç®—èŒƒæ•°
            query_norm = np.linalg.norm(query_array)
            target_norms = np.linalg.norm(target_array, axis=1)

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆé¿å…é™¤ä»¥é›¶ï¼‰
            denominators = target_norms * query_norm
            similarities = np.where(
                denominators > 0,
                dot_products / denominators,
                0.0
            )

            return similarities

        except Exception as e:
            self.logger.error(f"æ‰¹é‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return np.zeros(len(target_vectors))

    def _calculate_weighted_similarity(
        self,
        query_vector: List[float],
        title_vectors: List[Optional[List[float]]],
        content_vectors: List[Optional[List[float]]],
        title_weight: float = 0.2,
        content_weight: float = 0.8
    ) -> np.ndarray:
        """
        è®¡ç®—åŠ æƒç›¸ä¼¼åº¦ï¼šqueryåˆ†åˆ«ä¸Žtitleå’Œcontentæ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦ï¼Œç„¶åŽæŒ‰æƒé‡åˆå¹¶

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            title_vectors: æ ‡é¢˜å‘é‡åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«Noneï¼‰
            content_vectors: å†…å®¹å‘é‡åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å«Noneï¼‰
            title_weight: æ ‡é¢˜ç›¸ä¼¼åº¦æƒé‡ï¼Œé»˜è®¤0.2
            content_weight: å†…å®¹ç›¸ä¼¼åº¦æƒé‡ï¼Œé»˜è®¤0.8

        Returns:
            åŠ æƒç›¸ä¼¼åº¦æ•°ç»„
        """
        try:
            num_items = len(title_vectors)

            # åˆå§‹åŒ–ç›¸ä¼¼åº¦æ•°ç»„
            title_similarities = np.zeros(num_items)
            content_similarities = np.zeros(num_items)

            # 1. æ‰¹é‡è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦
            # æ”¶é›†æ‰€æœ‰éžNoneçš„æ ‡é¢˜å‘é‡åŠå…¶ç´¢å¼•
            valid_title_indices = []
            valid_title_vectors = []
            for i, vec in enumerate(title_vectors):
                if vec is not None:
                    valid_title_indices.append(i)
                    valid_title_vectors.append(vec)

            # æ‰¹é‡è®¡ç®—æ‰€æœ‰æœ‰æ•ˆæ ‡é¢˜å‘é‡çš„ç›¸ä¼¼åº¦
            if valid_title_vectors:
                batch_title_sims = self._batch_cosine_similarity(
                    query_vector,
                    valid_title_vectors
                )
                # å°†ç»“æžœå¡«å›žå¯¹åº”çš„ç´¢å¼•ä½ç½®
                for idx, sim in zip(valid_title_indices, batch_title_sims):
                    title_similarities[idx] = sim

            # 2. æ‰¹é‡è®¡ç®—å†…å®¹ç›¸ä¼¼åº¦
            # æ”¶é›†æ‰€æœ‰éžNoneçš„å†…å®¹å‘é‡åŠå…¶ç´¢å¼•
            valid_content_indices = []
            valid_content_vectors = []
            for i, vec in enumerate(content_vectors):
                if vec is not None:
                    valid_content_indices.append(i)
                    valid_content_vectors.append(vec)

            # æ‰¹é‡è®¡ç®—æ‰€æœ‰æœ‰æ•ˆå†…å®¹å‘é‡çš„ç›¸ä¼¼åº¦
            if valid_content_vectors:
                batch_content_sims = self._batch_cosine_similarity(
                    query_vector,
                    valid_content_vectors
                )
                # å°†ç»“æžœå¡«å›žå¯¹åº”çš„ç´¢å¼•ä½ç½®
                for idx, sim in zip(valid_content_indices, batch_content_sims):
                    content_similarities[idx] = sim

            # 3. å‘é‡åŒ–åŠ æƒåˆå¹¶
            weighted_similarities = (
                title_weight * title_similarities + content_weight * content_similarities
            )

            return weighted_similarities

        except Exception as e:
            self.logger.error(f"åŠ æƒç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return np.zeros(num_items)

    async def _step3_merge_result(
        self,
        step1_results: List[Dict[str, Any]],
        step2_results: List[Dict[str, Any]],
        config: SearchConfig
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3: åˆå¹¶æ­¥éª¤1å’Œæ­¥éª¤2çš„ç»“æžœï¼Œå¹¶åŽ»é‡

        åŽ»é‡è§„åˆ™ï¼šå¦‚æžœ article_id + section_id ç›¸åŒï¼Œåªä¿ç•™ SQL æœç´¢çš„ç»“æžœï¼ˆstep1ï¼‰

        Args:
            step1_results: æ­¥éª¤1çš„ç»“æžœï¼ˆSQLæœç´¢ï¼‰
            step2_results: æ­¥éª¤2çš„ç»“æžœï¼ˆEmbeddingæœç´¢ï¼‰
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽæž„å»ºçº¿ç´¢ï¼‰

        Returns:
            åˆå¹¶å¹¶åŽ»é‡åŽçš„ç»“æžœåˆ—è¡¨ï¼ˆæŒ‰å¾—åˆ†é™åºæŽ’åºï¼‰
        """
        self.logger.info(
            f"æ­¥éª¤3å¼€å§‹: åˆå¹¶ step1({len(step1_results)}ä¸ª) + step2({len(step2_results)}ä¸ª) å¹¶åŽ»é‡"
        )

        # 1. å…ˆè®°å½• SQL æœç´¢ç»“æžœä¸­æ‰€æœ‰çš„ (article_id, section_id)
        sql_sections = set()
        for result in step1_results:
            article_id = result.get('article_id')
            section_id = result.get('section_id')
            sql_sections.add((article_id, section_id))

        self.logger.debug(f"SQL æœç´¢æ‰¾åˆ° {len(sql_sections)} ä¸ªå”¯ä¸€æ®µè½")

        # 2. éåŽ† embedding ç»“æžœï¼Œè¿‡æ»¤æŽ‰å·²ç»åœ¨ SQL ç»“æžœä¸­çš„æ®µè½
        filtered_embedding_results = []
        duplicate_count = 0

        for result in step2_results:
            article_id = result.get('article_id')
            section_id = result.get('section_id')
            section_key = (article_id, section_id)

            if section_key in sql_sections:
                # è¿™ä¸ªæ®µè½å·²ç»åœ¨ SQL ç»“æžœä¸­ï¼Œè·³è¿‡
                duplicate_count += 1
                self.logger.debug(
                    f"æ®µè½ {section_id[:8]}... åœ¨ SQL å’Œ Embedding ä¸­éƒ½æ‰¾åˆ°ï¼Œä¿ç•™ SQL ç»“æžœ"
                )
            else:
                # è¿™æ˜¯æ–°æ®µè½ï¼Œä¿ç•™
                filtered_embedding_results.append(result)

        self.logger.info(
            f"åŽ»é‡ç»Ÿè®¡: Embedding ç»“æžœä¸­æœ‰ {duplicate_count} ä¸ªä¸Ž SQL é‡å¤çš„æ®µè½å·²ç§»é™¤"
        )

        # ðŸ†• æ˜¾ç¤º Embedding ä¸­è¿›å…¥ä¸‹ä¸€è½®çš„æ®µè½ï¼ˆè¡¥å……ä½œç”¨ï¼‰
        if filtered_embedding_results:
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€Step2 æ‰©å±•è¡¥å……ã€‘{len(filtered_embedding_results)} ä¸ª Embedding æ®µè½è¿›å…¥ä¸‹ä¸€è½®:")
            self.logger.info("-" * 80)
            for r in filtered_embedding_results[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                section_id = r.get('section_id', '')
                heading = r.get('heading', '')
                score = r.get('score', 0.0)
                search_type = r.get('search_type', '')
                heading_preview = heading[:40] if heading else "æ— æ ‡é¢˜"
                self.logger.info(
                    f"  {section_id[:8]}... | Cosine={score:.4f} | Type={search_type} | {heading_preview}"
                )
            if len(filtered_embedding_results) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(filtered_embedding_results) - 10} ä¸ª)")
            self.logger.info("=" * 80)

            # ðŸ†• æž„å»º query â†’ section çº¿ç´¢ï¼ˆStep2 embeddingå¬å›žçš„æ®µè½ï¼‰
            from sag.modules.search.tracker import Tracker
            tracker = Tracker(config)

            query_section_clue_count = 0
            for r in filtered_embedding_results:
                section_id = r.get('section_id', '')
                if not section_id:
                    continue

                # æž„å»º query èŠ‚ç‚¹
                query_node = Tracker.build_query_node(config)

                # æž„å»º section èŠ‚ç‚¹
                section_node = Tracker.build_section_node({
                    "section_id": section_id,
                    "id": section_id,
                    "heading": r.get('heading', ''),
                    "content": r.get('content', ''),
                    "summary": "",
                    "section_type": r.get('search_type', '')
                })

                # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                tracker.add_clue(
                    stage="rerank",
                    from_node=query_node,
                    to_node=section_node,
                    confidence=r.get('score', 0.0),
                    relation="è¯­ä¹‰å¬å›ž",
                    metadata={
                        "method": "embedding",
                        "search_type": r.get('search_type', ''),
                        "step": "step2"
                    }
                )
                query_section_clue_count += 1

            self.logger.info(
                f"ðŸ”— [Step2 çº¿ç´¢æž„å»º] Queryâ†’Section={query_section_clue_count}æ¡")

        # 3. åˆå¹¶ SQL ç»“æžœå’Œè¿‡æ»¤åŽçš„ embedding ç»“æžœ
        merged_list = step1_results + filtered_embedding_results

        # 4. æŒ‰å¾—åˆ†é™åºæŽ’åº
        merged_list.sort(key=lambda x: x['score'], reverse=True)

        self.logger.info(
            f"æ­¥éª¤3å®Œæˆ: step1={len(step1_results)}ä¸ª, "
            f"step2={len(step2_results)}ä¸ª(åŽ»é‡åŽä¿ç•™{len(filtered_embedding_results)}ä¸ª), "
            f"åˆå¹¶åŽ={len(merged_list)}ä¸ª"
        )

        return merged_list

    async def _step4_calculate_weight_of_contents(
        self,
        key_final: List[Dict[str, Any]],
        content_related: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤4: è®¡ç®—[content-related]çš„åˆå§‹æƒé‡å‘é‡

        æ ¹æ®key_finalä¸­çš„å…³é”®å®žä½“ä¿¡æ¯ï¼Œè®¡ç®—æ¯ä¸ªæ®µè½çš„æƒé‡ï¼Œ
        å¹¶å°†è®¡ç®—å‡ºçš„æƒé‡èµ‹å€¼ç»™æ¯ä¸ªæ®µè½çš„ weight å­—æ®µ

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„key_finalæ•°æ®
            content_related: ä»Žstep3åˆå¹¶åŽçš„æ®µè½åˆ—è¡¨

        Returns:
            æ›´æ–°äº†weightå­—æ®µçš„æ®µè½åˆ—è¡¨
        """
        try:
            self.logger.info(f"æ­¥éª¤4å¼€å§‹: è®¡ç®— {len(content_related)} ä¸ªæ®µè½çš„æƒé‡")

            # ç¬¬ä¸€å±‚å¾ªçŽ¯ï¼šéåŽ†æ‰€æœ‰æ®µè½
            for content in content_related:
                search_type = content.get("search_type")  # ä½¿ç”¨search_type
                content_text = content.get("content", "")
                heading = content.get("heading", "")
                full_text = f"{heading} {content_text}"  # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ç”¨äºŽæœç´¢key

                # 1. èŽ·å–æ®µè½ä¸Žqueryçš„ç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆæ¥è‡ªstep1æˆ–step2çš„scoreï¼‰
                similarity_score = content.get("score", 0.0)

                # 2. åˆå§‹åŒ–keyæƒé‡ç´¯åŠ å’Œ
                key_weight_sum = 0.0

                # ç¬¬äºŒå±‚å¾ªçŽ¯ï¼šéåŽ†æ‰€æœ‰å…³é”®å®žä½“
                for key in key_final:
                    key_name = key.get("name", "")
                    key_weight = key.get("weight", 0.0)
                    key_steps = key.get("steps", [1])  # ä¾‹å¦‚ [1] æˆ– [2]

                    # è®¡ç®—stepå€¼ï¼ˆå–ç¬¬ä¸€ä¸ªstepå€¼ï¼‰
                    step = key_steps[0] if key_steps else 1

                    # ç»Ÿè®¡keyåœ¨æ®µè½ä¸­å‡ºçŽ°çš„æ¬¡æ•°
                    count = full_text.count(key_name)

                    if count > 0:
                        # è®¡ç®—è¯¥keyçš„è´¡çŒ®ï¼škey_weight * ln(1 + count) / step
                        key_contribution = key_weight * \
                            math.log(1 + count) / step
                        key_weight_sum += key_contribution

                        self.logger.debug(
                            f"æ®µè½ {search_type} åŒ…å«key '{key_name}': "
                            f"count={count}, weight={key_weight:.3f}, step={step}, "
                            f"contribution={key_contribution:.4f}"
                        )

                # 3. è®¡ç®—æœ€ç»ˆæƒé‡ = 0.5 * similarity_score + ln(1 + key_weight_sum)
                total_weight = 0.5 * similarity_score + \
                    math.log(1 + key_weight_sum)

                # 4. å°†æƒé‡èµ‹å€¼ç»™è¯¥æ®µè½çš„ weight å­—æ®µ
                content["weight"] = total_weight

                self.logger.info(
                    f"æ®µè½ {search_type} æƒé‡è®¡ç®—: "
                    f"similarity={similarity_score:.4f}, key_sum={key_weight_sum:.4f}, "
                    f"total={total_weight:.4f}"
                )

            self.logger.info(f"æ­¥éª¤4å®Œæˆ: è®¡ç®—äº† {len(content_related)} ä¸ªæ®µè½çš„æƒé‡")

            return content_related

        except Exception as e:
            self.logger.error(f"æ­¥éª¤4æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step5_pageRank_of_contents(
        self,
        content_related: List[Dict[str, Any]],
        key_final: List[Dict[str, Any]] = None,
        damping: float = 0.85,
        iterations: int = 100,
        tolerance: float = 1e-6
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤5: PageRanké‡æŽ’åº

        æž„å»ºæ®µè½å…³ç³»å›¾å¹¶ä½¿ç”¨PageRankç®—æ³•è¿›è¡ŒæŽ’åº
        - åˆå§‹æƒé‡ï¼šä½¿ç”¨step4çš„weightä½œä¸ºåˆå§‹PageRankå€¼
        - ä¸‰ç§å…³è”å…³ç³»ï¼š
          1. äº‹ä»¶å…³è”ï¼ˆæƒé‡0.6ï¼‰ï¼šå…±äº«ç›¸åŒevent_idçš„æ®µè½ä¹‹é—´æœ‰è¾¹
          2. æ®µè½å…³è”ï¼ˆæƒé‡0.2ï¼‰ï¼šåŒä¸€æ–‡ç« å†…ç›¸é‚»çš„æ®µè½ä¹‹é—´æœ‰è¾¹
          3. å®žä½“å…³è”ï¼ˆæƒé‡0.2ï¼‰ï¼šåŒ…å«ç›¸åŒkey_finalå®žä½“çš„æ®µè½ä¹‹é—´æœ‰è¾¹

        Args:
            content_related: ä»Žstep4è®¡ç®—å®Œæƒé‡çš„æ®µè½åˆ—è¡¨
            key_final: ä»ŽRecallè¿”å›žçš„å…³é”®å®žä½“åˆ—è¡¨ï¼ˆç”¨äºŽå®žä½“å…³è”ï¼‰
            damping: PageRanké˜»å°¼ç³»æ•°ï¼Œé»˜è®¤0.85
            iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤30
            tolerance: æ”¶æ•›é˜ˆå€¼ï¼Œé»˜è®¤1e-6

        Returns:
            æŒ‰PageRankå€¼æŽ’åºåŽçš„æ®µè½åˆ—è¡¨
        """
        try:
            n = len(content_related)
            self.logger.info(f"æ­¥éª¤5å¼€å§‹: å¯¹ {n} ä¸ªæ®µè½ä½¿ç”¨PageRankç®—æ³•è¿›è¡ŒæŽ’åº")

            if n == 0:
                return []

            # ===== DEBUG: è®°å½•ç›´æŽ¥æŒ‰æƒé‡æŽ’åºçš„ç»“æžœ =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å¯¹æ¯”ã€‘ç›´æŽ¥æŒ‰æƒé‡æŽ’åºçš„ç»“æžœï¼ˆTop 10ï¼‰ï¼š")
            self.logger.debug("-" * 80)

            weight_sorted = sorted(
                enumerate(content_related),
                key=lambda x: x[1].get('weight', 0.0),
                reverse=True
            )

            for rank, (idx, content) in enumerate(weight_sorted[:10], 1):
                search_type = content.get('search_type', 'N/A')
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]
                section_id = content.get('section_id', '')[:8]
                event_count = len(content.get('event_ids', []))

                self.logger.debug(
                    f"Rank {rank:2d} [idx={idx:3d}]: {search_type:12s} | "
                    f"weight={weight:.4f}, score={score:.4f} | "
                    f"events={event_count} | section={section_id}... | "
                    f"{heading}"
                )
            self.logger.debug("=" * 80)

            # 1. åˆå§‹åŒ–PageRankå€¼ï¼ˆä½¿ç”¨step4çš„weightå½’ä¸€åŒ–åŽä½œä¸ºåˆå§‹å€¼ï¼‰
            weights = np.array([c.get('weight', 0.0) for c in content_related])
            if weights.sum() > 0:
                pagerank = weights / weights.sum()
                self.logger.info(f"ä½¿ç”¨step4çš„æƒé‡ä½œä¸ºåˆå§‹PageRankå€¼ï¼ˆå·²å½’ä¸€åŒ–ï¼‰")
            else:
                pagerank = np.ones(n) / n
                self.logger.warning(f"æ‰€æœ‰æƒé‡ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåˆå§‹PageRankå€¼")

            # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºç´¢å¼•æ˜ å°„
            section_to_idx = {c['section_id']: i for i,
                              c in enumerate(content_related)}

            # 2. æž„å»ºå…³ç³»å›¾ï¼ˆä½¿ç”¨å­—å…¸å­˜å‚¨è¾¹å’Œæƒé‡ï¼‰
            # graph[i] = [(j, weight), ...] è¡¨ç¤ºä»ŽèŠ‚ç‚¹iæŒ‡å‘èŠ‚ç‚¹jçš„è¾¹åŠå…¶æƒé‡
            graph = defaultdict(list)

            # 2.1 äº‹ä»¶å…³è”ï¼ˆæƒé‡0.6ï¼‰
            self.logger.info("æž„å»ºäº‹ä»¶å…³è”è¾¹...")
            event_to_sections = defaultdict(list)
            for i, content in enumerate(content_related):
                for event_id in content.get('event_ids', []):
                    event_to_sections[event_id].append(i)

            event_edges_count = 0
            for event_id, sections in event_to_sections.items():
                if len(sections) > 1:
                    # å…±äº«ç›¸åŒäº‹ä»¶çš„æ®µè½ä¹‹é—´äº’ç›¸å…³è”
                    for i in sections:
                        for j in sections:
                            if i != j:
                                graph[i].append((j, 0.6))
                                event_edges_count += 1

            self.logger.info(f"äº‹ä»¶å…³è”: æ·»åŠ äº† {event_edges_count} æ¡è¾¹")

            # 2.2 æ®µè½å…³è”ï¼ˆæƒé‡0.2ï¼‰- åŒä¸€æ–‡ç« å†…ç›¸é‚»æ®µè½
            self.logger.info("æž„å»ºæ®µè½å…³è”è¾¹ï¼ˆç›¸é‚»æ®µè½ï¼‰...")
            article_sections = defaultdict(list)
            for i, content in enumerate(content_related):
                article_id = content['article_id']
                rank = content['rank']
                article_sections[article_id].append((i, rank))

            paragraph_edges_count = 0
            for article_id, sections in article_sections.items():
                # æŒ‰rankæŽ’åº
                sections.sort(key=lambda x: x[1])
                # ç›¸é‚»æ®µè½ä¹‹é—´å»ºç«‹åŒå‘è¾¹
                for k in range(len(sections) - 1):
                    i, rank_i = sections[k]
                    j, rank_j = sections[k + 1]
                    if rank_j - rank_i == 1:  # ä¸¥æ ¼ç›¸é‚»
                        graph[i].append((j, 0.2))
                        graph[j].append((i, 0.2))
                        paragraph_edges_count += 2

            self.logger.info(f"æ®µè½å…³è”: æ·»åŠ äº† {paragraph_edges_count} æ¡è¾¹")

            # 2.3 å®žä½“å…³è”ï¼ˆæƒé‡0.2ï¼‰- åŒ…å«ç›¸åŒkey_finalå®žä½“
            entity_edges_count = 0
            if key_final:
                self.logger.info("æž„å»ºå®žä½“å…³è”è¾¹...")
                # ä¸ºæ¯ä¸ªæ®µè½æ‰¾åˆ°å®ƒåŒ…å«çš„å®žä½“
                section_entities = defaultdict(set)
                for i, content in enumerate(content_related):
                    full_text = f"{content.get('heading', '')} {content.get('content', '')}"
                    for key in key_final:
                        key_name = key.get('name', '')
                        if key_name and key_name in full_text:
                            section_entities[i].add(key_name)

                # å…±äº«å®žä½“çš„æ®µè½ä¹‹é—´å»ºç«‹åŒå‘è¾¹
                for i in range(n):
                    for j in range(i + 1, n):
                        common_entities = section_entities[i] & section_entities[j]
                        if common_entities:
                            # å¯ä»¥æ ¹æ®å…±äº«å®žä½“çš„æ•°é‡è°ƒæ•´æƒé‡ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå›ºå®š0.2
                            graph[i].append((j, 0.2))
                            graph[j].append((i, 0.2))
                            entity_edges_count += 2

                self.logger.info(f"å®žä½“å…³è”: æ·»åŠ äº† {entity_edges_count} æ¡è¾¹")
            else:
                self.logger.warning("æœªæä¾›key_finalï¼Œè·³è¿‡å®žä½“å…³è”è¾¹çš„æž„å»º")

            total_edges = event_edges_count + paragraph_edges_count + entity_edges_count
            self.logger.info(f"å…³ç³»å›¾æž„å»ºå®Œæˆ: èŠ‚ç‚¹æ•°={n}, æ€»è¾¹æ•°={total_edges}")

            # ===== DEBUG: å±•ç¤ºå›¾ç»“æž„ç»Ÿè®¡ä¿¡æ¯ =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å›¾ç»“æž„ç»Ÿè®¡ã€‘ï¼š")

            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å‡ºåº¦å’Œå…¥åº¦
            out_degrees = {i: len(graph.get(i, [])) for i in range(n)}
            in_degrees = defaultdict(int)
            for i in range(n):
                for j, _ in graph.get(i, []):
                    in_degrees[j] += 1

            # ç»Ÿè®¡å‡ºåº¦åˆ†å¸ƒ
            out_degree_values = list(out_degrees.values())
            in_degree_values = [in_degrees.get(i, 0) for i in range(n)]

            self.logger.debug(
                f"å‡ºåº¦ç»Ÿè®¡: å¹³å‡={np.mean(out_degree_values):.2f}, "
                f"æœ€å¤§={max(out_degree_values)}, "
                f"æœ€å°={min(out_degree_values)}, "
                f"å­¤ç«‹èŠ‚ç‚¹={sum(1 for d in out_degree_values if d == 0)}"
            )
            self.logger.debug(
                f"å…¥åº¦ç»Ÿè®¡: å¹³å‡={np.mean(in_degree_values):.2f}, "
                f"æœ€å¤§={max(in_degree_values)}, "
                f"æœ€å°={min(in_degree_values)}"
            )

            # å±•ç¤ºåº¦æ•°æœ€é«˜çš„å‰5ä¸ªèŠ‚ç‚¹
            top_out_degree = sorted(
                out_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.debug("å‡ºåº¦æœ€é«˜çš„5ä¸ªèŠ‚ç‚¹ï¼š")
            for idx, degree in top_out_degree:
                content = content_related[idx]
                section_id = content.get('section_id', '')[:8]
                heading = content.get('heading', '')[:30]
                self.logger.debug(
                    f"  èŠ‚ç‚¹{idx:3d} (section={section_id}...): å‡ºåº¦={degree}, æ ‡é¢˜={heading}")

            self.logger.debug("=" * 80)

            # 3. PageRankè¿­ä»£è®¡ç®—
            self.logger.info(
                f"å¼€å§‹PageRankè¿­ä»£ï¼ˆé˜»å°¼ç³»æ•°={damping}, æœ€å¤§è¿­ä»£={iterations}ï¼‰...")

            for iteration in range(iterations):
                new_pagerank = np.zeros(n)

                for i in range(n):
                    # è®¡ç®—æŒ‡å‘èŠ‚ç‚¹içš„æ‰€æœ‰èŠ‚ç‚¹çš„è´¡çŒ®
                    incoming_score = 0.0

                    for j in range(n):
                        # æ£€æŸ¥æ˜¯å¦æœ‰ä»Žjåˆ°içš„è¾¹
                        edges_from_j = graph.get(j, [])
                        if not edges_from_j:
                            continue

                        for target, edge_weight in edges_from_j:
                            if target == i:
                                # è®¡ç®—jå¯¹içš„è´¡çŒ®
                                # è´¡çŒ® = PR(j) * edge_weight / sum(æ‰€æœ‰ä»Žjå‡ºå‘çš„edge_weight)
                                total_out_weight = sum(
                                    w for _, w in edges_from_j)
                                if total_out_weight > 0:
                                    incoming_score += pagerank[j] * \
                                        edge_weight / total_out_weight

                    # PageRankå…¬å¼: PR(i) = (1-d)/n + d * incoming_score
                    new_pagerank[i] = (1 - damping) / n + \
                        damping * incoming_score

                # æ£€æŸ¥æ”¶æ•›
                diff = np.abs(new_pagerank - pagerank).sum()
                if diff < tolerance:
                    self.logger.info(
                        f"PageRankåœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£åŽæ”¶æ•›ï¼ˆå·®å¼‚={diff:.8f}ï¼‰")
                    pagerank = new_pagerank
                    break

                pagerank = new_pagerank

                # æ¯10æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if (iteration + 1) % 10 == 0:
                    self.logger.debug(
                        f"è¿­ä»£ {iteration+1}/{iterations}, å·®å¼‚={diff:.8f}")

            else:
                self.logger.warning(f"PageRankè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{iterations}ï¼Œæœªå®Œå…¨æ”¶æ•›")

            # 4. å°†PageRankå€¼èµ‹å€¼ç»™æ¯ä¸ªæ®µè½
            for i, content in enumerate(content_related):
                content['pagerank'] = float(pagerank[i])

            # 5. æŒ‰PageRankå€¼æŽ’åºï¼ˆä»Žå¤§åˆ°å°ï¼‰
            sorted_contents = sorted(
                content_related,
                key=lambda x: x.get('pagerank', 0.0),
                reverse=True
            )

            # ===== DEBUG: è®°å½•PageRankæŽ’åºç»“æžœå¹¶å¯¹æ¯”æƒé‡æŽ’åº =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å¯¹æ¯”ã€‘PageRankæŽ’åºçš„ç»“æžœï¼ˆTop 10ï¼‰ï¼š")
            self.logger.debug("-" * 80)

            # åˆ›å»º section_id åˆ°åŽŸå§‹ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºŽå¯¹æ¯”æŽ’åå˜åŒ–ï¼‰
            section_to_original_idx = {
                c['section_id']: i for i, c in enumerate(content_related)}

            # åˆ›å»ºæƒé‡æŽ’åæ˜ å°„
            weight_rank_map = {
                content[1]['section_id']: rank for rank, content in enumerate(weight_sorted, 1)}

            for rank, content in enumerate(sorted_contents[:10], 1):
                search_type = content.get('search_type', 'N/A')
                pagerank_val = content.get('pagerank', 0.0)
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]
                section_id = content.get('section_id', '')
                event_count = len(content.get('event_ids', []))

                # èŽ·å–åœ¨æƒé‡æŽ’åºä¸­çš„æŽ’å
                weight_rank = weight_rank_map.get(section_id, -1)
                rank_change = weight_rank - rank if weight_rank > 0 else 0

                # æŽ’åå˜åŒ–æ ‡è®°
                if rank_change > 0:
                    change_mark = f"â†‘{rank_change:+d}"  # ä¸Šå‡
                elif rank_change < 0:
                    change_mark = f"â†“{rank_change:+d}"  # ä¸‹é™
                else:
                    change_mark = " â”  "  # ä¸å˜

                original_idx = section_to_original_idx.get(section_id, -1)

                self.logger.debug(
                    f"Rank {rank:2d} [idx={original_idx:3d}] {change_mark:>5s} (was #{weight_rank:2d}): {search_type:12s} | "
                    f"PR={pagerank_val:.6f}, weight={weight:.4f}, score={score:.4f} | "
                    f"events={event_count} | section={section_id[:8]}... | "
                    f"{heading}"
                )

            self.logger.debug("=" * 80)
            self.logger.debug("ã€æŽ’åºå˜åŒ–ç»Ÿè®¡ã€‘ï¼š")

            # ç»Ÿè®¡æŽ’åå˜åŒ–
            rank_changes = []
            for rank, content in enumerate(sorted_contents, 1):
                section_id = content['section_id']
                weight_rank = weight_rank_map.get(section_id, -1)
                if weight_rank > 0:
                    change = weight_rank - rank
                    rank_changes.append(abs(change))

            if rank_changes:
                avg_change = np.mean(rank_changes)
                max_change = max(rank_changes)
                unchanged_count = sum(1 for c in rank_changes if c == 0)
                self.logger.debug(
                    f"å¹³å‡æŽ’åå˜åŒ–: {avg_change:.2f} ä½ | "
                    f"æœ€å¤§æŽ’åå˜åŒ–: {max_change} ä½ | "
                    f"æŽ’åä¸å˜: {unchanged_count}/{len(rank_changes)} ä¸ª"
                )

            self.logger.debug("=" * 80)

            # è®°å½•æŽ’åºåŽçš„å‰å‡ ä¸ªç»“æžœï¼ˆINFOçº§åˆ«ï¼‰
            self.logger.info("=" * 80)
            self.logger.info("æ­¥éª¤5æŽ’åºç»“æžœï¼ˆTop 5 by PageRankï¼‰ï¼š")
            self.logger.info("-" * 80)

            for i, content in enumerate(sorted_contents[:5], 1):
                search_type = content.get('search_type', 'N/A')
                pagerank_val = content.get('pagerank', 0.0)
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]

                self.logger.info(
                    f"Rank {i}: {search_type} | "
                    f"PageRank={pagerank_val:.6f}, weight={weight:.4f}, score={score:.4f} | "
                    f"æ ‡é¢˜: {heading}"
                )

            self.logger.info("=" * 80)
            self.logger.info(
                f"æ­¥éª¤5å®Œæˆ: æŽ’åºäº† {len(sorted_contents)} ä¸ªæ®µè½ "
                f"(å¹³å‡PageRank={pagerank.mean():.6f})"
            )

            return sorted_contents

        except Exception as e:
            self.logger.error(f"æ­¥éª¤5æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step6_get_topn_of_contents(
        self,
        sorted_contents: List[Dict[str, Any]],
        top_k: int,
        source_config_ids: List[str],
        query_vector: List[float],
        config: Optional[SearchConfig] = None
    ) -> Tuple[List[SourceEvent], Dict[str, List[Dict]]]:
        """
        æ­¥éª¤6: å–Top-Næ®µè½å¹¶è¿”å›žå…³è”çš„äº‹é¡¹åˆ—è¡¨

        å¤„ç†æµç¨‹ï¼š
        1. å–Top-kï¼šä»ŽæŽ’åºåŽçš„ç»“æžœä¸­å–å‰ k ä¸ªæ®µè½
        2. æ”¶é›†æ®µè½IDï¼šæ”¶é›†æ‰€æœ‰æ®µè½çš„ section_id
        3. åå‘æŸ¥è¯¢äº‹é¡¹ï¼šé€šè¿‡ SourceEvent.references å­—æ®µæŸ¥æ‰¾åŒ…å«è¿™äº› section_id çš„äº‹é¡¹
        4. èŽ·å–äº‹é¡¹å‘é‡ï¼šä»Ž ES ä¸­æ‰¹é‡èŽ·å–äº‹é¡¹çš„å‘é‡æ•°æ®
        5. ç›¸ä¼¼åº¦è®¡ç®—ï¼šè®¡ç®— query ä¸Žæ¯ä¸ªäº‹é¡¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
        6. é˜ˆå€¼è¿‡æ»¤ï¼šä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦äº‹é¡¹
        7. è¿”å›žäº‹é¡¹åˆ—è¡¨ï¼šä¿æŒæ®µè½ PageRank é¡ºåºï¼ˆäº‹é¡¹é¦–æ¬¡å‡ºçŽ°çš„ä½ç½®ï¼‰

        æ³¨æ„ï¼šä¸å†ä¾èµ– step1 çš„ event_idsï¼Œç»Ÿä¸€é€šè¿‡ references å­—æ®µåå‘æŸ¥æ‰¾

        Args:
            sorted_contents: ä»Žstep5æŽ’åºåŽçš„æ®µè½åˆ—è¡¨ï¼ˆå·²æŒ‰PageRanké™åºæŽ’åºï¼‰
            top_k: å–å‰kä¸ªç»“æžœ
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            query_vector: æŸ¥è¯¢å‘é‡
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽé˜ˆå€¼è¿‡æ»¤ï¼‰

        Returns:
            Tuple[List[SourceEvent], Dict[str, List[Dict]]]:
                - events: äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆä¿æŒæ®µè½ PageRank é¡ºåºï¼Œå·²è¿‡æ»¤å’ŒåŽ»é‡ï¼‰
                - event_to_clues: äº‹é¡¹IDåˆ°å®žä½“åˆ—è¡¨çš„æ˜ å°„ {event_id: [entity1, entity2, ...]}
        """
        try:
            self.logger.info(
                f"æ­¥éª¤6å¼€å§‹: ä»Ž {len(sorted_contents)} ä¸ªæ®µè½ä¸­å–Top-{top_k}")

            # 1. å–Top-kç»“æžœ
            topk_contents = sorted_contents[:top_k]
            self.logger.info(f"æå–äº†Top-{top_k}æ®µè½ï¼ˆå®žé™…: {len(topk_contents)} ä¸ªï¼‰")

            # 2. æ”¶é›†æ‰€æœ‰æ®µè½çš„ section_id
            section_ids = [content.get('section_id')
                           for content in topk_contents]
            self.logger.info(f"æ”¶é›†åˆ° {len(section_ids)} ä¸ªæ®µè½ID")

            # 3. é€šè¿‡ SourceEvent.references åå‘æŸ¥è¯¢å…³è”çš„äº‹é¡¹
            # æŸ¥è¯¢æ‰€æœ‰ references å­—æ®µåŒ…å«è¿™äº› section_id çš„äº‹é¡¹
            if not section_ids:
                self.logger.warning("Top-kæ®µè½ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ section_id")
                return [], {}  # è¿”å›žç©ºåˆ—è¡¨å’Œç©ºå­—å…¸

            async with self.session_factory() as session:
                # æŸ¥è¯¢æ‰€æœ‰äº‹é¡¹ï¼Œç„¶åŽåœ¨ Python ä¸­è¿‡æ»¤
                # å› ä¸º references æ˜¯ JSON å­—æ®µï¼ˆå­˜å‚¨åˆ—è¡¨ï¼‰ï¼Œæ— æ³•ç›´æŽ¥ä½¿ç”¨æ•°ç»„æ“ä½œç¬¦
                event_query = (
                    select(SourceEvent)
                    # é¢„åŠ è½½å…³è”å…³ç³»
                    .options(selectinload(SourceEvent.event_associations))
                    .where(
                        and_(
                            SourceEvent.source_id.in_(source_config_ids),
                            # æŽ’é™¤ references ä¸º NULL çš„äº‹é¡¹
                            SourceEvent.references.isnot(None)
                        )
                    )
                )

                event_result = await session.execute(event_query)
                all_events = event_result.scalars().all()

                # DEBUG: æ£€æŸ¥äº‹é¡¹çš„ summary å­—æ®µ
                empty_summary_count = sum(
                    1 for e in all_events if not e.summary or not e.summary.strip())
                if empty_summary_count > 0:
                    self.logger.warning(
                        f"å‘çŽ° {empty_summary_count}/{len(all_events)} ä¸ªäº‹é¡¹çš„ summary å­—æ®µä¸ºç©º"
                    )

                # åœ¨ Python ä¸­è¿‡æ»¤ï¼šæ‰¾å‡º references ä¸Ž section_ids æœ‰äº¤é›†çš„äº‹é¡¹
                events = []
                for event in all_events:
                    if event.references and isinstance(event.references, list):
                        # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
                        if any(section_id in event.references for section_id in section_ids):
                            events.append(event)

                self.logger.info(
                    f"ä»Žæ•°æ®åº“æŸ¥è¯¢åˆ° {len(all_events)} ä¸ªäº‹é¡¹ï¼Œ"
                    f"è¿‡æ»¤åŽå¾—åˆ° {len(events)} ä¸ªå…³è”çš„äº‹é¡¹"
                )

            if not events:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä¸Žæ®µè½å…³è”çš„äº‹é¡¹")
                return []

            # 4. ä»Ž ES æ‰¹é‡èŽ·å–äº‹é¡¹çš„å‘é‡æ•°æ®
            event_ids = [event.id for event in events]
            event_vectors_data = await self.event_repo.get_events_by_ids(event_ids)

            # æž„å»º event_id -> content_vector çš„æ˜ å°„
            event_vector_map = {}
            for event_data in event_vectors_data:
                event_id = event_data.get('event_id')
                content_vector = event_data.get('content_vector')
                if event_id and content_vector:
                    event_vector_map[event_id] = content_vector

            self.logger.info(
                f"ä»ŽESèŽ·å–åˆ° {len(event_vector_map)} ä¸ªäº‹é¡¹çš„å‘é‡æ•°æ®"
            )

            # 5. è®¡ç®— query ä¸Žæ¯ä¸ªäº‹é¡¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
            event_similarity_map = {}  # event_id -> similarity_score

            for event in events:
                event_vector = event_vector_map.get(event.id)

                if event_vector:
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = await self._cosine_similarity(query_vector, event_vector)
                    event_similarity_map[event.id] = similarity
                    self.logger.debug(
                        f"äº‹é¡¹ {event.id[:8]}... | ç›¸ä¼¼åº¦={similarity:.4f} | title='{event.title[:40]}'"
                    )
                else:
                    self.logger.warning(
                        f"äº‹é¡¹ {event.id[:8]}... åœ¨ESä¸­æœªæ‰¾åˆ°å‘é‡æ•°æ®ï¼Œè·³è¿‡"
                    )

            self.logger.info(f"è®¡ç®—äº† {len(event_similarity_map)} ä¸ªäº‹é¡¹çš„ç›¸ä¼¼åº¦")

            # ðŸ†• æ—¥å¿—1ï¼šè®°å½•æ¯ä¸ªæ®µè½æ‰¾åˆ°äº†å“ªäº›äº‹é¡¹ï¼ˆç›¸ä¼¼åº¦è¿‡æ»¤å‰ï¼‰
            self.logger.info("=" * 80)
            self.logger.info("ã€Step6 æ—¥å¿—1ã€‘æ¯ä¸ªæ®µè½æ‰¾åˆ°çš„äº‹é¡¹ï¼ˆç›¸ä¼¼åº¦è¿‡æ»¤å‰ï¼‰ï¼š")
            self.logger.info("-" * 80)

            section_to_events_before = {}  # section_id -> [event_info]
            for event in events:
                if event.references:
                    for section_id in event.references:
                        if section_id in section_ids:
                            if section_id not in section_to_events_before:
                                section_to_events_before[section_id] = []
                            section_to_events_before[section_id].append({
                                'id': event.id,
                                'title': event.title,
                                'similarity': event_similarity_map.get(event.id, 0.0)
                            })

            for idx, content in enumerate(topk_contents[:10], 1):
                section_id = content.get('section_id')
                heading = content.get('heading', '')[:40]
                found_events = section_to_events_before.get(section_id, [])

                if found_events:
                    event_preview = ', '.join([
                        f"{e['id'][:8]}...(sim={e['similarity']:.2f})"
                        for e in found_events[:3]
                    ])
                    if len(found_events) > 3:
                        event_preview += f' ... (å…±{len(found_events)}ä¸ª)'
                    self.logger.info(
                        f"  æ®µè½{idx} {section_id[:8]}... ('{heading}') "
                        f"â†’ {len(found_events)} ä¸ªäº‹é¡¹: {event_preview}"
                    )
                else:
                    self.logger.info(
                        f"  æ®µè½{idx} {section_id[:8]}... ('{heading}') â†’ 0 ä¸ªäº‹é¡¹"
                    )

            if len(topk_contents) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(topk_contents) - 10} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("-" * 80)
            self.logger.info(
                f"  ç»Ÿè®¡: {len(topk_contents)} ä¸ªæ®µè½ â†’ {len(events)} ä¸ªäº‹é¡¹")
            self.logger.info("=" * 80)

            # 6. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦äº‹é¡¹
            original_count = len(event_similarity_map)
            if config and config.rerank.score_threshold:
                # åªä¿ç•™ç›¸ä¼¼åº¦ >= threshold çš„äº‹é¡¹
                filtered_event_ids = {
                    event_id for event_id, score in event_similarity_map.items()
                    if score >= config.rerank.score_threshold
                }

                if len(filtered_event_ids) < original_count:
                    self.logger.info(
                        f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_event_ids)} ä¸ªäº‹é¡¹ "
                        f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                    )
            else:
                self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")
                filtered_event_ids = set(event_similarity_map.keys())

            # ðŸ†• æ—¥å¿—2ï¼šè®°å½•ç»è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤åŽï¼Œæ¯ä¸ªæ®µè½è¿˜æœ‰å“ªäº›äº‹é¡¹
            self.logger.info("=" * 80)
            self.logger.info("ã€Step6 æ—¥å¿—2ã€‘ç»è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤åŽï¼Œæ¯ä¸ªæ®µè½ä¿ç•™çš„äº‹é¡¹ï¼š")
            self.logger.info("-" * 80)

            section_to_events_after = {}  # section_id -> [event_info]
            for event in events:
                if event.id in filtered_event_ids and event.references:
                    for section_id in event.references:
                        if section_id in section_ids:
                            if section_id not in section_to_events_after:
                                section_to_events_after[section_id] = []
                            section_to_events_after[section_id].append({
                                'id': event.id,
                                'title': event.title,
                                'similarity': event_similarity_map.get(event.id, 0.0)
                            })

            for idx, content in enumerate(topk_contents[:10], 1):
                section_id = content.get('section_id')
                heading = content.get('heading', '')[:40]
                before_count = len(
                    section_to_events_before.get(section_id, []))
                after_events = section_to_events_after.get(section_id, [])
                after_count = len(after_events)

                if after_events:
                    event_preview = ', '.join([
                        f"{e['id'][:8]}...(sim={e['similarity']:.2f})"
                        for e in after_events[:3]
                    ])
                    if len(after_events) > 3:
                        event_preview += f' ... (å…±{after_count}ä¸ª)'
                    self.logger.info(
                        f"  æ®µè½{idx} {section_id[:8]}... ('{heading}') "
                        f"â†’ è¿‡æ»¤: {before_count} â†’ {after_count} ä¸ªäº‹é¡¹: {event_preview}"
                    )
                else:
                    self.logger.info(
                        f"  æ®µè½{idx} {section_id[:8]}... ('{heading}') "
                        f"â†’ è¿‡æ»¤: {before_count} â†’ 0 ä¸ªäº‹é¡¹ (å…¨éƒ¨è¢«è¿‡æ»¤)"
                    )

            if len(topk_contents) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(topk_contents) - 10} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("-" * 80)
            self.logger.info(
                f"  ç»Ÿè®¡: {len(topk_contents)} ä¸ªæ®µè½ â†’ "
                f"{len(events)} ä¸ªäº‹é¡¹ (è¿‡æ»¤å‰) â†’ {len(filtered_event_ids)} ä¸ªäº‹é¡¹ (è¿‡æ»¤åŽ)"
            )
            self.logger.info("=" * 80)

            # 7. æŒ‰æ®µè½ PageRank é¡ºåºè¿”å›žäº‹é¡¹ï¼ˆä¿æŒäº‹é¡¹é¦–æ¬¡å‡ºçŽ°çš„ä½ç½®ï¼‰
            # ä¸ºæ¯ä¸ªäº‹é¡¹è®¡ç®—å®ƒç¬¬ä¸€æ¬¡è¢«å¼•ç”¨çš„ä½ç½®ï¼ˆæ®µè½ç´¢å¼•ï¼‰
            # åŒæ—¶æ”¶é›†æ¯ä¸ªäº‹é¡¹çš„ clues
            event_first_appearance = {}  # event_id -> ç¬¬ä¸€æ¬¡å‡ºçŽ°çš„æ®µè½ç´¢å¼•
            event_to_clues = {}  # event_id -> clues åˆ—è¡¨

            for idx, content in enumerate(topk_contents):
                section_id = content.get('section_id')
                content_clues = content.get('clues', [])  # èŽ·å–æ®µè½çš„ clues

                # éåŽ†æ‰€æœ‰äº‹é¡¹ï¼Œæ£€æŸ¥å“ªäº›äº‹é¡¹å¼•ç”¨äº†è¿™ä¸ªæ®µè½
                for event in events:
                    # åªå¤„ç†é€šè¿‡ç›¸ä¼¼åº¦è¿‡æ»¤çš„äº‹é¡¹
                    if event.id in filtered_event_ids:
                        if event.references and section_id in event.references:
                            # å¦‚æžœè¿™ä¸ªäº‹é¡¹è¿˜æ²¡æœ‰è®°å½•ï¼Œè®°å½•å®ƒç¬¬ä¸€æ¬¡å‡ºçŽ°çš„ä½ç½®
                            if event.id not in event_first_appearance:
                                event_first_appearance[event.id] = idx
                                self.logger.debug(
                                    f"äº‹é¡¹ {event.id[:8]}... åœ¨ç¬¬ {idx} ä¸ªæ®µè½ä¸­é¦–æ¬¡å‡ºçŽ°"
                                )

                            # æ±‡æ€»è¯¥æ®µè½çš„ clues åˆ°äº‹é¡¹
                            if event.id not in event_to_clues:
                                event_to_clues[event.id] = []

                            # å°†æ®µè½çš„ clues æ·»åŠ åˆ°äº‹é¡¹ï¼ˆåŽ»é‡ï¼‰
                            for clue in content_clues:
                                # ç®€å•åŽ»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„ clue
                                clue_key = (clue.get('type'), clue.get('name'))
                                existing_keys = [(c.get('type'), c.get('name'))
                                                 for c in event_to_clues[event.id]]
                                if clue_key not in existing_keys:
                                    event_to_clues[event.id].append(clue)

            # æŒ‰ç…§ç¬¬ä¸€æ¬¡å‡ºçŽ°çš„ä½ç½®æŽ’åºäº‹é¡¹ï¼ˆä¸é™„åŠ  cluesï¼‰
            result_events = []
            for event in sorted(events, key=lambda e: event_first_appearance.get(e.id, float('inf'))):
                # åªä¿ç•™é€šè¿‡ç›¸ä¼¼åº¦è¿‡æ»¤ä¸”æœ‰å‡ºçŽ°ä½ç½®çš„äº‹é¡¹
                if event.id in event_first_appearance:
                    result_events.append(event)

            # è¾“å‡º event_to_clues æ˜ å°„è¡¨ç»Ÿè®¡ä¿¡æ¯
            clues_stats = [len(clues) for clues in event_to_clues.values()]
            if clues_stats:
                self.logger.info(
                    f"âœ… event_to_cluesç»Ÿè®¡: å¹³å‡={sum(clues_stats)/len(clues_stats):.1f}ä¸ªå®žä½“/äº‹é¡¹, "
                    f"æœ€å¤š={max(clues_stats)}ä¸ª, æœ€å°‘={min(clues_stats)}ä¸ª"
                )

            self.logger.info(
                f"æ­¥éª¤6å®Œæˆ: è¿”å›ž {len(result_events)} ä¸ªäº‹é¡¹ï¼ˆä¿æŒæ®µè½ PageRank é¡ºåºï¼Œå·²è¿‡æ»¤ï¼‰"
            )

            # æ³¨æ„ï¼šæ®µè½â†’äº‹é¡¹çš„çº¿ç´¢æž„å»ºå·²ç§»è‡³æ—¥å¿—3ä¹‹åŽï¼ˆé¿å…é‡å¤æž„å»ºï¼‰

            # ðŸ†• æ—¥å¿—3ï¼šè®°å½•ç»è¿‡top-kåŽï¼Œæœ€ç»ˆè¿”å›žçš„æ®µè½å’Œäº‹é¡¹æ˜ å°„
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€Step6 æ—¥å¿—3ã€‘ç»è¿‡top-kåŽ (max_results={config.rerank.max_results})ï¼Œæœ€ç»ˆæ®µè½å’Œäº‹é¡¹æ˜ å°„ï¼š")
            self.logger.info("-" * 80)

            # æœ€ç»ˆè¿”å›žçš„äº‹é¡¹IDé›†åˆï¼ˆç»è¿‡max_resultsé™åˆ¶ï¼‰
            final_result_event_ids = {
                e.id for e in result_events[:config.rerank.max_results]}

            # æž„å»ºæœ€ç»ˆçš„æ®µè½åˆ°äº‹é¡¹æ˜ å°„
            section_to_events_final = {}  # section_id -> [event_info]
            for event in result_events[:config.rerank.max_results]:
                if event.references:
                    for section_id in event.references:
                        if section_id in section_ids:
                            if section_id not in section_to_events_final:
                                section_to_events_final[section_id] = []
                            section_to_events_final[section_id].append({
                                'id': event.id,
                                'title': event.title,
                                'similarity': event_similarity_map.get(event.id, 0.0),
                                'first_pos': event_first_appearance.get(event.id, -1)
                            })

            # æ˜¾ç¤ºæ¯ä¸ªæ®µè½æœ€ç»ˆå…³è”çš„äº‹é¡¹
            displayed_sections = 0
            for idx, content in enumerate(topk_contents, 1):
                section_id = content.get('section_id')
                heading = content.get('heading', '')[:40]
                pagerank = content.get('pagerank', 0.0)

                final_events = section_to_events_final.get(section_id, [])

                if final_events or displayed_sections < 15:  # æ˜¾ç¤ºæ‰€æœ‰æœ‰äº‹é¡¹çš„æ®µè½ï¼Œæˆ–å‰15ä¸ª
                    # æŒ‰first_posæŽ’åº
                    final_events.sort(key=lambda e: e['first_pos'])

                    if final_events:
                        # æ˜¾ç¤ºæ®µè½åŸºæœ¬ä¿¡æ¯
                        self.logger.info(
                            f"  æ®µè½{idx} (PR={pagerank:.4f}) {section_id[:8]}... ('{heading}') "
                            f"â†’ {len(final_events)} ä¸ªæœ€ç»ˆäº‹é¡¹:"
                        )

                        # æ˜¾ç¤ºæ¯ä¸ªäº‹é¡¹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç¼©è¿›æ˜¾ç¤ºï¼‰
                        for e in final_events[:5]:  # æœ€å¤šæ˜¾ç¤ºå‰5ä¸ª
                            event_title_preview = e['title'][:50] if e['title'] else "æ— æ ‡é¢˜"
                            self.logger.info(
                                f"     â”œâ”€ {e['id'][:8]}... | pos={e['first_pos']}, sim={e['similarity']:.3f} | '{event_title_preview}'"
                            )

                        if len(final_events) > 5:
                            self.logger.info(
                                f"     â””â”€ ... (è¿˜æœ‰ {len(final_events) - 5} ä¸ªäº‹é¡¹)")
                    else:
                        self.logger.info(
                            f"  æ®µè½{idx} (PR={pagerank:.4f}) {section_id[:8]}... ('{heading}') "
                            f"â†’ 0 ä¸ªæœ€ç»ˆäº‹é¡¹"
                        )
                    displayed_sections += 1

            if displayed_sections < len(topk_contents):
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(topk_contents) - displayed_sections} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("-" * 80)
            self.logger.info(
                f"  ç»Ÿè®¡: Top-{len(topk_contents)} æ®µè½ä¸­æœ‰ {len(section_to_events_final)} ä¸ªæ®µè½ "
                f"å…³è”äº† {len(final_result_event_ids)} ä¸ªæœ€ç»ˆäº‹é¡¹"
            )
            self.logger.info("=" * 80)

            # ðŸ†• æž„å»ºæ®µè½â†’äº‹é¡¹çš„ä¿¡æ¯æº¯æºçº¿ç´¢ï¼ˆåŸºäºŽæ—¥å¿—3çš„æœ€ç»ˆæ˜ å°„ï¼‰
            # ðŸ”§ ä¿®å¤ï¼šåˆ›å»ºç»Ÿä¸€çš„ Tracker å®žä¾‹ï¼Œä¾›åŽç»­æ‰€æœ‰çº¿ç´¢æž„å»ºå…±ç”¨ï¼Œé¿å…é‡å¤åˆ›å»ºäº‹é¡¹èŠ‚ç‚¹
            from sag.modules.search.tracker import Tracker
            tracker = Tracker(config)  # ç»Ÿä¸€çš„ tracker å®žä¾‹

            if config and section_to_events_final:
                self.logger.info("")
                self.logger.info("ðŸ”— ã€Step6 çº¿ç´¢æž„å»ºã€‘æž„å»ºæ®µè½â†’äº‹é¡¹çš„ä¿¡æ¯æº¯æº...")

                section_event_clue_count = 0
                clue_details = []  # ç”¨äºŽè®°å½•è¯¦ç»†çš„çº¿ç´¢ä¿¡æ¯

                # éåŽ†æ‰€æœ‰æœ‰æœ€ç»ˆäº‹é¡¹çš„æ®µè½
                for section_id, events_list in section_to_events_final.items():
                    # æ‰¾åˆ°å¯¹åº”çš„æ®µè½å†…å®¹
                    section_content = None
                    for content in topk_contents:
                        if content.get('section_id') == section_id:
                            section_content = content
                            break

                    if not section_content:
                        continue

                    # æž„å»ºæ®µè½èŠ‚ç‚¹æ•°æ®
                    section_data = {
                        "section_id": section_id,
                        "id": section_id,
                        "heading": section_content.get('heading', ''),
                        "content": section_content.get('content', ''),
                        "summary": "",
                        "section_type": section_content.get('search_type', ''),
                        "score": section_content.get('score', 0.0),
                        "pagerank": section_content.get('pagerank', 0.0)
                    }

                    section_node = Tracker.build_section_node(section_data)

                    # ä¸ºè¿™ä¸ªæ®µè½çš„æ¯ä¸ªäº‹é¡¹æž„å»ºçº¿ç´¢
                    for event_info in events_list:
                        event_id = event_info['id']

                        # æ‰¾åˆ°å¯¹åº”çš„äº‹é¡¹å¯¹è±¡
                        event_obj = next(
                            (e for e in result_events[:config.rerank.max_results] if e.id == event_id), None)
                        if not event_obj:
                            continue

                        # ðŸ†• ä½¿ç”¨ tracker å®žä¾‹æ–¹æ³•ï¼ŒæŒ‡å®šå¬å›žæ–¹å¼ä¸º "section"
                        event_node = tracker.get_or_create_event_node(
                            event_obj, "rerank", recall_method="section")

                        # æ·»åŠ çº¿ç´¢ï¼ˆå‰ç«¯æ ¹æ® stage="rerank" + relation="æ®µè½å¬å›ž" æ¸²æŸ“ä¸ºç´«è‰²ï¼‰
                        tracker.add_clue(
                            stage="rerank",
                            from_node=section_node,
                            to_node=event_node,
                            # ä½¿ç”¨äº‹é¡¹çš„ç›¸ä¼¼åº¦ä½œä¸ºç½®ä¿¡åº¦
                            confidence=event_info['similarity'],
                            relation="æ®µè½å¬å›ž",
                            metadata={
                                "method": "section_to_event",
                                "event_similarity": event_info['similarity'],
                                "event_first_pos": event_info['first_pos'],
                                "section_pagerank": section_data['pagerank'],
                                "section_score": section_data['score'],
                                "step": "step6"
                            }
                        )
                        section_event_clue_count += 1

                        # è®°å½•çº¿ç´¢è¯¦æƒ…ï¼ˆç”¨äºŽæ—¥å¿—ï¼‰
                        clue_details.append({
                            'section_id': section_id[:8],
                            'section_heading': section_data['heading'][:30],
                            'event_id': event_id[:8],
                            'event_title': event_info['title'][:30],
                            'similarity': event_info['similarity'],
                            'pagerank': section_data['pagerank']
                        })

                self.logger.info(f"âœ… æˆåŠŸæž„å»º {section_event_clue_count} æ¡æ®µè½â†’äº‹é¡¹çº¿ç´¢")

                # æ˜¾ç¤ºéƒ¨åˆ†çº¿ç´¢è¯¦æƒ…
                if clue_details:
                    self.logger.info("")
                    self.logger.info(f"ðŸ“‹ çº¿ç´¢è¯¦æƒ…ï¼ˆå‰10æ¡ï¼‰ï¼š")
                    for idx, clue in enumerate(clue_details[:10], 1):
                        self.logger.info(
                            f"  {idx}. {clue['section_id']}... ('{clue['section_heading']}', PR={clue['pagerank']:.3f}) "
                            f"â†’ {clue['event_id']}... ('{clue['event_title']}', sim={clue['similarity']:.3f})"
                        )
                    if len(clue_details) > 10:
                        self.logger.info(
                            f"  ... (è¿˜æœ‰ {len(clue_details) - 10} æ¡çº¿ç´¢)")

                self.logger.info("")
                self.logger.info("=" * 80)

            # === ðŸ†• ç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level="final") ===
            # ä¸ºæœ€ç»ˆè¿”å›žçš„äº‹é¡¹ç”Ÿæˆ final çº¿ç´¢ï¼Œå‰ç«¯å¯æ®æ­¤åæŽ¨å®Œæ•´æŽ¨ç†è·¯å¾„
            # ðŸ”§ ä¿®å¤ï¼šå¤ç”¨ä¸Šé¢åˆ›å»ºçš„ tracker å®žä¾‹ï¼Œé¿å…é‡å¤åˆ›å»ºäº‹é¡¹èŠ‚ç‚¹
            if config and result_events[:config.rerank.max_results]:
                self.logger.info("")
                self.logger.info(
                    "ðŸŽ¯ [Rerank Final] ç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level=final)")

                # ðŸ”§ ä¸å†åˆ›å»ºæ–°çš„ Trackerï¼Œç›´æŽ¥ä½¿ç”¨ä¸Šé¢çš„ tracker å®žä¾‹
                final_events = result_events[:config.rerank.max_results]
                final_clue_count = 0

                # ä¸ºæ¯ä¸ªæœ€ç»ˆäº‹é¡¹ç”Ÿæˆ section â†’ event çš„ final çº¿ç´¢
                for event in final_events:
                    # æ‰¾åˆ°è¯¥äº‹é¡¹é¦–æ¬¡å‡ºçŽ°çš„æ®µè½ä½ç½®
                    first_pos = event_first_appearance.get(event.id)
                    if first_pos is not None and first_pos < len(topk_contents):
                        section_content = topk_contents[first_pos]
                        section_id = section_content.get('section_id')

                        # æž„å»ºæ®µè½èŠ‚ç‚¹
                        section_data = {
                            "section_id": section_id,
                            "id": section_id,
                            "heading": section_content.get('heading', ''),
                            "content": section_content.get('content', ''),
                            "summary": "",
                            "section_type": section_content.get('search_type', ''),
                            "score": section_content.get('score', 0.0),
                            "pagerank": section_content.get('pagerank', 0.0)
                        }

                        section_node = Tracker.build_section_node(section_data)
                        # ðŸ†• ä½¿ç”¨ tracker å®žä¾‹æ–¹æ³•ï¼ŒæŒ‡å®šå¬å›žæ–¹å¼ä¸º "section"ï¼ˆfinal çº¿ç´¢ä¹Ÿä½¿ç”¨ section å¬å›žï¼‰
                        event_node = tracker.get_or_create_event_node(
                            event, "rerank", recall_method="section")

                        # ç”Ÿæˆ section â†’ event final çº¿ç´¢
                        tracker.add_clue(
                            stage="rerank",
                            from_node=section_node,
                            to_node=event_node,
                            confidence=event_similarity_map.get(event.id, 0.0),
                            relation="æœ€ç»ˆäº‹é¡¹",
                            display_level="final",  # ðŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æžœ
                            metadata={
                                "method": "final_result",
                                "step": "step6",
                                "similarity": event_similarity_map.get(event.id, 0.0),
                                "first_position": first_pos,
                                "pagerank": section_data['pagerank'],
                                "section_score": section_data['score']
                            }
                        )
                        final_clue_count += 1

                        self.logger.debug(
                            f"  Final: {section_id[:8]}... ('{section_data['heading'][:30]}', PR={section_data['pagerank']:.3f}) "
                            f"â†’ {event.id[:8]}... ('{event.title[:30]}', sim={event_similarity_map.get(event.id, 0.0):.3f})"
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [Rerank Final] äº‹é¡¹ {event.id[:8]}... æ‰¾ä¸åˆ°é¦–æ¬¡å‡ºçŽ°çš„æ®µè½ä½ç½®"
                        )

                self.logger.info(
                    f"âœ… [Rerank Final] ç”Ÿæˆäº† {final_clue_count} æ¡æœ€ç»ˆçº¿ç´¢ (sectionâ†’event)"
                )
                self.logger.info(
                    f"âœ… [Rerank Final] å‰ç«¯å¯æ ¹æ®è¿™äº› final çº¿ç´¢åæŽ¨å®Œæ•´æŽ¨ç†è·¯å¾„ (queryâ†’entityâ†’sectionâ†’event)"
                )
                self.logger.info("")

            return result_events[:config.rerank.max_results], event_to_clues

        except Exception as e:
            self.logger.error(f"æ­¥éª¤6æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return [], {}  # å¤±è´¥æ—¶è¿”å›žç©ºåˆ—è¡¨å’Œç©ºå­—å…¸

    def _build_response(
        self,
        config: SearchConfig,
        key_final: List[Dict[str, Any]],
        events: List[SourceEvent],
        event_to_clues: Dict[str, List[Dict]]
    ) -> Dict[str, Any]:
        """
        æž„å»ºæ–°çš„å“åº”æ ¼å¼

        Args:
            config: æœç´¢é…ç½®å¯¹è±¡
            key_final: å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey-finalï¼‰
            events: äº‹é¡¹åˆ—è¡¨
            event_to_clues: äº‹é¡¹IDåˆ°å®žä½“åˆ—è¡¨çš„æ˜ å°„ {event_id: [entity1, entity2, ...]}

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events: äº‹é¡¹å¯¹è±¡åˆ—è¡¨
                - clues: å¬å›žçº¿ç´¢ä¿¡æ¯
                    - origin_query: åŽŸå§‹æŸ¥è¯¢
                    - final_query: LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆå¦‚æžœæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                    - query_entities: æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities: å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼ŒåŽ»é™¤query_entitiesä¸­çš„å€¼ï¼‰
                    - event_entities: äº‹é¡¹ä¸Žå®žä½“çš„å…³è”æ˜ å°„è¡¨ {event_id: [entity1, entity2, ...]}
        """
        # 1. å¤„ç† query_entitiesï¼šå°† config.query_recalled_keys ä¸­çš„ key_id æ”¹ä¸º id
        query_entities = []
        query_key_ids = set()  # ç”¨äºŽåŽç»­è¿‡æ»¤

        for key in config.query_recalled_keys:
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_id = key_copy.pop("key_id")
                key_copy["id"] = key_id
                query_key_ids.add(key_id)
            query_entities.append(key_copy)

        # 2. å¤„ç† recall_entitiesï¼šå°† key_final ä¸­çš„ key_id æ”¹ä¸º idï¼Œå¹¶è¿‡æ»¤æŽ‰ query_entities ä¸­çš„å€¼
        recall_entities = []

        for key in key_final:
            # èŽ·å– key_id ç”¨äºŽè¿‡æ»¤åˆ¤æ–­
            key_id = key.get("key_id")

            # å¦‚æžœè¿™ä¸ª key_id åœ¨ query_recalled_keys ä¸­ï¼Œåˆ™è·³è¿‡
            if key_id in query_key_ids:
                continue

            # å¤åˆ¶å¹¶é‡å‘½å key_id ä¸º id
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_copy["id"] = key_copy.pop("key_id")
            recall_entities.append(key_copy)

        # 3. åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿”å›ž final_query
        # å¦‚æžœå¯ç”¨äº†queryé‡å†™åŠŸèƒ½ï¼ˆenable_query_rewrite=Trueï¼‰ï¼Œåˆ™è¿”å›žé‡å†™åŽçš„query
        # å¦åˆ™è¿”å›ž None
        final_query = config.query if config.enable_query_rewrite and config.recall.use_fast_mode == False else None

        # 4. è¿‡æ»¤ event_to_cluesï¼Œåªä¿ç•™æœ€ç»ˆè¿”å›žçš„äº‹é¡¹
        final_event_ids = {event.id for event in events}
        filtered_event_entities = {
            event_id: clues
            for event_id, clues in event_to_clues.items()
            if event_id in final_event_ids
        }

        # 5. æž„å»ºå“åº”
        response = {
            "events": events,  # äº‹é¡¹åˆ—è¡¨
            "clues": {
                "origin_query": config.original_query,  # åŽŸå§‹queryï¼ˆé‡å†™å‰ï¼‰
                "final_query": final_query,  # é‡å†™åŽçš„queryï¼ˆæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                "query_entities": query_entities,
                "recall_entities": recall_entities,
                "event_entities": filtered_event_entities  # åªåŒ…å«æœ€ç»ˆè¿”å›žäº‹é¡¹çš„æº¯æºä¿¡æ¯
            }
        }

        self.logger.info(
            f"å“åº”æž„å»ºå®Œæˆ: origin_query='{config.original_query}', "
            f"final_query='{final_query}', "
            f"query_entities={len(query_entities)}ä¸ª, "
            f"recall_entities={len(recall_entities)}ä¸ª, "
            f"events={len(events)}ä¸ª, "
            f"event_entitiesæ˜ å°„={len(filtered_event_entities)}ä¸ª (è¿‡æ»¤å‰={len(event_to_clues)}ä¸ª)"
        )

        return response

    def _build_rerank_clues(
        self,
        config: SearchConfig,
        key_final: List[Dict[str, Any]],
        events: List[SourceEvent],
        event_to_clues: Dict[str, List[Dict]],
        sorted_contents: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æž„å»ºReranké˜¶æ®µçš„çº¿ç´¢ï¼ˆentity â†’ section â†’ eventï¼‰

        ðŸ†• ä¿®æ”¹ï¼šä¸å†ä½¿ç”¨å•æ¡entityâ†’eventçº¿ç´¢ï¼Œæ”¹ä¸ºæ‹†åˆ†æˆä¸¤æ¡ï¼š
        1. entity â†’ section
        2. section â†’ event

        è¿™æ ·ç¡®ä¿ä¸­é—´èŠ‚ç‚¹ï¼ˆsectionï¼‰ä¸ä¼šè¢«çœç•¥ï¼Œå‰ç«¯å¯ä»¥æž„å»ºå®Œæ•´çŸ¥è¯†å›¾è°±

        Args:
            config: æœç´¢é…ç½®
            key_final: æœ€ç»ˆçš„keyåˆ—è¡¨
            events: æœ€ç»ˆè¿”å›žçš„äº‹é¡¹åˆ—è¡¨
            event_to_clues: äº‹é¡¹IDåˆ°å®žä½“åˆ—è¡¨çš„æ˜ å°„
            sorted_contents: æŽ’åºåŽçš„æ®µè½åˆ—è¡¨

        Returns:
            Reranké˜¶æ®µçš„çº¿ç´¢åˆ—è¡¨ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼Œå®žé™…çº¿ç´¢å·²è¿½åŠ åˆ°config.all_cluesï¼‰
        """
        # ðŸ”• å·²æ³¨é‡Šï¼šæš‚æ—¶ç¦ç”¨ Rerank é˜¶æ®µçš„çº¿ç´¢æž„å»º
        # ============================================================
        # # ðŸ†• åˆ›å»ºçº¿ç´¢æž„å»ºå™¨
        # tracker = Tracker(config)
        #
        # # åˆ›å»ºkey_idåˆ°keyå¯¹è±¡çš„æ˜ å°„ï¼Œæ–¹ä¾¿æŸ¥æ‰¾æƒé‡ç­‰ä¿¡æ¯
        # key_map = {key["key_id"]: key for key in key_final}
        #
        # # åˆ›å»ºevent_idé›†åˆï¼Œåªå¤„ç†æœ€ç»ˆè¿”å›žçš„äº‹é¡¹
        # final_event_ids = {event.id for event in events}
        #
        # # ç»Ÿè®¡ä¿¡æ¯
        # query_section_clues = 0  # ðŸ†• query â†’ section çº¿ç´¢è®¡æ•°
        # entity_section_clues = 0
        # section_event_clues = 0
        #
        # # åªå¤„ç†Top-Nçš„sectionsï¼ˆä¸Žæœ€ç»ˆè¿”å›žçš„eventsç›¸å…³ï¼‰
        # top_n = config.rerank.max_results * 2  # ä¸Žstep6ä¿æŒä¸€è‡´
        # for content in sorted_contents[:top_n]:
        #     section_id = content.get("section_id")
        #     section_heading = content.get("heading", "")
        #     section_weight = content.get("weight", 0.0)
        #     section_score = content.get("score", 0.0)  # ðŸ†• èŽ·å–æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        #
        #     # æž„å»ºsectionå­—å…¸ï¼ˆç”¨äºŽæ ‡å‡†èŠ‚ç‚¹æž„å»ºï¼‰
        #     section_dict = {
        #         "section_id": section_id,
        #         "id": section_id,
        #         "heading": section_heading,
        #         "content": content.get("content", ""),
        #         "summary": "",  # PageRankæ²¡æœ‰summaryå­—æ®µ
        #         "section_type": content.get("search_type", "")
        #     }
        #
        #     # ä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹æž„å»ºå™¨
        #     section_node = Tracker.build_section_node(section_dict)
        #
        #     # 1. æž„å»º query/entity â†’ section çº¿ç´¢
        #     # content.clues åŒ…å«äº†å¯¼è‡´æ‰¾åˆ°è¿™ä¸ªsectionçš„æ‰€æœ‰queryæˆ–entities
        #     content_clues = content.get("clues", [])
        #     for clue in content_clues:
        #         clue_type = clue.get("type")
        #
        #         # ðŸ†• å¤„ç† query ç±»åž‹çš„ clueï¼ˆStep2 ç›´æŽ¥å¬å›žçš„æ®µè½ï¼‰
        #         if clue_type == "query":
        #             # ä½¿ç”¨ Tracker.build_query_node ç”Ÿæˆæ ‡å‡† query èŠ‚ç‚¹ï¼ˆå«ç¡®å®šæ€§ IDï¼‰
        #             query_node = Tracker.build_query_node(config)
        #
        #             # æ·»åŠ  query â†’ section çº¿ç´¢
        #             tracker.add_clue(
        #                 stage="rerank",
        #                 from_node=query_node,
        #                 to_node=section_node,
        #                 confidence=section_score,  # ä½¿ç”¨æ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦
        #                 relation="è¯­ä¹‰å¬å›ž",
        #                 metadata={
        #                     "method": "embedding"
        #                 }
        #             )
        #             query_section_clues += 1
        #             continue
        #
        #         # å¤„ç† entity ç±»åž‹çš„ clueï¼ˆåŽŸæœ‰é€»è¾‘ï¼‰
        #         # å…¼å®¹ä¸åŒçš„IDå­—æ®µå
        #         entity_id = clue.get("id") or clue.get("key_id")
        #         if not entity_id or entity_id not in key_map:
        #             continue
        #
        #         entity_info = key_map[entity_id]
        #
        #         # æž„å»ºå®Œæ•´çš„å®žä½“å­—å…¸ï¼ˆç”¨äºŽæ ‡å‡†èŠ‚ç‚¹æž„å»ºï¼‰
        #         entity_dict = {
        #             "key_id": entity_id,
        #             "id": entity_id,
        #             "name": entity_info.get("name", ""),
        #             "type": entity_info.get("type", "unknown"),
        #             "description": entity_info.get("description", "")
        #         }
        #
        #         # ä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹æž„å»ºå™¨
        #         entity_node = Tracker.build_entity_node(entity_dict)
        #
        #         # æ·»åŠ  entity â†’ section çº¿ç´¢ï¼ˆä½¿ç”¨æ–°çš„ç»Ÿä¸€æŽ¥å£ï¼‰
        #         tracker.add_clue(
        #             stage="rerank",
        #             from_node=entity_node,
        #             to_node=section_node,
        #             confidence=section_score,  # ðŸ†• ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºç½®ä¿¡åº¦
        #             relation="å†…å®¹é‡æŽ’",
        #             metadata={
        #                 "method": "pagerank",
        #                 "entity_weight": entity_info.get("weight", 0.0),
        #                 "section_weight": section_weight,  # ðŸ†• ä¿ç•™åŽŸå§‹ PageRank æƒé‡
        #                 "step": "step1"
        #             }
        #         )
        #         entity_section_clues += 1
        #
        #     # 2. æž„å»º section â†’ event çº¿ç´¢
        #     # content.event_ids åŒ…å«äº†å¼•ç”¨è¿™ä¸ªsectionçš„æ‰€æœ‰events
        #     content_event_ids = content.get("event_ids", [])
        #     for event_id in content_event_ids:
        #         # åªå¤„ç†æœ€ç»ˆè¿”å›žçš„events
        #         if event_id not in final_event_ids:
        #             continue
        #
        #         # æ‰¾åˆ°å¯¹åº”çš„eventå¯¹è±¡
        #         event_obj = next((e for e in events if e.id == event_id), None)
        #         if not event_obj:
        #             continue
        #
        #         # ä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹æž„å»ºå™¨
        #         event_node = Tracker.build_event_node(event_obj)
        #
        #         # èŽ·å–eventçš„PageRankåˆ†æ•°ä½œä¸ºç½®ä¿¡åº¦
        #         confidence = getattr(event_obj, 'pagerank_score', None)
        #         if confidence is None or confidence == 0.0:
        #             confidence = getattr(event_obj, 'similarity_score', 0.0)
        #
        #         # æ·»åŠ  section â†’ event çº¿ç´¢ï¼ˆä½¿ç”¨æ–°çš„ç»Ÿä¸€æŽ¥å£ï¼‰
        #         tracker.add_clue(
        #             stage="rerank",
        #             from_node=section_node,
        #             to_node=event_node,
        #             confidence=confidence,
        #             relation="å†…å®¹é‡æŽ’",
        #             metadata={
        #                 "method": "pagerank",
        #                 "pagerank_score": getattr(event_obj, 'pagerank_score', None),
        #                 "similarity_score": getattr(event_obj, 'similarity_score', None)
        #             }
        #         )
        #         section_event_clues += 1
        #
        # self.logger.info(
        #     f"ðŸ” [Rerankè¯Šæ–­] çº¿ç´¢ç»Ÿè®¡: "
        #     f"queryâ†’section={query_section_clues}æ¡, "
        #     f"entityâ†’section={entity_section_clues}æ¡, "
        #     f"sectionâ†’event={section_event_clues}æ¡"
        # )
        # ============================================================

        self.logger.info("ðŸ”• [Rerank] çº¿ç´¢æž„å»ºå·²ç¦ç”¨")

        # è¿”å›žç©ºåˆ—è¡¨ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼Œå®žé™…çº¿ç´¢å·²é€šè¿‡trackerè¿½åŠ åˆ°config.all_cluesï¼‰
        return []
