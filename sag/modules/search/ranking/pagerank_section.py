"""
æœç´¢ Rerank æ¨¡å—

å®žçŽ°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦åŽŸæ–‡å—çš„æ–¹æ³•ï¼š
1. keyæ‰¾contentï¼šæ ¹æ®[key-final]ä»Žsqlä¸­æå–åŽŸæ–‡å—[content-key-related]ï¼Œä»ŽESèŽ·å–é¢„å­˜å‘é‡å¹¶è®¡ç®—å’Œqueryçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè®°å½•event_idï¼‰
2. queryæ‰¾contentï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼ˆKNN+ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°åŽŸæ–‡å—[content-query-related]ï¼ˆevent_idsä¸ºç©ºï¼‰
3. contentåˆå¹¶+åŽ»é‡ï¼šåˆå¹¶[content-key-related]å’Œ[content-query-related]ï¼Œå¦‚æžœåŒä¸€åŽŸæ–‡å—ï¼ˆsource_id+chunk_idç›¸åŒï¼‰åŒæ—¶å‡ºçŽ°åœ¨SQLå’ŒEmbeddingç»“æžœä¸­ï¼Œåªä¿ç•™SQLçš„ç»“æžœ
4. åˆ¶ä½œ[content-related]æƒé‡å‘é‡ï¼šä½¿ç”¨å…¬å¼ weight = 0.5*ç›¸ä¼¼åº¦ + ln(1 + Î£(keyæƒé‡ Ã— ln(1+å‡ºçŽ°æ¬¡æ•°) / step))
5. PageRanké‡æŽ’åºï¼šæ ¹æ®æƒé‡å¯¹åŽŸæ–‡å—æŽ’åºï¼ˆä»Žå¤§åˆ°å°ï¼‰
6. å–Top-Nå¹¶è¿”å›žï¼šä»ŽTop-NåŽŸæ–‡å—ä¸­æå–å…³è”çš„äº‹é¡¹åˆ—è¡¨

è¿”å›žæ ¼å¼ï¼š
Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
    - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰åŽŸæ–‡å— PageRank é¡ºåºæŽ’åˆ—ï¼Œå·²åŽ»é‡ï¼‰
    - clues (Dict): å¬å›žçº¿ç´¢ä¿¡æ¯
        - origin_query (str): åŽŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
        - final_query (str): LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆé‡å†™åŽï¼‰
        - query_entities (List[Dict]): æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
        - recall_entities (List[Dict]): å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æŽ‰query_entitiesä¸­çš„å€¼ï¼‰

æ³¨æ„ï¼šstep1 å’Œ step2 éƒ½ä½¿ç”¨ ES é¢„å­˜çš„ content_vectorï¼ˆç”Ÿæˆæ—¶ä½¿ç”¨"æ ‡é¢˜ + å†…å®¹[:500]"ï¼‰ï¼Œç¡®ä¿å‘é‡ä¸€è‡´æ€§

"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math
import re
import time
import asyncio
from collections import Counter, defaultdict


from sqlalchemy import select, and_, or_
from sqlalchemy.orm import selectinload

from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import SourceEvent, Entity, EventEntity, ArticleSection, Article, SourceChunk, get_session_factory
from sag.exceptions import AIError
from sag.modules.load.processor import DocumentProcessor
from sag.modules.search.config import SearchConfig
from sag.modules.search.tracker import Tracker  # ðŸ†• æ·»åŠ çº¿ç´¢è¿½è¸ªå™¨
from sag.utils import get_logger

logger = get_logger("search.rerank.pagerank")


@dataclass
class ContentSearchResult:
    """
    æœç´¢ç»“æžœçš„ç»Ÿä¸€è¿”å›žæ ¼å¼ï¼ˆSourceChunkæž¶æž„ï¼‰

    ç”¨äºŽè¡¨ç¤ºä»ŽSQLæ•°æ®åº“æˆ–Embeddingå‘é‡æ•°æ®åº“æœç´¢åˆ°çš„å†…å®¹
    """
    # å¿…éœ€å­—æ®µ
    search_type: str      # "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
    source_config_id: str # æ•°æ®æºé…ç½®ID (UUID)
    source_id: str        # æ–‡ç« ID (Article.id æˆ– SourceChunk.source_id)
    chunk_id: str         # åŽŸæ–‡å—ID (SourceChunk.id)
    rank: int             # åŽŸæ–‡å—åœ¨æ–‡ç« ä¸­çš„æŽ’åº
    heading: str          # åŽŸæ–‡å—æ ‡é¢˜
    content: str          # åŽŸæ–‡å—å†…å®¹
    score: float = 0.0    # ç›¸å…³æ€§å¾—åˆ†
    weight: float = 0.0   # æƒé‡å€¼ï¼ˆstep4è®¡ç®—åŽèµ‹å€¼ï¼‰
    event_ids: List[str] = None  # å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
    event: str = ""  # èšåˆåŽçš„äº‹é¡¹æ‘˜è¦ï¼ˆå¤šä¸ªsummaryåˆå¹¶ï¼‰
    clues: List[Dict[str, Any]] = None  # å¬å›žè¯¥æ®µè½çš„çº¿ç´¢åˆ—è¡¨ï¼ˆæ¥è‡ª key_final æˆ– queryï¼‰

    def __post_init__(self):
        """åˆå§‹åŒ–åŽéªŒè¯"""
        # åˆå§‹åŒ– event_ids ä¸ºç©ºåˆ—è¡¨
        if self.event_ids is None:
            self.event_ids = []

        # åˆå§‹åŒ– clues ä¸ºç©ºåˆ—è¡¨
        if self.clues is None:
            self.clues = []

        # å…è®¸ "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
        valid_types = ["sql", "embedding"]
        is_valid = (
            self.search_type in valid_types or
            self.search_type.startswith("SQL-") or
            self.search_type.startswith("embedding-")
        )

        if not is_valid:
            raise ValueError(
                f"search_type å¿…é¡»æ˜¯ 'sql', 'embedding' æˆ–å¸¦ç¼–å·æ ¼å¼(å¦‚ 'SQL-1', 'embedding-1')ï¼Œ"
                f"å½“å‰å€¼: {self.search_type}"
            )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "search_type": self.search_type,
            "source_config_id": self.source_config_id,
            "source_id": self.source_id,
            "chunk_id": self.chunk_id,
            "rank": self.rank,
            "heading": self.heading,
            "content": self.content,
            "score": self.score,
            "weight": self.weight,
            "event_ids": self.event_ids,
            "event": self.event,
            "clues": self.clues,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ContentSearchResult":
        """ä»Žå­—å…¸åˆ›å»ºå®žä¾‹"""
        return cls(
            search_type=data.get("search_type", "sql"),
            source_config_id=data["source_config_id"],
            source_id=data["source_id"],
            chunk_id=data["chunk_id"],
            rank=data.get("rank", 0),
            heading=data.get("heading", ""),
            content=data.get("content", ""),
            score=data.get("score", 0.0),
            weight=data.get("weight", 0.0),
            event_ids=data.get("event_ids", []),
            event=data.get("event", ""),
            clues=data.get("clues", []),
        )

    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"ContentSearchResult(type={self.search_type}, "
            f"chunk_id={self.chunk_id}, "
            f"heading='{self.heading[:30]}...', "
            f"score={self.score:.3f})"
        )


class RerankPageRankSearcher:
    """Rerankæ®µè½æœç´¢å™¨ - å®žçŽ°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦æ®µè½çš„æ–¹æ³•"""

    def __init__(
        self,
        llm_client=None
    ):
        """
        åˆå§‹åŒ–Rerankæ®µè½æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """

        self.session_factory = get_session_factory()
        self.logger = get_logger("search.rerank.pagerank")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.content_repo = SourceChunkRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)  # æ·»åŠ äº‹é¡¹å‘é‡ä»“åº“

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºŽç”Ÿæˆembedingå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "Rerankæ®µè½æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    async def search(
        self,
        key_final: List[Dict[str, Any]],
        config: SearchConfig
    ) -> Dict[str, Any]:
        """
        Rerank æœç´¢ä¸»æ–¹æ³•

        æ•´åˆæ­¥éª¤1-6ï¼Œç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼Œé¿å…é‡å¤è®¡ç®—

        æ­¥éª¤æµç¨‹ï¼š
          1. keyæ‰¾content (SQL) - è®°å½•event_id
          2. queryæ‰¾content (KNN + ä½™å¼¦ç›¸ä¼¼åº¦) - event_idsä¸ºç©º
          3. åˆå¹¶ç»“æžœ+åŽ»é‡ï¼ˆä¼˜å…ˆä¿ç•™SQLç»“æžœï¼‰
          4. è®¡ç®—æƒé‡å‘é‡
          5. PageRankæŽ’åº
          6. å–Top-Næ®µè½å¹¶æå–å…³è”çš„äº‹é¡¹åˆ—è¡¨

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„å…³é”®å®žä½“åˆ—è¡¨
            config: Rerankæœç´¢é…ç½®

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰æ®µè½ PageRank é¡ºåºæŽ’åˆ—ï¼Œå·²åŽ»é‡ï¼‰
                - clues (Dict): å¬å›žçº¿ç´¢ä¿¡æ¯
                    - origin_query (str): åŽŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
                    - final_query (str): LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆé‡å†™åŽï¼‰
                    - query_entities (List[Dict]): æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities (List[Dict]): å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æŽ‰query_entitiesä¸­çš„å€¼ï¼‰
        """
        try:
            # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
            overall_start = time.perf_counter()

            self.logger.info(
                f"Rerankæœç´¢å¼€å§‹: query='{config.query}', source_config_ids={config.get_source_config_ids()}")

            # ç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼ˆé¿å…åœ¨step1å’Œstep2ä¸­é‡å¤è®¡ç®—ï¼‰
            vector_start = time.perf_counter()
            query_vector = await self._generate_query_vector(config.query, config)
            vector_time = time.perf_counter() - vector_start
            if config.has_query_embedding:
                self.logger.info(
                    f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")
            else:
                self.logger.info(
                    f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")

            # ç”¨äºŽè®°å½•å„æ­¥éª¤è€—æ—¶
            step_times = {}

            self.logger.info("=" * 80)
            self.logger.info("ã€Rerank æœç´¢ã€‘è€—æ—¶ç»Ÿè®¡")
            self.logger.info("=" * 80)

            # æ®µè½æ¨¡å¼ï¼šæ‰§è¡Œå®Œæ•´çš„step1-step6æµç¨‹
            # æ­¥éª¤1å’Œ2å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆäº’ä¸ä¾èµ–ï¼‰
            self.logger.info("æ­¥éª¤1å’Œ2å¹¶è¡Œå¼€å§‹...")
            parallel_start = time.perf_counter()

            # å¹¶è¡Œæ‰§è¡Œ step1 å’Œ step2
            step1_task = self._step1_keys_to_contents(
                key_final=key_final,
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                query_vector=query_vector,
                config=config  # ä¼ å…¥configç”¨äºŽç¼“å­˜
            )

            step2_task = self._step2_query_to_contents(
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                k=config.rerank.max_query_recall_results,  # ðŸ†• ä½¿ç”¨ max_query_recall_results
                query_vector=query_vector,
                config=config  # ä¼ å…¥configç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤
            )

            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆ
            step1_results, step2_results = await asyncio.gather(step1_task, step2_task)

            parallel_time = time.perf_counter() - parallel_start
            step_times['step1_2å¹¶è¡Œæ‰§è¡Œ'] = parallel_time

            self.logger.info(
                f"æ­¥éª¤1å’Œ2å¹¶è¡Œå®Œæˆ: "
                f"step1æ‰¾åˆ° {len(step1_results)} ä¸ªæ®µè½, "
                f"step2æ‰¾åˆ° {len(step2_results)} ä¸ªæ®µè½, "
                f"æ€»è€—æ—¶: {parallel_time:.3f}ç§’"
            )

            # æ­¥éª¤3: åˆå¹¶ç»“æžœï¼ˆä¸åŽ»é‡ï¼Œç›´æŽ¥åˆå¹¶ï¼‰
            step3_start = time.perf_counter()
            merged_results = await self._step3_merge_result(step1_results, step2_results, config)
            step3_time = time.perf_counter() - step3_start
            step_times['step3_åˆå¹¶åŽ»é‡'] = step3_time
            self.logger.info(
                f"æ­¥éª¤3å®Œæˆï¼Œåˆå¹¶åŽæ€»å…± {len(merged_results)} ä¸ªæ®µè½, è€—æ—¶: {step3_time:.3f}ç§’")

            # æ­¥éª¤4: è®¡ç®—æ®µè½æƒé‡
            step4_start = time.perf_counter()
            weighted_results = await self._step4_calculate_weight_of_contents(
                key_final=key_final,
                content_related=merged_results
            )
            step4_time = time.perf_counter() - step4_start
            step_times['step4_æƒé‡è®¡ç®—'] = step4_time
            self.logger.info(
                f"æ­¥éª¤4å®Œæˆï¼Œè®¡ç®—äº† {len(weighted_results)} ä¸ªæ®µè½çš„æƒé‡, è€—æ—¶: {step4_time:.3f}ç§’")

            # æ­¥éª¤5: PageRanké‡æŽ’åºï¼ˆä½¿ç”¨PageRankç®—æ³•ï¼‰
            step5_start = time.perf_counter()
            sorted_results = await self._step5_pageRank_of_contents(
                content_related=weighted_results,
                key_final=key_final  # ä¼ å…¥key_finalç”¨äºŽå®žä½“å…³è”
            )
            step5_time = time.perf_counter() - step5_start
            step_times['step5_PageRankæŽ’åº'] = step5_time
            self.logger.info(
                f"æ­¥éª¤5å®Œæˆï¼ŒæŽ’åºäº† {len(sorted_results)} ä¸ªæ®µè½, è€—æ—¶: {step5_time:.3f}ç§’")

            # æ­¥éª¤6: å–Top-Næ®µè½
            step6_start = time.perf_counter()
            final_sections = await self._step6_get_topn_sections(
                sorted_contents=sorted_results,
                top_k=config.rerank.max_results,
                config=config
            )
            step6_time = time.perf_counter() - step6_start
            step_times['Step6_Top-Nç­›é€‰'] = step6_time
            self.logger.info(
                f"âœ“ Step6 å®Œæˆ: æœ€ç»ˆè¿”å›ž {len(final_sections)} ä¸ªæ®µè½, è€—æ—¶: {step6_time:.3f}ç§’"
            )

            # è®¡ç®—æ€»è€—æ—¶
            overall_time = time.perf_counter() - overall_start

            # è¾“å‡ºè€—æ—¶ç»Ÿè®¡æ±‡æ€»
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ã€æ®µè½çº§ PageRankã€‘å„æ­¥éª¤è€—æ—¶æ±‡æ€»:")
            self.logger.info("-" * 80)
            self.logger.info(
                f"æŸ¥è¯¢å‘é‡ç”Ÿæˆ: {vector_time:.3f}ç§’ ({vector_time/overall_time*100:.1f}%)")
            for step_name, step_time in step_times.items():
                self.logger.info(
                    f"{step_name}: {step_time:.3f}ç§’ ({step_time/overall_time*100:.1f}%)")
            self.logger.info("-" * 80)
            self.logger.info(f"âœ“ æ€»è€—æ—¶: {overall_time:.3f}ç§’")
            self.logger.info("=" * 80)

            # ç›´æŽ¥è¿”å›žæ®µè½åˆ—è¡¨ï¼ˆä¸å†è½¬æ¢ä¸ºäº‹é¡¹ï¼‰
            return {"sections": final_sections}

        except Exception as e:
            self.logger.error(f"[æ®µè½çº§ PageRank] æœç´¢å¤±è´¥: {e}", exc_info=True)
            return {"sections": []}  # å¤±è´¥æ—¶è¿”å›žç©ºå­—å…¸

    async def _step1_keys_to_contents(
        self,
        key_final: List[Dict[str, Any]],
        query: str,
        source_config_ids: List[str],
        query_vector: Optional[List[float]] = None,  # å¯é€‰çš„æŸ¥è¯¢å‘é‡
        config: Optional[SearchConfig] = None  # æ·»åŠ configå‚æ•°ç”¨äºŽç¼“å­˜
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤1: keyæ‰¾content
        æ ¹æ®[key-final]ä»Žsqlä¸­æå–åŽŸæ–‡å—[content-key-related]ï¼Œå¹¶è®¡ç®—å’Œqueryçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        1. å®žä½“åŒ¹é…ï¼šæ ¹æ®ä¼ å…¥çš„ key_final ä¸­çš„å®žä½“åç§°å’Œç±»åž‹ï¼Œåœ¨ Entity è¡¨ä¸­æŸ¥æ‰¾åŒ¹é…çš„å®žä½“
        2. äº‹ä»¶å…³è”ï¼šé€šè¿‡ EventEntity è¡¨æ‰¾åˆ°ä¸Žè¿™äº›å®žä½“ç›¸å…³çš„äº‹ä»¶
        3. æ®µè½æŸ¥æ‰¾ï¼šé€šè¿‡ SourceEvent å’Œ Article è¡¨çš„å…³è”ï¼Œæ‰¾åˆ°å¯¹åº”çš„æ–‡ç« 
        4. æ®µè½è¿‡æ»¤ï¼šæ£€æŸ¥äº‹ä»¶çš„ references å­—æ®µï¼Œåªè¿”å›žçœŸæ­£è¢«äº‹ä»¶å¼•ç”¨çš„æ®µè½
        5. å‘é‡èŽ·å–ï¼šä»Ž ES æ‰¹é‡èŽ·å–æ®µè½çš„é¢„å­˜å‘é‡ï¼ˆcontent_vector = æ ‡é¢˜ + å†…å®¹[:500]ï¼‰
        6. ç›¸ä¼¼åº¦è®¡ç®—ï¼šè®¡ç®—æ¯ä¸ªæ®µè½å‘é‡ä¸Ž query çš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆå¾—åˆ†

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„key_finalæ•°æ®
            query: æŸ¥è¯¢æ–‡æœ¬
            source_id: æ•°æ®æºID
            k: è¿”å›žç»“æžœæ•°é‡
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            ç›¸å…³æ®µè½åˆ—è¡¨ï¼ˆContentSearchResult.to_dict()æ ¼å¼ï¼‰ï¼ŒæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦é™åºæŽ’åº
        """
        try:
            self.logger.info(
                f"æ­¥éª¤1å¼€å§‹: å¤„ç† {len(key_final)} ä¸ªkey, query='{query}'")

            if not key_final:
                return []

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æžœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
                if config and config.has_query_embedding:
                    self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
                else:
                    self.logger.debug(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.debug(f"ä½¿ç”¨ä¼ å…¥çš„æŸ¥è¯¢å‘é‡ï¼Œç»´åº¦: {len(query_vector)}")

            # 2. æå–å®žä½“IDå’Œæƒé‡
            # key_id å°±æ˜¯å®žä½“çš„ IDï¼Œæ— éœ€å†æŸ¥è¯¢ Entity è¡¨
            entity_ids = [key.get("key_id") or key.get("id")
                          for key in key_final]
            entity_weight_map = {
                key.get("key_id") or key.get("id"): key["weight"]
                for key in key_final
            }

            # è¿‡æ»¤æŽ‰å¯èƒ½ä¸º None çš„ ID
            entity_ids = [eid for eid in entity_ids if eid]

            if not entity_ids:
                self.logger.warning("key_final ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®žä½“ID")
                return []

            self.logger.info(
                f"ä»Ž {len(key_final)} ä¸ªkeyä¸­æå–åˆ° {len(entity_ids)} ä¸ªå®žä½“ID")

            async with self.session_factory() as session:
                # ðŸ†• ä¼˜åŒ–ï¼šç›´æŽ¥é€šè¿‡å®žä½“IDæŸ¥è¯¢ Entity è¡¨ï¼ˆä»…ç”¨äºŽèŽ·å–å®žä½“è¯¦æƒ…ï¼‰
                entity_query = (
                    select(Entity)
                    .where(
                        and_(
                            Entity.source_config_id.in_(source_config_ids),
                            Entity.id.in_(entity_ids)
                        )
                    )
                )

                entity_result = await session.execute(entity_query)
                found_entities = entity_result.scalars().all()

                if not found_entities:
                    self.logger.warning("æœªæ‰¾åˆ°åŒ¹é…çš„å®žä½“")
                    return []

                self.logger.info(f"æ‰¾åˆ° {len(found_entities)} ä¸ªåŒ¹é…å®žä½“")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ª key å¬å›žçš„å®žä½“ï¼ˆæŒ‰ key_id ä¸€å¯¹ä¸€æ˜¾ç¤ºï¼‰
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Key â†’ Entity æ˜ å°„ (ä¸€å¯¹ä¸€):")
                self.logger.info("-" * 80)

                for key in key_final:
                    key_id = key.get("key_id") or key.get("id")
                    key_display = f"{key['name']}({key['type']})"

                    # æ‰¾åˆ°å¯¹åº”çš„ entity
                    entity = next(
                        (e for e in found_entities if e.id == key_id), None)
                    if entity:
                        entity_display = f"{entity.name}({entity.type})"
                        self.logger.info(
                            f"  Key '{key_display}' [id={key_id[:8]}...] â†’ Entity '{entity_display}'")
                    else:
                        self.logger.warning(
                            f"  Key '{key_display}' [id={key_id[:8]}...] â†’ âŒ æœªæ‰¾åˆ°å¯¹åº”å®žä½“")

                self.logger.info("-" * 80)
                self.logger.info(
                    f"  æ€»è®¡: {len(key_final)} ä¸ªKey â†’ {len(found_entities)} ä¸ªEntity")
                self.logger.info("=" * 80)

                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¯ä¸ªåŒ¹é…çš„å®žä½“
                for entity in found_entities:
                    self.logger.debug(
                        f"  å®žä½“: {entity.name} (type={entity.type}, id={entity.id[:8]}...)"
                    )

                # 4. é€šè¿‡EventEntityæŸ¥æ‰¾ç›¸å…³äº‹ä»¶ï¼ˆé™åˆ¶åœ¨æŒ‡å®šsource_config_idså†…ï¼‰
                event_entity_query = (
                    select(EventEntity.event_id,
                           EventEntity.entity_id, EventEntity.weight)
                    .join(SourceEvent, EventEntity.event_id == SourceEvent.id)
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(source_config_ids),
                            EventEntity.entity_id.in_(entity_ids)
                        )
                    )
                    .distinct()
                )

                event_result = await session.execute(event_entity_query)
                event_entities = event_result.fetchall()

                if not event_entities:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³äº‹ä»¶")
                    return []

                # 5. è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„æƒé‡ï¼Œå¹¶è®°å½•äº‹ä»¶åˆ°å®žä½“çš„æ˜ å°„
                event_weights = {}
                event_to_entities = {}  # äº‹ä»¶ID -> å®žä½“IDåˆ—è¡¨çš„æ˜ å°„

                # åˆ›å»º entity_id -> key å¯¹è±¡çš„æ˜ å°„ï¼ˆç”¨äºŽåŽç»­æž„å»º cluesï¼‰
                entity_to_key = {}
                for key in key_final:
                    key_id = key.get("key_id") or key.get("id")
                    if key_id:
                        # åˆ›å»º key çš„å‰¯æœ¬ï¼Œå°† key_id é‡å‘½åä¸º id
                        key_copy = key.copy()
                        if "key_id" in key_copy:
                            key_copy["id"] = key_copy.pop("key_id")
                        elif "id" not in key_copy:
                            key_copy["id"] = key_id
                        entity_to_key[key_id] = key_copy

                for event_entity in event_entities:
                    event_id = event_entity.event_id
                    entity_id = event_entity.entity_id
                    event_entity_weight = event_entity.weight or 1.0
                    entity_weight = entity_weight_map.get(
                        entity_id, 1.0)  # ðŸ†• ç›´æŽ¥ç”¨ entity_id æŸ¥æ‰¾

                    # ç»¼åˆæƒé‡ = å®žä½“æƒé‡ Ã— å…³è”æƒé‡
                    combined_weight = float(
                        entity_weight) * float(event_entity_weight)
                    event_weights[event_id] = event_weights.get(
                        event_id, 0) + combined_weight

                    # è®°å½•äº‹ä»¶åˆ°å®žä½“çš„æ˜ å°„
                    if event_id not in event_to_entities:
                        event_to_entities[event_id] = []
                    event_to_entities[event_id].append(entity_id)

                event_ids = list(event_weights.keys())
                self.logger.info(f"æ‰¾åˆ° {len(event_ids)} ä¸ªç›¸å…³äº‹ä»¶")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤º Entity â†’ Event æ˜ å°„ï¼ˆé€šè¿‡keyå…³è”ï¼‰
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Entity â†’ Event æ˜ å°„:")
                self.logger.info("-" * 80)

                # æž„å»º entity_id â†’ entity_name æ˜ å°„
                entity_id_to_name = {
                    e.id: f"{e.name}({e.type})" for e in found_entities}

                # æŒ‰ entity åˆ†ç»„æ˜¾ç¤ºå…³è”çš„ events
                for entity_id, entity_name in entity_id_to_name.items():
                    related_events = [
                        event_id for event_id, entity_ids in event_to_entities.items()
                        if entity_id in entity_ids
                    ]
                    if related_events:
                        self.logger.info(
                            f"  Entity '{entity_name}' â†’ {len(related_events)} ä¸ªäº‹é¡¹: "
                            f"{', '.join([eid[:8]+'...' for eid in related_events[:5]])}"
                            f"{' ...' if len(related_events) > 5 else ''}"
                        )

                self.logger.info("=" * 80)

                # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¯ä¸ªäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯
                for event_id in event_ids:
                    self.logger.debug(
                        f"  äº‹ä»¶ {event_id[:8]}... æƒé‡={event_weights[event_id]:.3f}"
                    )

                # 6. é€šè¿‡äº‹ä»¶çš„ chunk_id æŸ¥æ‰¾å¯¹åº”çš„åŽŸæ–‡å—ï¼ˆSourceChunkï¼‰
                # ðŸ†• æ–°æž¶æž„ï¼šSourceEvent.chunk_id â†’ SourceChunkï¼ˆä¸€å¯¹ä¸€å…³ç³»ï¼‰

                # é¦–å…ˆèŽ·å–æ‰€æœ‰äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…æ‹¬ chunk_id å­—æ®µï¼‰
                event_detail_query = (
                    select(SourceEvent)
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(source_config_ids),
                            SourceEvent.id.in_(event_ids)
                        )
                    )
                )
                event_detail_result = await session.execute(event_detail_query)
                events = event_detail_result.scalars().all()

                # æ”¶é›†æ‰€æœ‰äº‹ä»¶çš„ chunk_id
                chunk_ids = set()
                event_to_chunk = {}  # äº‹ä»¶ID -> chunk_id çš„æ˜ å°„

                for event in events:
                    if event.chunk_id:
                        event_to_chunk[event.id] = event.chunk_id
                        chunk_ids.add(event.chunk_id)
                        self.logger.debug(
                            f"äº‹ä»¶ {event.id[:8]}... å…³è”åˆ° chunk {event.chunk_id[:8]}..."
                        )
                    else:
                        self.logger.warning(
                            f"äº‹ä»¶ {event.id[:8]}... æ²¡æœ‰ chunk_id å­—æ®µ"
                        )

                if not chunk_ids:
                    self.logger.warning("æ‰€æœ‰äº‹ä»¶éƒ½æ²¡æœ‰å…³è”åˆ°åŽŸæ–‡å—")
                    return []

                self.logger.info(
                    f"æ”¶é›†åˆ° {len(chunk_ids)} ä¸ªåŽŸæ–‡å—IDï¼ˆæ¥è‡ª {len(events)} ä¸ªäº‹ä»¶ï¼‰")

                # ä»Ž MySQL æŸ¥è¯¢ SourceChunk çš„åŸºæœ¬ä¿¡æ¯
                chunk_query = (
                    select(SourceChunk)
                    .where(
                        and_(
                            SourceChunk.source_config_id.in_(source_config_ids),
                            SourceChunk.id.in_(list(chunk_ids))
                        )
                    )
                    .order_by(SourceChunk.rank)
                )

                chunk_result = await session.execute(chunk_query)
                chunks = chunk_result.scalars().all()

                if not chunks:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³åŽŸæ–‡å—")
                    return []

                self.logger.info(f"ä»Ž MySQL æ‰¾åˆ° {len(chunks)} ä¸ªåŽŸæ–‡å—")

                # 7. æž„å»ºåŽŸæ–‡å—æ•°æ®
                # ä½¿ç”¨å­—å…¸å­˜å‚¨ï¼šchunk_id -> chunk data
                chunks_dict = {}  # key: chunk_id, value: chunk data

                # åå‘æ˜ å°„ï¼šchunk_id -> [event_ids]
                chunk_to_events = {}
                for event_id, chunk_id in event_to_chunk.items():
                    if chunk_id not in chunk_to_events:
                        chunk_to_events[chunk_id] = []
                    chunk_to_events[chunk_id].append(event_id)

                self.logger.debug(f"æž„å»ºäº† {len(chunk_to_events)} ä¸ªåŽŸæ–‡å—åˆ°äº‹ä»¶çš„æ˜ å°„")

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤º Event â†’ Chunk æ˜ å°„
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„ã€‘Event â†’ Chunk æ˜ å°„:")
                self.logger.info("-" * 80)

                # æ˜¾ç¤ºæ¯ä¸ªäº‹é¡¹å…³è”çš„åŽŸæ–‡å—
                for event in events[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªäº‹é¡¹
                    if event.chunk_id:
                        self.logger.info(
                            f"  Event {event.id[:8]}... ('{event.title[:30]}') â†’ Chunk {event.chunk_id[:8]}..."
                        )

                if len(events) > 10:
                    self.logger.info(f"  ... (è¿˜æœ‰ {len(events) - 10} ä¸ªäº‹é¡¹æœªæ˜¾ç¤º)")

                self.logger.info("=" * 80)

                # éåŽ†æ‰€æœ‰åŽŸæ–‡å—ï¼Œæž„å»ºåŽŸæ–‡å—æ•°æ®
                for chunk in chunks:
                    chunk_id = chunk.id

                    # æ‰¾åˆ°å¼•ç”¨è¯¥åŽŸæ–‡å—çš„æ‰€æœ‰äº‹ä»¶
                    related_event_ids = chunk_to_events.get(chunk_id, [])

                    if not related_event_ids:
                        self.logger.warning(
                            f"åŽŸæ–‡å— {chunk_id[:8]}... æ²¡æœ‰æ‰¾åˆ°å…³è”çš„äº‹ä»¶")
                        continue

                    # è®¡ç®—è¯¥åŽŸæ–‡å—çš„ç»¼åˆæƒé‡ï¼ˆæ‰€æœ‰å…³è”äº‹ä»¶çš„æƒé‡ä¹‹å’Œï¼‰
                    total_event_weight = sum(event_weights.get(
                        eid, 1.0) for eid in related_event_ids)

                    # æ”¶é›†è¯¥åŽŸæ–‡å—çš„ cluesï¼ˆä»Žå…³è”çš„äº‹ä»¶ä¸­æ”¶é›†å…³è”çš„å®žä½“ï¼‰
                    chunk_clues = []
                    seen_entity_ids = set()
                    for event_id in related_event_ids:
                        # èŽ·å–è¯¥äº‹ä»¶å…³è”çš„æ‰€æœ‰å®žä½“
                        entity_ids = event_to_entities.get(event_id, [])
                        for entity_id in entity_ids:
                            if entity_id not in seen_entity_ids:
                                # èŽ·å–å¯¹åº”çš„ key å¯¹è±¡
                                key = entity_to_key.get(entity_id)
                                if key:
                                    chunk_clues.append(key)
                                    seen_entity_ids.add(entity_id)

                    # åˆ›å»ºåŽŸæ–‡å—æ•°æ®
                    chunk_data = {
                        "chunk_id": chunk_id,
                        "source_id": chunk.source_id,
                        "event_ids": related_event_ids,  # æ‰€æœ‰å…³è”çš„äº‹ä»¶ID
                        "rank": chunk.rank,
                        "heading": chunk.heading,
                        "content": chunk.content,
                        "content_vector": None,  # å°†åœ¨åŽé¢ä»Ž ES èŽ·å–
                        "entity_weight": total_event_weight,
                        "created_time": chunk.created_time,
                        "extra_data": chunk.extra_data or {},
                        "clues": chunk_clues,  # å¬å›žè¯¥åŽŸæ–‡å—çš„ key åˆ—è¡¨
                        "references": chunk.references or []  # SourceChunk å¼•ç”¨çš„ ArticleSection IDåˆ—è¡¨
                    }
                    chunks_dict[chunk_id] = chunk_data

                    self.logger.debug(
                        f"åŽŸæ–‡å— {chunk_id[:8]}... å…³è”äº† {len(related_event_ids)} ä¸ªäº‹ä»¶ï¼Œ"
                        f"ç»¼åˆæƒé‡={total_event_weight:.3f}ï¼Œcluesæ•°={len(chunk_clues)}"
                    )

                # è½¬æ¢ä¸ºåˆ—è¡¨
                chunks_data = list(chunks_dict.values())

                # ç»Ÿè®¡ä¿¡æ¯
                multi_event_chunks = [
                    c for c in chunks_data if len(c["event_ids"]) > 1]
                self.logger.info(
                    f"æž„å»ºäº† {len(chunks_data)} ä¸ªå”¯ä¸€åŽŸæ–‡å—æ•°æ®"
                )
                if multi_event_chunks:
                    self.logger.info(
                        f"å…¶ä¸­ {len(multi_event_chunks)} ä¸ªåŽŸæ–‡å—å…³è”äº†å¤šä¸ªäº‹ä»¶"
                    )

                if not chunks_data:
                    self.logger.warning("è¿‡æ»¤åŽæ²¡æœ‰æ‰¾åˆ°çœŸæ­£å…³è”çš„åŽŸæ–‡å—")
                    return []

                # ðŸ†• æ—¥å¿—ï¼šæ±‡æ€»æ˜¾ç¤ºå®Œæ•´å¬å›žè·¯å¾„ç»Ÿè®¡
                self.logger.info("=" * 80)
                self.logger.info("ã€Step1 å¬å›žè·¯å¾„æ±‡æ€»ã€‘å®Œæ•´å¬å›žé“¾:")
                self.logger.info("-" * 80)
                self.logger.info(f"  Keyæ•°é‡: {len(key_final)}")
                self.logger.info(
                    f"  â†’ Entityæ•°é‡: {len(found_entities)} (é€šè¿‡KeyåŒ¹é…)")
                self.logger.info(f"  â†’ Eventæ•°é‡: {len(events)} (é€šè¿‡Entityå…³è”)")
                self.logger.info(
                    f"  â†’ Chunkæ•°é‡: {len(chunks_data)} (é€šè¿‡Event.chunk_id)")
                self.logger.info("-" * 80)
                self.logger.info(f"  å¬å›žè·¯å¾„: Key â†’ Entity â†’ Event â†’ Chunk")
                self.logger.info("=" * 80)

                # 7.5. ä»Ž ES æ‰¹é‡èŽ·å–åŽŸæ–‡å—çš„é¢„å­˜å‘é‡
                chunk_ids_list = list(chunks_dict.keys())
                self.logger.info(
                    f"ä»Ž ES æ‰¹é‡èŽ·å– {len(chunk_ids_list)} ä¸ªåŽŸæ–‡å—çš„é¢„å­˜å‘é‡...")

                es_chunks_data = await self.content_repo.get_chunks_by_ids(
                    chunk_ids=chunk_ids_list,
                    include_vectors=True
                )

                # æž„å»º chunk_id -> content_vector çš„æ˜ å°„
                chunk_vector_map = {}
                for es_chunk in es_chunks_data:
                    chunk_id = es_chunk.get('chunk_id')
                    content_vector = es_chunk.get('content_vector')
                    if chunk_id and content_vector:
                        chunk_vector_map[chunk_id] = content_vector

                self.logger.info(
                    f"ä»Ž ES èŽ·å–åˆ° {len(chunk_vector_map)} ä¸ªåŽŸæ–‡å—çš„é¢„å­˜å‘é‡ "
                    f"(è¯·æ±‚äº† {len(chunk_ids_list)} ä¸ª)"
                )

                # å°†èŽ·å–åˆ°çš„å‘é‡å¡«å……åˆ°åŽŸæ–‡å—æ•°æ®ä¸­
                filled_count = 0
                for chunk in chunks_data:
                    chunk_id = chunk['chunk_id']
                    if chunk_id in chunk_vector_map:
                        chunk['content_vector'] = chunk_vector_map[chunk_id]
                        filled_count += 1
                    else:
                        self.logger.warning(
                            f"åŽŸæ–‡å— {chunk_id[:8]}... åœ¨ ES ä¸­æœªæ‰¾åˆ°é¢„å­˜å‘é‡ï¼Œå°†çŽ°åœºç”Ÿæˆ"
                        )

                self.logger.info(
                    f"æˆåŠŸå¡«å…… {filled_count}/{len(chunks_data)} ä¸ªåŽŸæ–‡å—çš„é¢„å­˜å‘é‡"
                )

                # 8. è®¡ç®—å‘é‡ç›¸ä¼¼åº¦å¾—åˆ†
                similarity_scores = await self._calculate_cosine_scores(
                    query_vector=query_vector,
                    paragraphs=chunks_data  # è¿™é‡Œä¿æŒå‚æ•°åä¸º paragraphsï¼Œå› ä¸ºæ–¹æ³•å†…éƒ¨ä½¿ç”¨è¿™ä¸ªåç§°
                )
                self.logger.debug(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ")

                # 9. ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆå¾—åˆ†
                content_results = []

                # æ·»åŠ è¯¦ç»†å¾—åˆ†æ—¥å¿—ï¼ˆä¸Žæ­¥éª¤2å¯¹åº”ï¼‰
                self.logger.info("=" * 80)
                self.logger.info("æ­¥éª¤1 å¾—åˆ†è¯¦æƒ…ï¼ˆçº¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨ESé¢„å­˜å‘é‡ï¼‰ï¼š")
                self.logger.info("-" * 80)

                for idx, chunk in enumerate(chunks_data, start=1):
                    chunk_id = chunk["chunk_id"]

                    # ç›´æŽ¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†ï¼ˆèŒƒå›´é€šå¸¸åœ¨[0, 1]ä¹‹é—´ï¼‰
                    cosine_score = similarity_scores.get(chunk_id, 0.0)

                    # èŽ·å–è¯¥åŽŸæ–‡å—å…³è”çš„æ‰€æœ‰äº‹ä»¶ID
                    event_ids = chunk["event_ids"]

                    # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ªåŽŸæ–‡å—çš„å¾—åˆ†å’Œå…³è”çš„äº‹ä»¶æ•°
                    heading_preview = chunk.get("heading", "")[:40]
                    self.logger.info(
                        f"åŽŸæ–‡å— {chunk_id[:8]}... | "
                        f"Cosine={cosine_score:.4f} | "
                        f"å…³è”äº‹ä»¶æ•°={len(event_ids)} | "
                        f"æ ‡é¢˜: {heading_preview}"
                    )

                    # DEBUGçº§åˆ«ï¼šæ˜¾ç¤ºæ‰€æœ‰å…³è”çš„äº‹ä»¶ID
                    if len(event_ids) > 1:
                        event_ids_preview = [
                            eid[:8] + "..." for eid in event_ids]
                        self.logger.debug(f"  äº‹ä»¶IDåˆ—è¡¨: {event_ids_preview}")

                    # åˆ›å»º ContentSearchResult å¯¹è±¡ï¼Œæ·»åŠ ç¼–å·å’Œ clues
                    result = ContentSearchResult(
                        search_type=f"SQL-{idx}",
                        source_config_id=source_config_ids[0] if source_config_ids else "",
                        source_id=chunk["source_id"],  # SourceChunk.source_id (æ–‡ç« ID)
                        chunk_id=chunk_id,  # SourceChunk.id
                        rank=chunk["rank"],
                        heading=chunk["heading"],
                        content=chunk["content"],
                        score=cosine_score,
                        event_ids=event_ids,  # è®°å½•æ‰€æœ‰å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
                        clues=chunk.get("clues", []),  # è®°å½•å¬å›žè¯¥åŽŸæ–‡å—çš„ key åˆ—è¡¨
                    )

                    content_results.append(result)

                self.logger.info("=" * 80)

                # 10. æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æŽ’åº
                content_results.sort(key=lambda x: x.score, reverse=True)

                # 11. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦ç»“æžœ
                original_count = len(content_results)
                if config and config.rerank.score_threshold:
                    filtered_results = [
                        r for r in content_results if r.score >= config.rerank.score_threshold]

                    if len(filtered_results) < original_count:
                        self.logger.info(
                            f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªåŽŸæ–‡å— "
                            f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                        )

                        # å±•ç¤ºè¿‡æ»¤åŽä¿ç•™çš„æ®µè½ä¿¡æ¯
                        if filtered_results:
                            self.logger.info("=" * 80)
                            self.logger.info(
                                f"è¿‡æ»¤åŽä¿ç•™çš„ {len(filtered_results)} ä¸ªæ®µè½ï¼š")
                            self.logger.info("-" * 80)
                            for result in filtered_results:
                                heading_preview = result.heading[:
                                                                 40] if result.heading else "æ— æ ‡é¢˜"
                                self.logger.info(
                                    f"æ®µè½ {result.chunk_id[:8]}... | "
                                    f"Cosine={result.score:.4f} | "
                                    f"æ ‡é¢˜: {heading_preview}"
                                )
                            self.logger.info("=" * 80)

                    content_results = filtered_results
                else:
                    self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

                # ðŸ†• æ ¹æ® max_key_recall_results æˆªæ–­ï¼ˆåœ¨æž„å»ºçº¿ç´¢å‰ï¼ŒæŒ‰ç›¸ä¼¼åº¦æŽ’åºï¼‰
                max_key_results = config.rerank.max_key_recall_results if config else 30
                if len(content_results) > max_key_results:
                    self.logger.warning(
                        f"âš ï¸  [æ®µè½çº§Step1] Keyå¬å›žæ®µè½æ•°({len(content_results)})è¶…è¿‡max_key_recall_results({max_key_results})ï¼Œ"
                        f"å°†æŒ‰ç›¸ä¼¼åº¦æŽ’åºåŽæˆªæ–­"
                    )

                    # å·²ç»æŒ‰ç›¸ä¼¼åº¦é™åºæŽ’åºäº†ï¼ˆç¬¬820è¡Œï¼‰ï¼Œç›´æŽ¥æˆªæ–­
                    truncated_results = content_results[:max_key_results]

                    self.logger.info(
                        f"ðŸ“Š [æ®µè½çº§Step1] æˆªæ–­ç»Ÿè®¡: "
                        f"ä¿ç•™{len(truncated_results)}ä¸ª, "
                        f"ä¸¢å¼ƒ{len(content_results) - len(truncated_results)}ä¸ª"
                    )

                    content_results = truncated_results

                # ðŸ†• æ—¥å¿—ï¼šæ˜¾ç¤ºè¿‡æ»¤åŽçš„æœ‰æ•ˆæ˜ å°„å…³ç³»
                if content_results:
                    # æ”¶é›†è¿‡æ»¤åŽçš„æ‰€æœ‰ chunk_ids å’Œ event_ids
                    filtered_chunk_ids = {
                        r.chunk_id for r in content_results}
                    filtered_event_ids = set()
                    for r in content_results:
                        filtered_event_ids.update(r.event_ids)

                    self.logger.info("=" * 80)
                    self.logger.info("ã€Step1 å¬å›žè·¯å¾„è¿‡æ»¤åŽã€‘æœ‰æ•ˆæ˜ å°„å…³ç³»:")
                    self.logger.info("-" * 80)

                    # 1. Entity â†’ Event æ˜ å°„ï¼ˆåªæ˜¾ç¤ºæœ‰ä¿ç•™æ®µè½çš„äº‹é¡¹ï¼‰
                    self.logger.info("  1ï¸âƒ£ Entity â†’ Event (ä»…ä¿ç•™æœ‰æ•ˆäº‹é¡¹):")
                    entity_event_count = {}

                    # ðŸ†• åˆ›å»º event_id â†’ event_title çš„æ˜ å°„
                    event_title_map = {
                        event.id: event.title for event in events}

                    for entity_id, entity_name in entity_id_to_name.items():
                        # æ‰¾åˆ°è¯¥å®žä½“å…³è”çš„ã€ä¸”æœ‰ä¿ç•™æ®µè½çš„äº‹é¡¹
                        valid_events = [
                            event_id for event_id, entity_ids in event_to_entities.items()
                            if entity_id in entity_ids and event_id in filtered_event_ids
                        ]
                        if valid_events:
                            entity_event_count[entity_name] = len(valid_events)

                            # ðŸ†• æ˜¾ç¤ºäº‹é¡¹IDå’Œæ ‡é¢˜
                            events_preview_parts = []
                            for eid in valid_events[:3]:
                                event_title = event_title_map.get(eid, "")
                                title_preview = event_title[:30] if event_title else "æ— æ ‡é¢˜"
                                events_preview_parts.append(
                                    f"{eid[:8]}...({title_preview})")

                            events_preview = ', '.join(events_preview_parts)
                            if len(valid_events) > 3:
                                events_preview += f' ... (å…±{len(valid_events)}ä¸ª)'

                            self.logger.info(
                                f"     {entity_name} â†’ {len(valid_events)} ä¸ªäº‹é¡¹: {events_preview}")

                    # 2. Event â†’ Chunk æ˜ å°„ï¼ˆåªæ˜¾ç¤ºä¿ç•™çš„åŽŸæ–‡å—ï¼‰
                    self.logger.info("")
                    self.logger.info("  2ï¸âƒ£ Event â†’ Chunk (ä»…ä¿ç•™åŽŸæ–‡å—):")
                    event_section_count = {}
                    displayed_count = 0

                    # ðŸ†• åˆ›å»º chunk_id â†’ heading çš„æ˜ å°„
                    chunk_heading_map = {}
                    for content in content_results:
                        chunk_heading_map[content.chunk_id] = content.heading or ""

                    # ðŸ†• åªéåŽ†æœ‰æ•ˆçš„äº‹é¡¹ï¼ˆåœ¨ filtered_event_ids ä¸­çš„ï¼‰
                    for event in events:
                        if event.id in filtered_event_ids:
                            # æ‰¾åˆ°è¯¥äº‹é¡¹å…³è”çš„ã€ä¸”è¢«ä¿ç•™çš„æ®µè½
                            valid_sections = [
                                sid for sid in (event.references or [])
                                if sid in filtered_chunk_ids
                            ]
                            if valid_sections:
                                event_section_count[event.id] = len(
                                    valid_sections)

                                # ðŸ†• æ˜¾ç¤ºæ®µè½IDå’Œæ ‡é¢˜
                                sections_preview_parts = []
                                for sid in valid_sections[:3]:
                                    section_heading = chunk_heading_map.get(
                                        sid, "")
                                    heading_preview = section_heading[:
                                                                      30] if section_heading else "æ— æ ‡é¢˜"
                                    sections_preview_parts.append(
                                        f"{sid[:8]}...({heading_preview})")

                                sections_preview = ', '.join(
                                    sections_preview_parts)
                                if len(valid_sections) > 3:
                                    sections_preview += f' ... (å…±{len(valid_sections)}ä¸ª)'

                                self.logger.info(
                                    f"     Event {event.id[:8]}... ('{event.title[:30]}') "
                                    f"â†’ {len(valid_sections)} ä¸ªæ®µè½: {sections_preview}"
                                )
                                displayed_count += 1

                                # é™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼ˆé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
                                if displayed_count >= 10:
                                    break

                    # ç»Ÿè®¡æ‰€æœ‰æœ‰æ•ˆäº‹é¡¹
                    total_valid_events = sum(
                        1 for e in events if e.id in filtered_event_ids)
                    if displayed_count < total_valid_events:
                        self.logger.info(
                            f"     ... (è¿˜æœ‰ {total_valid_events - displayed_count} ä¸ªæœ‰æ•ˆäº‹é¡¹æœªæ˜¾ç¤º)")

                    # 3. ç»Ÿè®¡æ±‡æ€»
                    self.logger.info("")
                    self.logger.info("  ðŸ“Š è¿‡æ»¤æ•ˆæžœç»Ÿè®¡:")
                    self.logger.info(
                        f"     è¿‡æ»¤å‰: {len(found_entities)} ä¸ªEntity â†’ {len(events)} ä¸ªEvent â†’ {len(chunks_data)} ä¸ªChunk")
                    self.logger.info(
                        f"     è¿‡æ»¤åŽ: {len(entity_event_count)} ä¸ªæœ‰æ•ˆEntity â†’ {len(filtered_event_ids)} ä¸ªæœ‰æ•ˆEvent â†’ {len(filtered_chunk_ids)} ä¸ªæœ‰æ•ˆChunk")
                    self.logger.info(
                        f"     è¿‡æ»¤çŽ‡: Entity={len(entity_event_count)/len(found_entities)*100:.1f}%, Event={len(filtered_event_ids)/len(events)*100:.1f}%, Chunk={len(filtered_chunk_ids)/len(chunks_data)*100:.1f}%")

                    self.logger.info("=" * 80)

                    # ðŸ†• æž„å»º Step1 é˜¶æ®µçš„çº¿ç´¢
                    from sag.modules.search.tracker import Tracker
                    tracker = Tracker(config)

                    # å‡†å¤‡æ•°æ®æ˜ å°„
                    # 1. entity_id â†’ entity_weight æ˜ å°„
                    entity_weight_map = {key["key_id"]: key.get(
                        "weight", 0.0) for key in key_final}

                    # 2. 
                    chunk_data_map = {}
                    for content in content_results:
                        chunk_data_map[content.chunk_id] = {
                            "chunk_id": content.chunk_id,
                            "id": content.chunk_id,
                            "heading": content.heading or "",
                            "content": content.content or "",
                            "summary": "",
                            "section_type": getattr(content, 'search_type', ''),
                            "score": content.score  # ä½™å¼¦ç›¸ä¼¼åº¦
                        }

                    # A. æž„å»º Entity â†’ Event çº¿ç´¢
                    entity_event_clue_count = 0
                    for entity in found_entities:
                        entity_id = entity.id
                        entity_weight = entity_weight_map.get(entity_id, 0.0)

                        # æ‰¾åˆ°è¿™ä¸ªå®žä½“å…³è”çš„æ‰€æœ‰æœ‰æ•ˆäº‹é¡¹
                        for event in events:
                            if event.id in filtered_event_ids:
                                # æ£€æŸ¥è¿™ä¸ªäº‹é¡¹æ˜¯å¦å…³è”è¿™ä¸ªå®žä½“
                                if entity_id in event_to_entities.get(event.id, []):
                                    # æž„å»ºèŠ‚ç‚¹
                                    entity_node = Tracker.build_entity_node({
                                        "key_id": entity_id,
                                        "id": entity_id,
                                        "name": entity.name,
                                        "type": entity.type,
                                        "description": entity.description or ""
                                    })
                                    # ðŸ†• ä½¿ç”¨ tracker å®žä¾‹æ–¹æ³•ï¼ŒæŒ‡å®šå¬å›žæ–¹å¼ä¸º "entity"
                                    event_node = tracker.get_or_create_event_node(
                                        event, "rerank", recall_method="entity")

                                    # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨å®žä½“æƒé‡ï¼‰
                                    tracker.add_clue(
                                        stage="rerank",
                                        from_node=entity_node,
                                        to_node=event_node,
                                        confidence=entity_weight,
                                        relation="å®žä½“å¬å›ž",
                                        metadata={
                                            "method": "entity_recall",
                                            "entity_weight": entity_weight,
                                            "step": "step1"
                                        }
                                    )
                                    entity_event_clue_count += 1

                    # B. æž„å»º Event â†’ Chunk çº¿ç´¢
                    event_chunk_clue_count = 0
                    for event in events:
                        if event.id in filtered_event_ids and event.chunk_id:
                            # æ£€æŸ¥è¯¥äº‹é¡¹çš„chunkæ˜¯å¦åœ¨è¿‡æ»¤åŽçš„ç»“æžœä¸­
                            if event.chunk_id in filtered_chunk_ids:
                                # èŽ·å–chunkæ•°æ®
                                chunk_data = chunk_data_map.get(event.chunk_id)
                                if chunk_data:
                                    # æž„å»ºèŠ‚ç‚¹
                                    event_node = tracker.get_or_create_event_node(
                                        event, "rerank", recall_method="entity")
                                    chunk_node = Tracker.build_section_node(chunk_data)

                                    # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨chunkçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                                    tracker.add_clue(
                                        stage="rerank",
                                        from_node=event_node,
                                        to_node=chunk_node,
                                        confidence=chunk_data["score"],
                                        relation="åŽŸæ–‡å—å¬å›ž",
                                        metadata={
                                            "method": "chunk_recall",
                                            "chunk_score": chunk_data["score"],
                                            "step": "step1"
                                        }
                                    )
                                    event_chunk_clue_count += 1

                    self.logger.info("=" * 80)
                    self.logger.info(
                        f"ðŸ”— [Step1 çº¿ç´¢æž„å»º] Entityâ†’Event={entity_event_clue_count}æ¡, "
                        f"Eventâ†’Chunk={event_chunk_clue_count}æ¡"
                    )
                    self.logger.info("=" * 80)

                self.logger.info(
                    f"æ­¥éª¤1å®Œæˆ: å¤„ç†äº† {len(content_results)} ä¸ªæ®µè½",
                    extra={
                        "avg_cosine_score": np.mean([r.score for r in content_results]) if content_results else 0.0
                    }
                )

                # æ˜¾ç¤ºTop 5ç»“æžœ
                top_results = content_results[:5]
                for i, result in enumerate(top_results, 1):
                    self.logger.debug(
                        f"Top {i}: {result.heading[:50]} - "
                        f"Cosine:{result.score:.3f}"
                    )

                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨è¿”å›ž
                return [r.to_dict() for r in content_results]

        except Exception as e:
            self.logger.error(f"æ­¥éª¤1æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step2_query_to_contents(
        self,
        query: str,
        source_config_ids: List[str],
        k: int = 30,  # ðŸ†• é»˜è®¤å€¼æ”¹ä¸º30ï¼Œä¸Žmax_query_recall_resultsä¸€è‡´
        query_vector: Optional[List[float]] = None,  # å¯é€‰çš„æŸ¥è¯¢å‘é‡
        config: Optional[SearchConfig] = None  # æ·»åŠ configå‚æ•°ç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤2: queryæ‰¾contentï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°åŽŸæ–‡å—[content-query-related]ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: ESå¬å›žæ•°é‡ï¼ˆå»ºè®®ä½¿ç”¨config.rerank.max_query_recall_resultsï¼‰
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤ï¼‰

        Returns:
            ç›¸å…³æ®µè½åˆ—è¡¨ï¼ˆContentSearchResult.to_dict()æ ¼å¼ï¼‰
        """
        try:
            self.logger.info(
                f"æ­¥éª¤2å¼€å§‹: query='{query}', source_config_ids={source_config_ids}")

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æžœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
            if config and config.has_query_embedding:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # 2. ä½¿ç”¨KNNæœç´¢æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ
            similar_paragraphs = await self._search_similar_paragraphs(
                query_vector=query_vector,
                source_config_ids=source_config_ids,
                k=k
            )
            self.logger.info(f"KNNæœç´¢æ‰¾åˆ° {len(similar_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")

            if not similar_paragraphs:
                self.logger.warning("æœªæ‰¾åˆ°ç›¸ä¼¼æ®µè½")
                return []

            # 3. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
            cosine_scores = await self._calculate_cosine_scores(
                query_vector=query_vector,
                paragraphs=similar_paragraphs
            )

            # 4. ä¸ºæ¯ä¸ªåŽŸæ–‡å—æ·»åŠ å¾—åˆ†ä¿¡æ¯å¹¶åˆ›å»º ContentSearchResult å¯¹è±¡
            content_results = []

            # æ·»åŠ è¯¦ç»†å¾—åˆ†æ—¥å¿—
            self.logger.info("=" * 80)
            self.logger.info("æ­¥éª¤2 å¾—åˆ†è¯¦æƒ…ï¼ˆçº¯ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼š")
            self.logger.info("-" * 80)

            for idx, chunk in enumerate(similar_paragraphs, start=1):
                # ES è¿”å›žçš„æ˜¯ chunk_id å’Œ source_id
                chunk_id = chunk.get("chunk_id")
                cosine_score = cosine_scores.get(chunk_id, 0.0)

                # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ªåŽŸæ–‡å—çš„å¾—åˆ†
                heading_preview = chunk.get("heading", "")[:40]
                self.logger.info(
                    f"åŽŸæ–‡å— {chunk_id[:8]}... | "
                    f"Cosine={cosine_score:.4f} | "
                    f"æ ‡é¢˜: {heading_preview}"
                )

                # åˆ›å»º ContentSearchResult å¯¹è±¡ï¼Œæ·»åŠ ç¼–å·å’Œ clues
                # Embedding æœç´¢ä½¿ç”¨ query ä½œä¸º clue
                query_clue = {
                    "type": "query",
                    "name": query,
                    "weight": 1.0,
                    "source": "embedding"
                }

                # åˆ›å»º ContentSearchResult å¯¹è±¡ï¼Œæ·»åŠ ç¼–å·å’Œ clues
                # Embedding æœç´¢ä½¿ç”¨ query ä½œä¸º clue
                result = ContentSearchResult(
                    search_type=f"embedding-{idx}",
                    source_config_id=source_config_ids[0] if source_config_ids else "",
                    source_id=chunk.get("source_id", ""),  # SourceChunk.source_id (æ–‡ç« ID)
                    chunk_id=chunk_id,  # SourceChunk.id
                    rank=chunk.get("rank", 0),
                    heading=chunk.get("heading", ""),
                    content=chunk.get("content", ""),
                    score=cosine_score,  # ç›´æŽ¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†
                    event_ids=[],  # embeddingæœç´¢æ²¡æœ‰ç›´æŽ¥å…³è”çš„event_id
                    clues=[query_clue],  # ä½¿ç”¨ query ä½œä¸ºå¬å›žçº¿ç´¢
                )
                content_results.append(result)

            self.logger.info("=" * 80)

            # 5. æŒ‰ä½™å¼¦ç›¸ä¼¼åº¦æŽ’åº
            content_results.sort(key=lambda x: x.score, reverse=True)

            # 6. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½Žç›¸ä¼¼åº¦ç»“æžœ
            original_count = len(content_results)
            if config and config.rerank.score_threshold:
                filtered_results = [
                    r for r in content_results if r.score >= config.rerank.score_threshold]

                if len(filtered_results) < original_count:
                    self.logger.info(
                        f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªæ®µè½ "
                        f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                    )

                    # å±•ç¤ºè¿‡æ»¤åŽä¿ç•™çš„æ®µè½ä¿¡æ¯
                    if filtered_results:
                        self.logger.info("=" * 80)
                        self.logger.info(
                            f"è¿‡æ»¤åŽä¿ç•™çš„ {len(filtered_results)} ä¸ªæ®µè½ï¼š")
                        self.logger.info("-" * 80)
                        for result in filtered_results:
                            heading_preview = result.heading[:
                                                             40] if result.heading else "æ— æ ‡é¢˜"
                            self.logger.info(
                                f"æ®µè½ {result.chunk_id[:8]}... | "
                                f"Cosine={result.score:.4f} | "
                                f"æ ‡é¢˜: {heading_preview}"
                            )
                        self.logger.info("=" * 80)

                content_results = filtered_results
            else:
                self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

            self.logger.info(
                f"æ­¥éª¤2å®Œæˆ: å¤„ç†äº† {len(content_results)} ä¸ªæ®µè½",
                extra={
                    "avg_cosine_score": np.mean([r.score for r in content_results]) if content_results else 0.0
                }
            )

            # æ˜¾ç¤ºTop 5ç»“æžœ
            top_results = content_results[:5]
            for i, result in enumerate(top_results, 1):
                self.logger.debug(
                    f"Top {i}: {result.heading[:50]} - "
                    f"Cosine:{result.score:.3f}"
                )

            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨è¿”å›ž
            return [r.to_dict() for r in content_results]

        except Exception as e:
            self.logger.error(f"æ­¥éª¤2æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _generate_query_vector(self, query: str, config: SearchConfig = None) -> List[float]:
        """
        å°†queryè½¬åŒ–æˆå‘é‡

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽç¼“å­˜query_vectorï¼‰

        Returns:
            æŸ¥è¯¢å‘é‡
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_vectorï¼ˆå¦‚æžœæœ‰configä¼ å…¥ï¼‰
            if config and config.has_query_embedding and config.query_embedding:
                self.logger.debug(
                    f"ðŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(config.query_embedding)}")
                return config.query_embedding

            # ä½¿ç”¨processorç”Ÿæˆå‘é‡
            query_vector = await self.processor.generate_embedding(query)
            self.logger.debug(f"Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # å¦‚æžœæœ‰configï¼Œç¼“å­˜query_vector
            if config:
                config.query_embedding = query_vector
                config.has_query_embedding = True
                self.logger.debug("ðŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")

            return query_vector
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

    async def _search_similar_paragraphs(
        self,
        query_vector: List[float],
        source_config_ids: List[str],
        k: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨KNNæœç´¢æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ

        éåŽ†æ‰€æœ‰æ•°æ®æºï¼Œåˆå¹¶æœç´¢ç»“æžœ

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: æ¯ä¸ªæ•°æ®æºè¿”å›žçš„æ•°é‡

        Returns:
            ç›¸ä¼¼æ®µè½åˆ—è¡¨ï¼ˆåˆå¹¶æ‰€æœ‰æ•°æ®æºçš„ç»“æžœï¼‰
        """
        try:
            all_paragraphs = []

            # å¦‚æžœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œåˆ™æœç´¢æ‰€æœ‰æ•°æ®æº
            if not source_config_ids:
                self.logger.info("æœªæŒ‡å®šæ•°æ®æºï¼Œæœç´¢æ‰€æœ‰æ•°æ®æº")
                similar_paragraphs = await self.content_repo.search_similar_by_content(
                    query_vector=query_vector,
                    k=k,
                    source_id=None
                )
                all_paragraphs.extend(similar_paragraphs)
            else:
                # éåŽ†æ¯ä¸ªæ•°æ®æºè¿›è¡Œæœç´¢
                self.logger.info(f"éåŽ† {len(source_config_ids)} ä¸ªæ•°æ®æºè¿›è¡ŒKNNæœç´¢")
                for source_id in source_config_ids:
                    try:
                        similar_paragraphs = await self.content_repo.search_similar_by_content(
                            query_vector=query_vector,
                            k=k,
                            source_id=source_id  # ä½¿ç”¨å•ä¸ª source_id
                        )
                        all_paragraphs.extend(similar_paragraphs)
                        self.logger.debug(
                            f"æ•°æ®æº {source_id[:8]}... æ‰¾åˆ° {len(similar_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
                    except Exception as e:
                        self.logger.warning(
                            f"æ•°æ®æº {source_id[:8]}... æœç´¢å¤±è´¥: {e}")
                        continue

            self.logger.info(f"KNNæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
            return all_paragraphs

        except Exception as e:
            self.logger.error(f"KNNæœç´¢å¤±è´¥: {e}")
            raise AIError(f"KNNæœç´¢å¤±è´¥: {e}") from e

    async def _calculate_cosine_scores(
        self,
        query_vector: List[float],
        paragraphs: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            paragraphs: æ®µè½åˆ—è¡¨ï¼ˆåŒ…å« chunk_id  å­—æ®µï¼‰

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å­—å…¸ {chunk_id: score}
        """
        try:
            if not paragraphs:
                return {}

            cosine_scores = {}

            for paragraph in paragraphs:
                # ðŸ”‘ æå–chunk_idï¼ˆä¼˜å…ˆä½¿ç”¨æ–°æž¶æž„çš„chunk_idå­—æ®µï¼‰
                chunk_id = paragraph.get("chunk_id") 

                # ä»Žæ®µè½ä¸­èŽ·å–é¢„å­˜çš„å‘é‡
                content_vector = paragraph.get("content_vector")

                if content_vector:
                    # ç›´æŽ¥ä½¿ç”¨ESè¿”å›žçš„é¢„å­˜å‘é‡
                    pass
                else:
                    # å¦‚æžœæ²¡æœ‰é¢„å­˜å‘é‡ï¼ŒçŽ°åœºç”Ÿæˆï¼ˆåªå¯¹contentç”Ÿæˆï¼Œä¸åŒ…å«æ ‡é¢˜ï¼‰
                    content = paragraph.get(
                        'section_content') or paragraph.get('content', '')
                    if not content.strip():
                        # å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡
                        self.logger.warning(f"æ®µè½ {chunk_id} å†…å®¹ä¸ºç©ºä¸”æ— é¢„å­˜å‘é‡ï¼Œè·³è¿‡")
                        continue

                    content_vector = await self.processor.generate_embedding(content)

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = await self._cosine_similarity(query_vector, content_vector)
                cosine_scores[chunk_id] = similarity

            self.logger.info(
                f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ - å…± {len(cosine_scores)} ä¸ªæ®µè½, "
                f"å¹³å‡ç›¸ä¼¼åº¦: {np.mean(list(cosine_scores.values())):.4f}"
            )

            return cosine_scores

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

    async def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            v1 = np.array(vec1)
            v2 = np.array(vec2)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(v1, v2)
            norm_v1 = np.linalg.norm(v1)
            norm_v2 = np.linalg.norm(v2)

            if norm_v1 == 0 or norm_v2 == 0:
                return 0.0

            similarity = dot_product / (norm_v1 * norm_v2)
            return float(similarity)

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return 0.0

    def _batch_cosine_similarity(
        self,
        query_vector: List[float],
        target_vectors: List[List[float]]
    ) -> np.ndarray:
        """
        æ‰¹é‡è®¡ç®—queryå‘é‡ä¸Žå¤šä¸ªç›®æ ‡å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            target_vectors: ç›®æ ‡å‘é‡åˆ—è¡¨

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦æ•°ç»„
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            query_array = np.array(query_vector)
            target_array = np.array(target_vectors)

            # è®¡ç®—ç‚¹ç§¯
            dot_products = np.dot(target_array, query_array)

            # è®¡ç®—èŒƒæ•°
            query_norm = np.linalg.norm(query_array)
            target_norms = np.linalg.norm(target_array, axis=1)

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆé¿å…é™¤ä»¥é›¶ï¼‰
            denominators = target_norms * query_norm
            similarities = np.where(
                denominators > 0,
                dot_products / denominators,
                0.0
            )

            return similarities

        except Exception as e:
            self.logger.error(f"æ‰¹é‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return np.zeros(len(target_vectors))

    
    async def _step3_merge_result(
        self,
        step1_results: List[Dict[str, Any]],
        step2_results: List[Dict[str, Any]],
        config: SearchConfig
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3: åˆå¹¶æ­¥éª¤1å’Œæ­¥éª¤2çš„ç»“æžœï¼Œå¹¶åŽ»é‡

        åŽ»é‡è§„åˆ™ï¼šå¦‚æžœ source_id + chunk_id ç›¸åŒï¼Œåªä¿ç•™ SQL æœç´¢çš„ç»“æžœï¼ˆstep1ï¼‰

        Args:
            step1_results: æ­¥éª¤1çš„ç»“æžœï¼ˆSQLæœç´¢ï¼‰
            step2_results: æ­¥éª¤2çš„ç»“æžœï¼ˆEmbeddingæœç´¢ï¼‰
            config: æœç´¢é…ç½®ï¼ˆç”¨äºŽæž„å»ºçº¿ç´¢ï¼‰

        Returns:
            åˆå¹¶å¹¶åŽ»é‡åŽçš„ç»“æžœåˆ—è¡¨ï¼ˆæŒ‰å¾—åˆ†é™åºæŽ’åºï¼‰
        """
        self.logger.info(
            f"æ­¥éª¤3å¼€å§‹: åˆå¹¶ step1({len(step1_results)}ä¸ª) + step2({len(step2_results)}ä¸ª) å¹¶åŽ»é‡"
        )

        # 1. å…ˆè®°å½• SQL æœç´¢ç»“æžœä¸­æ‰€æœ‰çš„ (source_id, chunk_id)
        sql_chunks = set()
        for result in step1_results:
            source_id = result.get('source_id')
            chunk_id = result.get('chunk_id')
            sql_chunks.add((source_id, chunk_id))

        self.logger.debug(f"SQL æœç´¢æ‰¾åˆ° {len(sql_chunks)} ä¸ªå”¯ä¸€åŽŸæ–‡å—")

        # 2. éåŽ† embedding ç»“æžœï¼Œè¿‡æ»¤æŽ‰å·²ç»åœ¨ SQL ç»“æžœä¸­çš„åŽŸæ–‡å—
        filtered_embedding_results = []
        duplicate_count = 0

        for result in step2_results:
            source_id = result.get('source_id')
            chunk_id = result.get('chunk_id')
            chunk_key = (source_id, chunk_id)

            if chunk_key in sql_chunks:
                # è¿™ä¸ªåŽŸæ–‡å—å·²ç»åœ¨ SQL ç»“æžœä¸­ï¼Œè·³è¿‡
                duplicate_count += 1
                self.logger.debug(
                    f"åŽŸæ–‡å— {chunk_id[:8]}... åœ¨ SQL å’Œ Embedding ä¸­éƒ½æ‰¾åˆ°ï¼Œä¿ç•™ SQL ç»“æžœ"
                )
            else:
                # è¿™æ˜¯æ–°åŽŸæ–‡å—ï¼Œä¿ç•™
                filtered_embedding_results.append(result)

        self.logger.info(
            f"åŽ»é‡ç»Ÿè®¡: Embedding ç»“æžœä¸­æœ‰ {duplicate_count} ä¸ªä¸Ž SQL é‡å¤çš„åŽŸæ–‡å—å·²ç§»é™¤"
        )

        # ðŸ†• æ˜¾ç¤º Embedding ä¸­è¿›å…¥ä¸‹ä¸€è½®çš„æ®µè½ï¼ˆè¡¥å……ä½œç”¨ï¼‰
        if filtered_embedding_results:
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€Step2 æ‰©å±•è¡¥å……ã€‘{len(filtered_embedding_results)} ä¸ª Embedding åŽŸæ–‡å—è¿›å…¥ä¸‹ä¸€è½®:")
            self.logger.info("-" * 80)
            for r in filtered_embedding_results[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                chunk_id = r.get('chunk_id', '')
                heading = r.get('heading', '')
                score = r.get('score', 0.0)
                search_type = r.get('search_type', '')
                heading_preview = heading[:40] if heading else "æ— æ ‡é¢˜"
                self.logger.info(
                    f"  {chunk_id[:8]}... | Cosine={score:.4f} | Type={search_type} | {heading_preview}"
                )
            if len(filtered_embedding_results) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(filtered_embedding_results) - 10} ä¸ª)")
            self.logger.info("=" * 80)

            # ðŸ†• æž„å»º query â†’ chunk çº¿ç´¢ï¼ˆStep2 embeddingå¬å›žçš„åŽŸæ–‡å—ï¼‰
            from sag.modules.search.tracker import Tracker
            tracker = Tracker(config)

            query_chunk_clue_count = 0
            for r in filtered_embedding_results:
                chunk_id = r.get('chunk_id', '')
                if not chunk_id:
                    continue

                # æž„å»º query èŠ‚ç‚¹
                query_node = Tracker.build_query_node(config)

                # æž„å»º chunk èŠ‚ç‚¹ï¼ˆä½¿ç”¨section nodeæ–¹æ³•ï¼Œå› ä¸ºæ•°æ®ç»“æž„å…¼å®¹ï¼‰
                chunk_node = Tracker.build_section_node({
                    "chunk_id": chunk_id,
                    "id": chunk_id,
                    "heading": r.get('heading', ''),
                    "content": r.get('content', ''),
                    "summary": "",
                    "section_type": r.get('search_type', '')
                })

                # æ·»åŠ çº¿ç´¢ï¼ˆç½®ä¿¡åº¦ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
                tracker.add_clue(
                    stage="rerank",
                    from_node=query_node,
                    to_node=chunk_node,
                    confidence=r.get('score', 0.0),
                    relation="è¯­ä¹‰å¬å›ž",
                    metadata={
                        "method": "embedding",
                        "search_type": r.get('search_type', ''),
                        "step": "step2"
                    }
                )
                query_chunk_clue_count += 1

            self.logger.info(
                f"ðŸ”— [Step2 çº¿ç´¢æž„å»º] Queryâ†’Chunk={query_chunk_clue_count}æ¡")

        # 3. åˆå¹¶ SQL ç»“æžœå’Œè¿‡æ»¤åŽçš„ embedding ç»“æžœ
        merged_list = step1_results + filtered_embedding_results

        return merged_list

    async def _step4_calculate_weight_of_contents(
        self,
        key_final: List[Dict[str, Any]],
        content_related: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤4: è®¡ç®—[content-related]çš„åˆå§‹æƒé‡å‘é‡

        æ ¹æ®key_finalä¸­çš„å…³é”®å®žä½“ä¿¡æ¯ï¼Œè®¡ç®—æ¯ä¸ªæ®µè½çš„æƒé‡ï¼Œ
        å¹¶å°†è®¡ç®—å‡ºçš„æƒé‡èµ‹å€¼ç»™æ¯ä¸ªæ®µè½çš„ weight å­—æ®µ

        Args:
            key_final: ä»ŽRecallè¿”å›žçš„key_finalæ•°æ®
            content_related: ä»Žstep3åˆå¹¶åŽçš„æ®µè½åˆ—è¡¨

        Returns:
            æ›´æ–°äº†weightå­—æ®µçš„æ®µè½åˆ—è¡¨
        """
        try:
            self.logger.info(f"æ­¥éª¤4å¼€å§‹: è®¡ç®— {len(content_related)} ä¸ªæ®µè½çš„æƒé‡")

            # ç¬¬ä¸€å±‚å¾ªçŽ¯ï¼šéåŽ†æ‰€æœ‰æ®µè½
            for content in content_related:
                search_type = content.get("search_type")  # ä½¿ç”¨search_type
                content_text = content.get("content", "")
                heading = content.get("heading", "")
                full_text = f"{heading} {content_text}"  # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ç”¨äºŽæœç´¢key

                # 1. èŽ·å–æ®µè½ä¸Žqueryçš„ç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆæ¥è‡ªstep1æˆ–step2çš„scoreï¼‰
                similarity_score = content.get("score", 0.0)

                # 2. åˆå§‹åŒ–keyæƒé‡ç´¯åŠ å’Œ
                key_weight_sum = 0.0

                # ç¬¬äºŒå±‚å¾ªçŽ¯ï¼šéåŽ†æ‰€æœ‰å…³é”®å®žä½“
                for key in key_final:
                    key_name = key.get("name", "")
                    key_weight = key.get("weight", 0.0)
                    key_steps = key.get("steps", [1])  # ä¾‹å¦‚ [1] æˆ– [2]

                    # è®¡ç®—stepå€¼ï¼ˆå–ç¬¬ä¸€ä¸ªstepå€¼ï¼‰
                    step = key_steps[0] if key_steps else 1

                    # ç»Ÿè®¡keyåœ¨æ®µè½ä¸­å‡ºçŽ°çš„æ¬¡æ•°
                    count = full_text.count(key_name)

                    if count > 0:
                        # è®¡ç®—è¯¥keyçš„è´¡çŒ®ï¼škey_weight * ln(1 + count) / step
                        key_contribution = key_weight * \
                            math.log(1 + count) / step
                        key_weight_sum += key_contribution

                        self.logger.debug(
                            f"æ®µè½ {search_type} åŒ…å«key '{key_name}': "
                            f"count={count}, weight={key_weight:.3f}, step={step}, "
                            f"contribution={key_contribution:.4f}"
                        )

                # 3. è®¡ç®—æœ€ç»ˆæƒé‡ = 0.5 * similarity_score + ln(1 + key_weight_sum)
                total_weight = 0.5 * similarity_score + \
                    math.log(1 + key_weight_sum)

                # 4. å°†æƒé‡èµ‹å€¼ç»™è¯¥æ®µè½çš„ weight å­—æ®µ
                content["weight"] = total_weight

                self.logger.info(
                    f"æ®µè½ {search_type} æƒé‡è®¡ç®—: "
                    f"similarity={similarity_score:.4f}, key_sum={key_weight_sum:.4f}, "
                    f"total={total_weight:.4f}"
                )

            self.logger.info(f"æ­¥éª¤4å®Œæˆ: è®¡ç®—äº† {len(content_related)} ä¸ªæ®µè½çš„æƒé‡")

            return content_related

        except Exception as e:
            self.logger.error(f"æ­¥éª¤4æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step5_pageRank_of_contents(
        self,
        content_related: List[Dict[str, Any]],
        key_final: List[Dict[str, Any]] = None,
        damping: float = 0.85,
        iterations: int = 100,
        tolerance: float = 1e-6
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤5: PageRanké‡æŽ’åº

        æž„å»ºæ®µè½å…³ç³»å›¾å¹¶ä½¿ç”¨PageRankç®—æ³•è¿›è¡ŒæŽ’åº
        - åˆå§‹æƒé‡ï¼šä½¿ç”¨step4çš„weightä½œä¸ºåˆå§‹PageRankå€¼
        - ä¸¤ç§å…³è”å…³ç³»ï¼š
          1. æ®µè½å…³è”ï¼ˆæƒé‡0.5ï¼‰ï¼šåŒä¸€æ–‡ç« å†…ç›¸é‚»çš„æ®µè½ä¹‹é—´æœ‰è¾¹
          2. å®žä½“å…³è”ï¼ˆæƒé‡0.5ï¼‰ï¼šåŒ…å«ç›¸åŒkey_finalå®žä½“çš„æ®µè½ä¹‹é—´æœ‰è¾¹

        Args:
            content_related: ä»Žstep4è®¡ç®—å®Œæƒé‡çš„æ®µè½åˆ—è¡¨
            key_final: ä»ŽRecallè¿”å›žçš„å…³é”®å®žä½“åˆ—è¡¨ï¼ˆç”¨äºŽå®žä½“å…³è”ï¼‰
            damping: PageRanké˜»å°¼ç³»æ•°ï¼Œé»˜è®¤0.85
            iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤30
            tolerance: æ”¶æ•›é˜ˆå€¼ï¼Œé»˜è®¤1e-6

        Returns:
            æŒ‰PageRankå€¼æŽ’åºåŽçš„æ®µè½åˆ—è¡¨
        """
        try:
            n = len(content_related)
            self.logger.info(f"æ­¥éª¤5å¼€å§‹: å¯¹ {n} ä¸ªæ®µè½ä½¿ç”¨PageRankç®—æ³•è¿›è¡ŒæŽ’åº")

            if n == 0:
                return []

            # ===== DEBUG: è®°å½•ç›´æŽ¥æŒ‰æƒé‡æŽ’åºçš„ç»“æžœ =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å¯¹æ¯”ã€‘ç›´æŽ¥æŒ‰æƒé‡æŽ’åºçš„ç»“æžœï¼ˆTop 10ï¼‰ï¼š")
            self.logger.debug("-" * 80)

            weight_sorted = sorted(
                enumerate(content_related),
                key=lambda x: x[1].get('weight', 0.0),
                reverse=True
            )

            for rank, (idx, content) in enumerate(weight_sorted[:10], 1):
                search_type = content.get('search_type', 'N/A')
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]
                chunk_id = content.get('chunk_id', '')[:8]
                event_count = len(content.get('event_ids', []))

                self.logger.debug(
                    f"Rank {rank:2d} [idx={idx:3d}]: {search_type:12s} | "
                    f"weight={weight:.4f}, score={score:.4f} | "
                    f"events={event_count} | chunk={chunk_id}... | "
                    f"{heading}"
                )
            self.logger.debug("=" * 80)

            # 1. åˆå§‹åŒ–PageRankå€¼ï¼ˆä½¿ç”¨step4çš„weightå½’ä¸€åŒ–åŽä½œä¸ºåˆå§‹å€¼ï¼‰
            weights = np.array([c.get('weight', 0.0) for c in content_related])
            if weights.sum() > 0:
                pagerank = weights / weights.sum()
                self.logger.info(f"ä½¿ç”¨step4çš„æƒé‡ä½œä¸ºåˆå§‹PageRankå€¼ï¼ˆå·²å½’ä¸€åŒ–ï¼‰")
            else:
                pagerank = np.ones(n) / n
                self.logger.warning(f"æ‰€æœ‰æƒé‡ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåˆå§‹PageRankå€¼")

            # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºç´¢å¼•æ˜ å°„
            chunk_to_idx = {c['chunk_id']: i for i,
                              c in enumerate(content_related)}

            # 2. æž„å»ºå…³ç³»å›¾ï¼ˆä½¿ç”¨å­—å…¸å­˜å‚¨è¾¹å’Œæƒé‡ï¼‰
            # graph[i] = [(j, weight), ...] è¡¨ç¤ºä»ŽèŠ‚ç‚¹iæŒ‡å‘èŠ‚ç‚¹jçš„è¾¹åŠå…¶æƒé‡
            graph = defaultdict(list)

            # 2.1 å®žä½“å…³è” - ç›´æŽ¥ä½¿ç”¨å®žä½“æƒé‡æž„å»ºæ®µè½é—´å…³ç³»
            entity_edges_count = 0
            if key_final:
                self.logger.info("æž„å»ºå®žä½“å…³è”è¾¹ï¼ˆä½¿ç”¨å®žä½“æƒé‡ï¼‰...")
                # ä¸ºæ¯ä¸ªæ®µè½æ‰¾åˆ°å®ƒåŒ…å«çš„å®žä½“åŠå…¶æƒé‡
                section_entities = defaultdict(list)  # section_idx -> [(entity_name, entity_weight), ...]
                for i, content in enumerate(content_related):
                    full_text = f"{content.get('heading', '')} {content.get('content', '')}"
                    for key in key_final:
                        key_name = key.get('name', '')
                        key_weight = key.get('weight', 0.0)
                        if key_name and key_name in full_text:
                            section_entities[i].append((key_name, key_weight))

                # åŸºäºŽkey-finalæž„å»ºæ®µè½é—´å…³ç³»ï¼šæ–¹å‘æ€§æƒé‡ä½“çŽ°é‡è¦æ€§å·®å¼‚
                for i in range(n):
                    for j in range(i + 1, n):
                        # èŽ·å–ä¸¤ä¸ªæ®µè½å…±äº«çš„å®žä½“
                        entities_i = {entity_name for entity_name, _ in section_entities[i]}
                        entities_j = {entity_name for entity_name, _ in section_entities[j]}
                        common_entities = entities_i & entities_j

                        if common_entities:
                            # è®¡ç®—æ–¹å‘æ€§æƒé‡
                            weight_i_to_j = 0.0  # æ®µè½iâ†’æ®µè½jçš„æƒé‡
                            weight_j_to_i = 0.0  # æ®µè½jâ†’æ®µè½içš„æƒé‡

                            # èŽ·å–ä¸¤ä¸ªæ®µè½çš„å†…å®¹
                            content_i = f"{content_related[i].get('heading', '')} {content_related[i].get('content', '')}"
                            content_j = f"{content_related[j].get('heading', '')} {content_related[j].get('content', '')}"

                            for entity_name in common_entities:
                                # æ‰¾åˆ°å®žä½“åœ¨key_finalä¸­çš„æƒé‡ï¼ˆä»»æ„ä¸€ä¸ªæ®µè½çš„æƒé‡å³å¯ï¼‰
                                key_weight = next(w for name, w in section_entities[i] if name == entity_name)

                                # è®¡ç®—keyåœ¨ä¸¤ä¸ªæ®µè½ä¸­çš„å‡ºçŽ°æ¬¡æ•°
                                count_i = content_i.count(entity_name)
                                count_j = content_j.count(entity_name)

                                # æ–¹å‘æ€§æƒé‡ï¼šæŠ•ç¥¨æƒé‡åŸºäºŽç›®æ ‡æ®µè½çš„é‡è¦æ€§
                                weight_i_to_j += key_weight * count_j  # iâ†’jçš„æƒé‡åŸºäºŽjä¸­çš„å‡ºçŽ°æ¬¡æ•°
                                weight_j_to_i += key_weight * count_i  # jâ†’içš„æƒé‡åŸºäºŽiä¸­çš„å‡ºçŽ°æ¬¡æ•°

                            # å»ºç«‹æ–¹å‘æ€§è¾¹
                            if weight_i_to_j > 0:
                                graph[i].append((j, weight_i_to_j))
                            if weight_j_to_i > 0:
                                graph[j].append((i, weight_j_to_i))

                            # è®°å½•è¾¹çš„æ•°é‡ï¼ˆåªè®¡ç®—å®žé™…å»ºç«‹çš„è¾¹ï¼‰
                            if weight_i_to_j > 0 or weight_j_to_i > 0:
                                entity_edges_count += 1

                self.logger.info(f"å®žä½“å…³è”: æ·»åŠ äº† {entity_edges_count} æ¡æœ‰å‘è¾¹ï¼ˆåŸºäºŽkey-finalå’Œå‡ºçŽ°æ¬¡æ•°ï¼‰")
            else:
                self.logger.warning("æœªæä¾›key_finalï¼Œè·³è¿‡å®žä½“å…³è”è¾¹çš„æž„å»º")

            total_edges = entity_edges_count
            self.logger.info(f"å…³ç³»å›¾æž„å»ºå®Œæˆ: èŠ‚ç‚¹æ•°={n}, æ€»è¾¹æ•°={total_edges} (ä»…å®žä½“å…³è”)")

            # ===== DEBUG: å±•ç¤ºå›¾ç»“æž„ç»Ÿè®¡ä¿¡æ¯ =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å›¾ç»“æž„ç»Ÿè®¡ã€‘ï¼š")

            # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„å‡ºåº¦å’Œå…¥åº¦
            out_degrees = {i: len(graph.get(i, [])) for i in range(n)}
            in_degrees = defaultdict(int)
            for i in range(n):
                for j, _ in graph.get(i, []):
                    in_degrees[j] += 1

            # ç»Ÿè®¡å‡ºåº¦åˆ†å¸ƒ
            out_degree_values = list(out_degrees.values())
            in_degree_values = [in_degrees.get(i, 0) for i in range(n)]

            self.logger.debug(
                f"å‡ºåº¦ç»Ÿè®¡: å¹³å‡={np.mean(out_degree_values):.2f}, "
                f"æœ€å¤§={max(out_degree_values)}, "
                f"æœ€å°={min(out_degree_values)}, "
                f"å­¤ç«‹èŠ‚ç‚¹={sum(1 for d in out_degree_values if d == 0)}"
            )
            self.logger.debug(
                f"å…¥åº¦ç»Ÿè®¡: å¹³å‡={np.mean(in_degree_values):.2f}, "
                f"æœ€å¤§={max(in_degree_values)}, "
                f"æœ€å°={min(in_degree_values)}"
            )

            # å±•ç¤ºåº¦æ•°æœ€é«˜çš„å‰5ä¸ªèŠ‚ç‚¹ï¼ˆåŸºäºŽå®žä½“å…³è”ï¼‰
            top_out_degree = sorted(
                out_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
            self.logger.debug("å‡ºåº¦æœ€é«˜çš„5ä¸ªèŠ‚ç‚¹ï¼ˆåŸºäºŽå®žä½“å…³è”ï¼‰ï¼š")
            for idx, degree in top_out_degree:
                content = content_related[idx]
                chunk_id = content.get('chunk_id', '')[:8]
                heading = content.get('heading', '')[:30]
                self.logger.debug(
                    f"  èŠ‚ç‚¹{idx:3d} (chunk={chunk_id}...): å‡ºåº¦={degree}, æ ‡é¢˜={heading}")

            self.logger.debug("=" * 80)

            # 3. é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ€»å‡ºæƒé‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
            out_weights = {}
            for j in range(n):
                edges = graph.get(j, [])
                out_weights[j] = sum(w for _, w in edges) if edges else 0.0

            nodes_with_edges = sum(1 for w in out_weights.values() if w > 0)
            self.logger.debug(
                f"é¢„è®¡ç®—å®Œæˆ: {nodes_with_edges}/{n} ä¸ªèŠ‚ç‚¹æœ‰å‡ºè¾¹")

            # 4. PageRankè¿­ä»£è®¡ç®—ï¼ˆä¼˜åŒ–ç‰ˆï¼šåå‘éåŽ†å›¾ï¼Œé¿å…O(nÂ²)å¤æ‚åº¦ï¼‰
            self.logger.info(
                f"å¼€å§‹PageRankè¿­ä»£ï¼ˆé˜»å°¼ç³»æ•°={damping}, æœ€å¤§è¿­ä»£={iterations}ï¼‰...")

            for iteration in range(iterations):
                # åˆå§‹åŒ–ä¸ºåŸºç¡€å€¼ (1-d)/n
                new_pagerank = np.ones(n) * (1 - damping) / n

                # éåŽ†æ‰€æœ‰æºèŠ‚ç‚¹jï¼ˆè€Œä¸æ˜¯éåŽ†ç›®æ ‡èŠ‚ç‚¹iï¼‰
                for j in range(n):
                    # è·³è¿‡æ²¡æœ‰PageRankå€¼æˆ–æ²¡æœ‰å‡ºè¾¹çš„èŠ‚ç‚¹
                    if pagerank[j] == 0 or out_weights[j] == 0:
                        continue

                    # è®¡ç®—jå¯¹æ¯æ¡å‡ºè¾¹çš„å•ä½æƒé‡è´¡çŒ®
                    # è´¡çŒ® = d Ã— PR(j) Ã— edge_weight / total_out_weight
                    contribution_per_weight = damping * pagerank[j] / out_weights[j]

                    # éåŽ†jçš„æ‰€æœ‰å‡ºè¾¹ï¼Œå°†è´¡çŒ®åˆ†é…ç»™ç›®æ ‡èŠ‚ç‚¹
                    for target, edge_weight in graph.get(j, []):
                        new_pagerank[target] += contribution_per_weight * edge_weight

                # æ£€æŸ¥æ”¶æ•›
                diff = np.abs(new_pagerank - pagerank).sum()
                if diff < tolerance:
                    self.logger.info(
                        f"PageRankåœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£åŽæ”¶æ•›ï¼ˆå·®å¼‚={diff:.8f}ï¼‰")
                    pagerank = new_pagerank
                    break

                pagerank = new_pagerank

                # æ¯10æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡æ—¥å¿—
                if (iteration + 1) % 10 == 0:
                    self.logger.debug(
                        f"è¿­ä»£ {iteration+1}/{iterations}, å·®å¼‚={diff:.8f}")

            else:
                self.logger.warning(f"PageRankè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{iterations}ï¼Œæœªå®Œå…¨æ”¶æ•›")

            # 4. å°†PageRankå€¼èµ‹å€¼ç»™æ¯ä¸ªæ®µè½
            for i, content in enumerate(content_related):
                content['pagerank'] = float(pagerank[i])

            # 5. æŒ‰PageRankå€¼æŽ’åºï¼ˆä»Žå¤§åˆ°å°ï¼‰
            sorted_contents = sorted(
                content_related,
                key=lambda x: x.get('pagerank', 0.0),
                reverse=True
            )

            # ===== DEBUG: è®°å½•PageRankæŽ’åºç»“æžœå¹¶å¯¹æ¯”æƒé‡æŽ’åº =====
            self.logger.debug("=" * 80)
            self.logger.debug("ã€å¯¹æ¯”ã€‘PageRankæŽ’åºçš„ç»“æžœï¼ˆTop 10ï¼‰ï¼š")
            self.logger.debug("-" * 80)

            # åˆ›å»º chunk_id åˆ°åŽŸå§‹ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºŽå¯¹æ¯”æŽ’åå˜åŒ–ï¼‰
            chunk_to_original_idx = {
                c['chunk_id']: i for i, c in enumerate(content_related)}

            # åˆ›å»ºæƒé‡æŽ’åæ˜ å°„
            weight_rank_map = {
                content[1]['chunk_id']: rank for rank, content in enumerate(weight_sorted, 1)}

            for rank, content in enumerate(sorted_contents[:10], 1):
                search_type = content.get('search_type', 'N/A')
                pagerank_val = content.get('pagerank', 0.0)
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]
                chunk_id = content.get('chunk_id', '')
                event_count = len(content.get('event_ids', []))

                # èŽ·å–åœ¨æƒé‡æŽ’åºä¸­çš„æŽ’å
                weight_rank = weight_rank_map.get(chunk_id, -1)
                rank_change = weight_rank - rank if weight_rank > 0 else 0

                # æŽ’åå˜åŒ–æ ‡è®°
                if rank_change > 0:
                    change_mark = f"â†‘{rank_change:+d}"  # ä¸Šå‡
                elif rank_change < 0:
                    change_mark = f"â†“{rank_change:+d}"  # ä¸‹é™
                else:
                    change_mark = " â”  "  # ä¸å˜

                original_idx = chunk_to_original_idx.get(chunk_id, -1)

                self.logger.debug(
                    f"Rank {rank:2d} [idx={original_idx:3d}] {change_mark:>5s} (was #{weight_rank:2d}): {search_type:12s} | "
                    f"PR={pagerank_val:.6f}, weight={weight:.4f}, score={score:.4f} | "
                    f"events={event_count} | chunk={chunk_id[:8]}... | "
                    f"{heading}"
                )

            self.logger.debug("=" * 80)
            self.logger.debug("ã€æŽ’åºå˜åŒ–ç»Ÿè®¡ã€‘ï¼š")

            # ç»Ÿè®¡æŽ’åå˜åŒ–
            rank_changes = []
            for rank, content in enumerate(sorted_contents, 1):
                chunk_id = content['chunk_id']
                weight_rank = weight_rank_map.get(chunk_id, -1)
                if weight_rank > 0:
                    change = weight_rank - rank
                    rank_changes.append(abs(change))

            if rank_changes:
                avg_change = np.mean(rank_changes)
                max_change = max(rank_changes)
                unchanged_count = sum(1 for c in rank_changes if c == 0)
                self.logger.debug(
                    f"å¹³å‡æŽ’åå˜åŒ–: {avg_change:.2f} ä½ | "
                    f"æœ€å¤§æŽ’åå˜åŒ–: {max_change} ä½ | "
                    f"æŽ’åä¸å˜: {unchanged_count}/{len(rank_changes)} ä¸ª"
                )

            self.logger.debug("=" * 80)

            # è®°å½•æŽ’åºåŽçš„å‰å‡ ä¸ªç»“æžœï¼ˆINFOçº§åˆ«ï¼‰
            self.logger.info("=" * 80)
            self.logger.info("æ­¥éª¤5æŽ’åºç»“æžœï¼ˆTop 5 by PageRankï¼‰ï¼š")
            self.logger.info("-" * 80)

            for i, content in enumerate(sorted_contents[:5], 1):
                search_type = content.get('search_type', 'N/A')
                pagerank_val = content.get('pagerank', 0.0)
                weight = content.get('weight', 0.0)
                score = content.get('score', 0.0)
                heading = content.get('heading', '')[:40]

                self.logger.info(
                    f"Rank {i}: {search_type} | "
                    f"PageRank={pagerank_val:.6f}, weight={weight:.4f}, score={score:.4f} | "
                    f"æ ‡é¢˜: {heading}"
                )

            self.logger.info("=" * 80)
            self.logger.info(
                f"æ­¥éª¤5å®Œæˆ: æŽ’åºäº† {len(sorted_contents)} ä¸ªæ®µè½ "
                f"(å¹³å‡PageRank={pagerank.mean():.6f})"
            )

            return sorted_contents

        except Exception as e:
            self.logger.error(f"æ­¥éª¤5æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step6_get_topn_sections(
        self,
        sorted_contents: List[Dict[str, Any]],
        top_k: int,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤6: å–Top-Næ®µè½å¹¶è¿”å›ž

        å¤„ç†æµç¨‹ï¼š
        1. å–Top-kï¼šä»ŽæŽ’åºåŽçš„ç»“æžœä¸­å–å‰ k ä¸ªæ®µè½
        2. ç›´æŽ¥è¿”å›žè¿™äº›æ®µè½ï¼Œä¸å†è¿›è¡Œæ®µè½â†’äº‹é¡¹çš„è½¬æ¢

        Args:
            sorted_contents: ä»Žstep5æŽ’åºåŽçš„æ®µè½åˆ—è¡¨ï¼ˆå·²æŒ‰PageRanké™åºæŽ’åºï¼‰
            top_k: å–å‰kä¸ªç»“æžœ
            config: æœç´¢é…ç½®

        Returns:
            List[Dict[str, Any]]: æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªæ®µè½åŒ…å«ï¼š
                - chunk_id: åŽŸæ–‡å—ID
                - heading: æ®µè½æ ‡é¢˜
                - content: æ®µè½å†…å®¹
                - pagerank: PageRankå¾—åˆ†
                - weight: æƒé‡å¾—åˆ†
                - clues: çº¿ç´¢åˆ—è¡¨ï¼ˆå¬å›žè¯¥æ®µè½çš„å®žä½“ï¼‰
        """
        try:
            self.logger.info(
                f"[æ®µè½çº§Step6] å¼€å§‹: ä»Ž {len(sorted_contents)} ä¸ªæ®µè½ä¸­å–Top-{top_k}")

            # 1. å–Top-kæ®µè½
            topk_sections = sorted_contents[:top_k]
            self.logger.info(f"âœ“ [æ®µè½çº§Step6] æå–äº†Top-{len(topk_sections)}ä¸ªæ®µè½")

            # 2. æ˜¾ç¤ºTop-10æ®µè½ä¿¡æ¯
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€æ®µè½çº§Step6ã€‘Top-{min(len(topk_sections), 10)}æ®µè½è¯¦æƒ…:")
            self.logger.info("-" * 80)

            for idx, section in enumerate(topk_sections[:10], 1):
                heading = section.get('heading', '')[:50]
                pagerank = section.get('pagerank', 0.0)
                weight = section.get('weight', 0.0)
                chunk_id = section.get('chunk_id', '')[:8]

                self.logger.info(
                    f"  æ®µè½{idx}: {chunk_id}... | PR={pagerank:.4f}, W={weight:.3f} | '{heading}'"
                )

            if len(topk_sections) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(topk_sections) - 10} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("=" * 80)
            self.logger.info(f"âœ“ [æ®µè½çº§Step6] å®Œæˆ: è¿”å›ž {len(topk_sections)} ä¸ªæ®µè½")

            return topk_sections

        except Exception as e:
            self.logger.error(f"[æ®µè½çº§Step6] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []  # å¤±è´¥æ—¶è¿”å›žç©ºåˆ—è¡¨

    def _build_response(
        self,
        config: SearchConfig,
        key_final: List[Dict[str, Any]],
        events: List[SourceEvent],
        event_to_clues: Dict[str, List[Dict]]
    ) -> Dict[str, Any]:
        """
        æž„å»ºæ–°çš„å“åº”æ ¼å¼

        Args:
            config: æœç´¢é…ç½®å¯¹è±¡
            key_final: å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey-finalï¼‰
            events: äº‹é¡¹åˆ—è¡¨
            event_to_clues: äº‹é¡¹IDåˆ°å®žä½“åˆ—è¡¨çš„æ˜ å°„ {event_id: [entity1, entity2, ...]}

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events: äº‹é¡¹å¯¹è±¡åˆ—è¡¨
                - clues: å¬å›žçº¿ç´¢ä¿¡æ¯
                    - origin_query: åŽŸå§‹æŸ¥è¯¢
                    - final_query: LLMé‡å†™åŽçš„æŸ¥è¯¢ï¼ˆå¦‚æžœæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                    - query_entities: æŸ¥è¯¢å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities: å¬å›žçš„å®žä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼ŒåŽ»é™¤query_entitiesä¸­çš„å€¼ï¼‰
                    - event_entities: äº‹é¡¹ä¸Žå®žä½“çš„å…³è”æ˜ å°„è¡¨ {event_id: [entity1, entity2, ...]}
        """
        # 1. å¤„ç† query_entitiesï¼šå°† config.query_recalled_keys ä¸­çš„ key_id æ”¹ä¸º id
        query_entities = []
        query_key_ids = set()  # ç”¨äºŽåŽç»­è¿‡æ»¤

        for key in config.query_recalled_keys:
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_id = key_copy.pop("key_id")
                key_copy["id"] = key_id
                query_key_ids.add(key_id)
            query_entities.append(key_copy)

        # 2. å¤„ç† recall_entitiesï¼šå°† key_final ä¸­çš„ key_id æ”¹ä¸º idï¼Œå¹¶è¿‡æ»¤æŽ‰ query_entities ä¸­çš„å€¼
        recall_entities = []

        for key in key_final:
            # èŽ·å– key_id ç”¨äºŽè¿‡æ»¤åˆ¤æ–­
            key_id = key.get("key_id")

            # å¦‚æžœè¿™ä¸ª key_id åœ¨ query_recalled_keys ä¸­ï¼Œåˆ™è·³è¿‡
            if key_id in query_key_ids:
                continue

            # å¤åˆ¶å¹¶é‡å‘½å key_id ä¸º id
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_copy["id"] = key_copy.pop("key_id")
            recall_entities.append(key_copy)

        # 3. åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿”å›ž final_query
        # å¦‚æžœå¯ç”¨äº†queryé‡å†™åŠŸèƒ½ï¼ˆenable_query_rewrite=Trueï¼‰ï¼Œåˆ™è¿”å›žé‡å†™åŽçš„query
        # å¦åˆ™è¿”å›ž None
        final_query = config.query if config.enable_query_rewrite and config.recall.use_fast_mode == False else None

        # 4. è¿‡æ»¤ event_to_cluesï¼Œåªä¿ç•™æœ€ç»ˆè¿”å›žçš„äº‹é¡¹
        final_event_ids = {event.id for event in events}
        filtered_event_entities = {
            event_id: clues
            for event_id, clues in event_to_clues.items()
            if event_id in final_event_ids
        }

        # 5. æž„å»ºå“åº”
        response = {
            "events": events,  # äº‹é¡¹åˆ—è¡¨
            "clues": {
                "origin_query": config.original_query,  # åŽŸå§‹queryï¼ˆé‡å†™å‰ï¼‰
                "final_query": final_query,  # é‡å†™åŽçš„queryï¼ˆæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                "query_entities": query_entities,
                "recall_entities": recall_entities,
                "event_entities": filtered_event_entities  # åªåŒ…å«æœ€ç»ˆè¿”å›žäº‹é¡¹çš„æº¯æºä¿¡æ¯
            }
        }

        self.logger.info(
            f"å“åº”æž„å»ºå®Œæˆ: origin_query='{config.original_query}', "
            f"final_query='{final_query}', "
            f"query_entities={len(query_entities)}ä¸ª, "
            f"recall_entities={len(recall_entities)}ä¸ª, "
            f"events={len(events)}ä¸ª, "
            f"event_entitiesæ˜ å°„={len(filtered_event_entities)}ä¸ª (è¿‡æ»¤å‰={len(event_to_clues)}ä¸ª)"
        )

        return response

