"""
æœç´¢ Rerank æ¨¡å— - äº‹é¡¹çº§ PageRank å®ç°

å®ç°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦äº‹é¡¹çš„æ–¹æ³•ï¼š
1. keyæ‰¾eventï¼šæ ¹æ®[key-final]ä»SQLä¸­æå–ç›¸å…³äº‹é¡¹ï¼Œè®¡ç®—äº‹é¡¹å‘é‡ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†
2. queryæ‰¾eventï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼ˆKNN+ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°ç›¸ä¼¼äº‹é¡¹
3. åˆå¹¶eventå»é‡ï¼šä¼˜å…ˆä¿ç•™step1ç»“æœï¼ˆå®ä½“å¬å›çš„äº‹é¡¹ï¼‰
4. è®¡ç®—äº‹é¡¹æƒé‡å‘é‡ï¼šä½¿ç”¨å…¬å¼ weight = 0.5*ç›¸ä¼¼åº¦ + ln(1 + Î£(keyæƒé‡ Ã— ln(1+å‡ºç°æ¬¡æ•°) / step))
5. æ„å»ºäº‹é¡¹å…³ç³»å›¾ + PageRankæ’åºï¼š
   - å®ä½“å…³è”ï¼šæ–¹å‘æ€§æƒé‡ï¼Œäº‹é¡¹iâ†’äº‹é¡¹jçš„æƒé‡ = keyæƒé‡ Ã— keyåœ¨äº‹é¡¹jçš„å‡ºç°æ¬¡æ•°
   - ç±»åˆ«å…³è”ï¼šæ–¹å‘æ€§æƒé‡ï¼ŒåŸºäºç›®æ ‡äº‹é¡¹çš„å†…å®¹ä¸°å¯Œåº¦
6. é€‰æ‹©Top-Näº‹é¡¹ï¼šä¿ç•™æº¯æºä¿¡æ¯ï¼ŒæŒ‰PageRankå¾—åˆ†æ’åº

è¿”å›æ ¼å¼ï¼š
Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
    - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰ PageRank é¡ºåºæ’åˆ—ï¼‰
    - clues (Dict): å¬å›çº¿ç´¢ä¿¡æ¯
        - origin_query (str): åŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
        - final_query (str): LLMé‡å†™åçš„æŸ¥è¯¢ï¼ˆé‡å†™åï¼‰
        - query_entities (List[Dict]): æŸ¥è¯¢å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
        - recall_entities (List[Dict]): å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æ‰query_entitiesä¸­çš„å€¼ï¼‰

å…³é”®ç‰¹æ€§ï¼š
- æ–¹å‘æ€§PageRankï¼šèƒ½åŒºåˆ†äº‹é¡¹é‡è¦æ€§ï¼Œé‡è¦äº‹é¡¹æŠ•ç¥¨æƒé‡æ›´å¤§
- å†…å®¹æ„ŸçŸ¥ï¼šæƒé‡åŸºäºkeyåœ¨äº‹é¡¹å†…å®¹ä¸­çš„å®é™…å‡ºç°é¢‘æ¬¡
- å¤šç»´åº¦å…³è”ï¼šå®ä½“å…³è” + ç±»åˆ«å…³è”ï¼Œæ„å»ºå®Œæ•´çš„å…³ç³»å›¾

"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math
import re
import time
import asyncio
from collections import Counter, defaultdict


from sqlalchemy import select, and_, or_
from sqlalchemy.orm import selectinload, joinedload

from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import SourceEvent, Entity, EventEntity, ArticleSection, Article, SourceConfig, get_session_factory
from sag.exceptions import AIError
from sag.modules.load.processor import DocumentProcessor
from sag.modules.search.config import SearchConfig
from sag.modules.search.tracker import Tracker  # ğŸ†• æ·»åŠ çº¿ç´¢è¿½è¸ªå™¨
from sag.utils import get_logger

logger = get_logger("search.rerank.pagerank")


@dataclass
class ContentSearchResult:
    """
    æœç´¢ç»“æœçš„ç»Ÿä¸€è¿”å›æ ¼å¼

    ç”¨äºè¡¨ç¤ºä»SQLæ•°æ®åº“æˆ–Embeddingå‘é‡æ•°æ®åº“æœç´¢åˆ°çš„å†…å®¹
    """
    # å¿…éœ€å­—æ®µ
    search_type: str      # "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
    source_config_id: str        # æ•°æ®æºID (UUID)
    article_id: str       # æ–‡ç« ID (UUID)
    section_id: str       # æ®µè½ID (UUID)
    rank: int             # æ®µè½åœ¨æ–‡ç« ä¸­çš„æ’åº
    heading: str          # æ®µè½æ ‡é¢˜
    content: str          # æ®µè½å†…å®¹
    score: float = 0.0    # ç›¸å…³æ€§å¾—åˆ†
    weight: float = 0.0   # æƒé‡å€¼ï¼ˆstep4è®¡ç®—åèµ‹å€¼ï¼‰
    event_ids: List[str] = None  # å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
    event: str = ""  # èšåˆåçš„äº‹é¡¹æ‘˜è¦ï¼ˆå¤šä¸ªsummaryåˆå¹¶ï¼‰
    clues: List[Dict[str, Any]] = None  # å¬å›è¯¥æ®µè½çš„çº¿ç´¢åˆ—è¡¨ï¼ˆæ¥è‡ª key_final æˆ– queryï¼‰

    def __post_init__(self):
        """åˆå§‹åŒ–åéªŒè¯"""
        # åˆå§‹åŒ– event_ids ä¸ºç©ºåˆ—è¡¨
        if self.event_ids is None:
            self.event_ids = []

        # åˆå§‹åŒ– clues ä¸ºç©ºåˆ—è¡¨
        if self.clues is None:
            self.clues = []

        # å…è®¸ "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
        valid_types = ["sql", "embedding"]
        is_valid = (
            self.search_type in valid_types or
            self.search_type.startswith("SQL-") or
            self.search_type.startswith("embedding-")
        )

        if not is_valid:
            raise ValueError(
                f"search_type å¿…é¡»æ˜¯ 'sql', 'embedding' æˆ–å¸¦ç¼–å·æ ¼å¼(å¦‚ 'SQL-1', 'embedding-1')ï¼Œ"
                f"å½“å‰å€¼: {self.search_type}"
            )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "search_type": self.search_type,
            "source_config_id": self.source_config_id,
            "article_id": self.article_id,
            "section_id": self.section_id,
            "rank": self.rank,
            "heading": self.heading,
            "content": self.content,
            "score": self.score,
            "weight": self.weight,
            "event_ids": self.event_ids,
            "event": self.event,
            "clues": self.clues,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ContentSearchResult":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            search_type=data.get("search_type", "sql"),
            source_config_id=data["source_config_id"],
            article_id=data["article_id"],
            section_id=data["section_id"],
            rank=data.get("rank", 0),
            heading=data.get("heading", ""),
            content=data.get("content", ""),
            score=data.get("score", 0.0),
            weight=data.get("weight", 0.0),
            event_ids=data.get("event_ids", []),
            event=data.get("event", ""),
            clues=data.get("clues", []),
        )

    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"ContentSearchResult(type={self.search_type}, "
            f"section_id={self.section_id}, "
            f"heading='{self.heading[:30]}...', "
            f"score={self.score:.3f})"
        )


class RerankPageRankSearcher:
    """Rerankæ®µè½æœç´¢å™¨ - å®ç°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦æ®µè½çš„æ–¹æ³•"""

    def __init__(
        self,
        llm_client=None
    ):
        """
        åˆå§‹åŒ–Rerankæ®µè½æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """

        self.session_factory = get_session_factory()
        self.logger = get_logger("search.rerank.pagerank")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.content_repo = SourceChunkRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)  # æ·»åŠ äº‹é¡¹å‘é‡ä»“åº“

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºç”Ÿæˆembedingå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "Rerankæ®µè½æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    async def search(
        self,
        key_final: List[Dict[str, Any]],
        config: SearchConfig
    ) -> Dict[str, Any]:
        """
        Rerank æœç´¢ä¸»æ–¹æ³•ï¼ˆäº‹é¡¹çº§ PageRankï¼‰

        æ•´åˆæ­¥éª¤1-6ï¼Œç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼Œé¿å…é‡å¤è®¡ç®—

        æ­¥éª¤æµç¨‹ï¼š
          1. keyæ‰¾eventï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰ï¼šåŸºäºå®ä½“å…³è”æ‰¾åˆ°ç›¸å…³äº‹é¡¹
          2. queryæ‰¾eventï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾åˆ°ç›¸å…³äº‹é¡¹
          3. åˆå¹¶eventå»é‡ï¼ˆä¼˜å…ˆä¿ç•™step1ç»“æœï¼‰ï¼šå®ä½“å¬å›çš„äº‹é¡¹ä¼˜å…ˆçº§æ›´é«˜
          4. è®¡ç®—äº‹é¡¹æƒé‡å‘é‡ï¼šç»“åˆç›¸ä¼¼åº¦å’Œå®ä½“æƒé‡è®¡ç®—åˆå§‹æƒé‡
          5. æ„å»ºäº‹é¡¹å…³ç³»å›¾ + æ–¹å‘æ€§PageRankæ’åºï¼š
             - å®ä½“å…³è”ï¼šæ–¹å‘æ€§æƒé‡ï¼Œkeyæƒé‡ Ã— ç›®æ ‡äº‹é¡¹å‡ºç°æ¬¡æ•°
             - ç±»åˆ«å…³è”ï¼šæ–¹å‘æ€§æƒé‡ï¼ŒåŸºäºç›®æ ‡äº‹é¡¹å†…å®¹ä¸°å¯Œåº¦
          6. é€‰æ‹©Top-Näº‹é¡¹ï¼ˆä¿ç•™æº¯æºï¼‰ï¼šæŒ‰PageRankå¾—åˆ†æ’åºå¹¶ä¿ç•™çº¿ç´¢

        Args:
            key_final: ä»Recallè¿”å›çš„å…³é”®å®ä½“åˆ—è¡¨
            config: Rerankæœç´¢é…ç½®

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events (List[SourceEvent]): äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰ PageRank é¡ºåºæ’åˆ—ï¼‰
                - clues (Dict): å¬å›çº¿ç´¢ä¿¡æ¯
                    - origin_query (str): åŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
                    - final_query (str): LLMé‡å†™åçš„æŸ¥è¯¢ï¼ˆé‡å†™åï¼‰
                    - query_entities (List[Dict]): æŸ¥è¯¢å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities (List[Dict]): å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æ‰query_entitiesä¸­çš„å€¼ï¼‰
        """
        try:
            # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
            overall_start = time.perf_counter()

            self.logger.info("=" * 80)
            self.logger.info(f"ã€äº‹é¡¹çº§ PageRankã€‘Rerankæœç´¢å¼€å§‹")
            self.logger.info(f"Query: '{config.query}'")
            self.logger.info(f"Source IDs: {config.get_source_config_ids()}")
            self.logger.info("=" * 80)

            # åˆå§‹åŒ– Tracker
            from sag.modules.search.tracker import Tracker
            tracker = Tracker(config)

            # ç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼ˆé¿å…åœ¨step1å’Œstep2ä¸­é‡å¤è®¡ç®—ï¼‰
            vector_start = time.perf_counter()
            query_vector = await self._generate_query_vector(config.query, config)
            vector_time = time.perf_counter() - vector_start
            if config.has_query_embedding:
                self.logger.info(
                    f"âœ“ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")
            else:
                self.logger.info(
                    f"âœ“ æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")

            # ç”¨äºè®°å½•å„æ­¥éª¤è€—æ—¶
            step_times = {}

            # æ­¥éª¤1å’Œ2å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼ˆäº’ä¸ä¾èµ–ï¼‰
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ã€Step1 & Step2ã€‘å¹¶è¡Œæ‰§è¡Œ...")
            self.logger.info("=" * 80)
            parallel_start = time.perf_counter()

            # å¹¶è¡Œæ‰§è¡Œ step1 å’Œ step2ï¼ˆäº‹é¡¹çº§ï¼‰
            step1_task = self._step1_keys_to_events(
                key_final=key_final,
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                query_vector=query_vector,
                config=config
            )

            step2_task = self._step2_query_to_events(
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                k=config.rerank.max_query_recall_results,  # ä½¿ç”¨max_query_recall_resultsæ§åˆ¶å¬å›æ•°é‡
                query_vector=query_vector,
                config=config
            )

            # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡éƒ½å®Œæˆ
            step1_events, step2_events = await asyncio.gather(step1_task, step2_task)

            parallel_time = time.perf_counter() - parallel_start
            step_times['Step1&2_å¹¶è¡Œæ‰§è¡Œ'] = parallel_time

            self.logger.info(
                f"âœ“ Step1&2 å¹¶è¡Œå®Œæˆ: "
                f"Step1={len(step1_events)}ä¸ªäº‹é¡¹, "
                f"Step2={len(step2_events)}ä¸ªäº‹é¡¹, "
                f"è€—æ—¶: {parallel_time:.3f}ç§’"
            )

            # æ­¥éª¤3: åˆå¹¶äº‹é¡¹å»é‡
            step3_start = time.perf_counter()
            merged_events = await self._step3_merge_events(
                step1_events, step2_events)
            step3_time = time.perf_counter() - step3_start
            step_times['Step3_åˆå¹¶å»é‡'] = step3_time
            self.logger.info(
                f"âœ“ Step3 å®Œæˆ: åˆå¹¶å {len(merged_events)} ä¸ªäº‹é¡¹, è€—æ—¶: {step3_time:.3f}ç§’")

            # æ­¥éª¤4: è®¡ç®—äº‹é¡¹æƒé‡
            step4_start = time.perf_counter()
            event_weights = await self._step4_calculate_event_weights(merged_events)
            step4_time = time.perf_counter() - step4_start
            step_times['Step4_æƒé‡è®¡ç®—'] = step4_time
            self.logger.info(
                f"âœ“ Step4 å®Œæˆ: è®¡ç®—äº† {len(event_weights)} ä¸ªäº‹é¡¹çš„æƒé‡, è€—æ—¶: {step4_time:.3f}ç§’")

            # æ­¥éª¤5: æ„å»ºäº‹é¡¹å…³ç³»å›¾ + PageRankæ’åº
            step5_start = time.perf_counter()
            sorted_events = await self._step5_build_event_graph_and_pagerank(
                merged_events=merged_events,
                event_weights=event_weights,
                key_final=key_final,  # ä¼ å…¥key_finalä»¥è·å–å®ä½“æƒé‡
                damping=0.85,
                max_iterations=100
            )
            step5_time = time.perf_counter() - step5_start
            step_times['Step5_PageRankæ’åº'] = step5_time
            self.logger.info(
                f"âœ“ Step5 å®Œæˆ: æ’åºäº† {len(sorted_events)} ä¸ªäº‹é¡¹, è€—æ—¶: {step5_time:.3f}ç§’")

            # æ­¥éª¤6: é€‰æ‹©Top-Näº‹é¡¹å¹¶ç”Ÿæˆæº¯æºçº¿ç´¢
            step6_start = time.perf_counter()
            final_events = await self._step6_get_topn_events(
                sorted_events=sorted_events,
                key_final=key_final,
                config=config,
                tracker=tracker
            )
            step6_time = time.perf_counter() - step6_start
            step_times['Step6_Top-Nç­›é€‰'] = step6_time
            self.logger.info(
                f"âœ“ Step6 å®Œæˆ: æœ€ç»ˆè¿”å› {len(final_events)} ä¸ªäº‹é¡¹, è€—æ—¶: {step6_time:.3f}ç§’")

            # è®¡ç®—æ€»è€—æ—¶
            overall_time = time.perf_counter() - overall_start

            # è¾“å‡ºè€—æ—¶ç»Ÿè®¡æ±‡æ€»
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ã€äº‹é¡¹çº§ PageRankã€‘å„æ­¥éª¤è€—æ—¶æ±‡æ€»:")
            self.logger.info("-" * 80)
            self.logger.info(
                f"æŸ¥è¯¢å‘é‡ç”Ÿæˆ: {vector_time:.3f}ç§’ ({vector_time/overall_time*100:.1f}%)")
            for step_name, step_time in step_times.items():
                self.logger.info(
                    f"{step_name}: {step_time:.3f}ç§’ ({step_time/overall_time*100:.1f}%)")
            self.logger.info("-" * 80)
            self.logger.info(f"âœ“ æ€»è€—æ—¶: {overall_time:.3f}ç§’")
            self.logger.info("=" * 80)

            # æ„å»º event_to_cluesï¼ˆä»trackerè·å–ï¼‰
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä» sorted_events æ„å»º
            event_to_clues = {}
            for event_data in sorted_events[:config.rerank.max_results]:
                event_id = event_data["event_id"]
                source_entities = event_data.get("source_entities", [])

                # å°†å®ä½“IDåˆ—è¡¨è½¬æ¢ä¸ºå®ä½“å¯¹è±¡åˆ—è¡¨
                entity_objects = []
                for entity_id in source_entities:
                    # ä» key_final ä¸­æŸ¥æ‰¾å¯¹åº”çš„å®ä½“ä¿¡æ¯
                    for key in key_final:
                        if key.get("key_id") == entity_id or key.get("id") == entity_id:
                            entity_objects.append({
                                "id": entity_id,
                                "name": key.get("name", ""),
                                "type": key.get("type", ""),
                                "weight": key.get("weight", 0.0)
                            })
                            break

                event_to_clues[event_id] = entity_objects

            # æ„å»ºå¹¶è¿”å›æ–°çš„å“åº”æ ¼å¼
            return await self._build_response(config, key_final, final_events, event_to_clues)

        except Exception as e:
            self.logger.error(f"Rerankæœç´¢å¤±è´¥: {e}", exc_info=True)
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿”å› final_query
            # å¦‚æœå¯ç”¨äº†queryé‡å†™åŠŸèƒ½ï¼ˆenable_query_rewrite=Trueï¼‰ï¼Œåˆ™è¿”å›é‡å†™åçš„query
            # å¦åˆ™è¿”å› None
            final_query = config.query if config.enable_query_rewrite else None
            return {
                "events": [],
                "clues": {
                    "origin_query": config.original_query,
                    "final_query": final_query,
                    "query_entities": [],
                    "recall_entities": []
                }
            }  # å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸

    async def _step1_keys_to_events(
        self,
        key_final: List[Dict[str, Any]],
        query: str,
        source_config_ids: List[str],
        query_vector: Optional[List[float]] = None,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤1ï¼ˆäº‹é¡¹çº§ï¼‰: keyæ‰¾eventï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰

        æ ¹æ®[key-final]ä»SQLä¸­æå–ç›¸å…³äº‹é¡¹ï¼Œç„¶åä»ESè·å–äº‹é¡¹å‘é‡å¹¶è®¡ç®—ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        æµç¨‹ï¼š
        1. æå–å®ä½“IDå’Œæƒé‡
        2. é€šè¿‡EventEntityè¡¨æ‰¾åˆ°ä¸è¿™äº›å®ä½“ç›¸å…³çš„äº‹ä»¶
        3. ä»ESæ‰¹é‡è·å–äº‹é¡¹çš„content_vector
        4. è®¡ç®—æ¯ä¸ªäº‹é¡¹ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦
        5. æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤äº‹é¡¹
        6. ä¿ç•™source_entitiesç”¨äºæº¯æº

        Args:
            key_final: ä»Recallè¿”å›çš„key_finalæ•°æ®
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºé…ç½®IDåˆ—è¡¨
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            config: æœç´¢é…ç½®ï¼ˆç”¨äºç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤ï¼‰

        Returns:
            äº‹é¡¹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªäº‹é¡¹åŒ…å«ï¼š
            {
                "event_id": str,
                "event": SourceEventå¯¹è±¡,
                "similarity_score": float,  # ä½™å¼¦ç›¸ä¼¼åº¦
                "source_entities": List[str],  # å¬å›è¯¥äº‹é¡¹çš„å®ä½“IDåˆ—è¡¨
                "entity_weights": Dict[str, float]  # å®ä½“ID -> æƒé‡æ˜ å°„
            }
        """
        try:
            self.logger.info(
                f"[äº‹é¡¹çº§Step1] å¼€å§‹: å¤„ç† {len(key_final)} ä¸ªkey, query='{query}'")

            if not key_final:
                return []

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
                if config and config.has_query_embedding:
                    self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
                else:
                    self.logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.info(f"ä½¿ç”¨ä¼ å…¥çš„æŸ¥è¯¢å‘é‡ï¼Œç»´åº¦: {len(query_vector)}")

            # 2. æå–å®ä½“IDå’Œæƒé‡
            entity_ids = [key.get("key_id") or key.get("id")
                          for key in key_final]
            entity_weight_map = {
                key.get("key_id") or key.get("id"): key["weight"]
                for key in key_final
            }

            # è¿‡æ»¤æ‰å¯èƒ½ä¸º None çš„ ID
            entity_ids = [eid for eid in entity_ids if eid]

            if not entity_ids:
                self.logger.warning("key_final ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®ä½“ID")
                return []

            self.logger.info(
                f"ä» {len(key_final)} ä¸ªkeyä¸­æå–åˆ° {len(entity_ids)} ä¸ªå®ä½“ID")

            async with self.session_factory() as session:
                # 3. é€šè¿‡EventEntityæŸ¥æ‰¾ç›¸å…³äº‹ä»¶ï¼ˆé™åˆ¶åœ¨æŒ‡å®šsource_config_idså†…ï¼‰
                event_entity_query = (
                    select(EventEntity.event_id,
                           EventEntity.entity_id, EventEntity.weight)
                    .join(SourceEvent, EventEntity.event_id == SourceEvent.id)
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(
                                source_config_ids),
                            EventEntity.entity_id.in_(entity_ids)
                        )
                    )
                    .distinct()
                )

                event_result = await session.execute(event_entity_query)
                event_entities = event_result.fetchall()

                if not event_entities:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³äº‹ä»¶")
                    return []

                # 4. è®°å½•äº‹ä»¶åˆ°å®ä½“çš„æ˜ å°„å…³ç³»
                event_to_entities = {}  # äº‹ä»¶ID -> å®ä½“IDåˆ—è¡¨
                event_entity_weights = {}  # (äº‹ä»¶ID, å®ä½“ID) -> EventEntityæƒé‡

                for event_entity in event_entities:
                    event_id = event_entity.event_id
                    entity_id = event_entity.entity_id
                    event_entity_weight = event_entity.weight or 1.0

                    # è®°å½•æ˜ å°„å…³ç³»
                    if event_id not in event_to_entities:
                        event_to_entities[event_id] = []
                    event_to_entities[event_id].append(entity_id)

                    # è®°å½•EventEntityæƒé‡
                    event_entity_weights[(event_id, entity_id)
                                         ] = event_entity_weight

                event_ids = list(event_to_entities.keys())
                self.logger.info(f"æ‰¾åˆ° {len(event_ids)} ä¸ªç›¸å…³äº‹ä»¶")

                # 5. è·å–äº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ˆé¢„åŠ è½½ source å’Œ article å…³ç³»ï¼‰
                event_detail_query = (
                    select(SourceEvent)
                    .options(
                        selectinload(SourceEvent.source),  # é¢„åŠ è½½ SourceConfig
                        selectinload(SourceEvent.article)  # é¢„åŠ è½½ Article
                    )
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(
                                source_config_ids),
                            SourceEvent.id.in_(event_ids)
                        )
                    )
                )
                event_detail_result = await session.execute(event_detail_query)
                events = event_detail_result.scalars().all()

                if not events:
                    self.logger.warning("æœªæ‰¾åˆ°äº‹ä»¶è¯¦æƒ…")
                    return []

                self.logger.info(f"è·å–åˆ° {len(events)} ä¸ªäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯")

                # 6. ä¸ºæ¯ä¸ªäº‹é¡¹æ·»åŠ  document_name å’Œ source_name å±æ€§
                for event in events:
                    # æ·»åŠ  source_name (from SourceConfig.name)
                    event.source_name = event.source.name if event.source else ""

                    # æ·»åŠ  document_name (from Article.title)
                    event.document_name = event.article.title if event.article else ""

                # 7. ä»ESæ‰¹é‡è·å–äº‹é¡¹çš„content_vector
                self.logger.info(f"ä» ES æ‰¹é‡è·å– {len(event_ids)} ä¸ªäº‹é¡¹çš„å‘é‡...")

                es_events_data = await self.event_repo.get_events_by_ids(event_ids=event_ids)

                # æ„å»º event_id -> content_vector çš„æ˜ å°„
                event_vector_map = {}
                for es_event in es_events_data:
                    event_id = es_event.get('event_id')
                    content_vector = es_event.get('content_vector')
                    if event_id and content_vector:
                        event_vector_map[event_id] = content_vector

                self.logger.info(
                    f"ä» ES è·å–åˆ° {len(event_vector_map)} ä¸ªäº‹é¡¹çš„å‘é‡ "
                    f"(è¯·æ±‚äº† {len(event_ids)} ä¸ª)"
                )

                # 8. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
                self.logger.info("=" * 80)
                self.logger.info("[äº‹é¡¹çº§Step1] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—:")
                self.logger.info("-" * 80)

                event_results = []

                for event in events:
                    event_id = event.id

                    # è·å–äº‹é¡¹å‘é‡
                    event_vector = event_vector_map.get(event_id)

                    if not event_vector:
                        self.logger.warning(
                            f"äº‹é¡¹ {event_id[:8]}... åœ¨ESä¸­æœªæ‰¾åˆ°å‘é‡ï¼Œè·³è¿‡")
                        continue

                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    try:
                        # è½¬æ¢ä¸ºnumpyæ•°ç»„
                        query_np = np.array(query_vector, dtype=np.float32)
                        event_np = np.array(event_vector, dtype=np.float32)

                        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                        cosine_score = float(
                            np.dot(query_np, event_np) /
                            (np.linalg.norm(query_np) * np.linalg.norm(event_np))
                        )
                    except Exception as e:
                        self.logger.warning(
                            f"äº‹é¡¹ {event_id[:8]}... ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                        cosine_score = 0.0

                    # è·å–å¬å›è¯¥äº‹é¡¹çš„å®ä½“åˆ—è¡¨
                    source_entities = event_to_entities.get(event_id, [])

                    # æ„å»ºå®ä½“æƒé‡æ˜ å°„ï¼ˆåŒ…å«keyæƒé‡å’ŒEventEntityæƒé‡ï¼‰
                    entity_weights = {}
                    for entity_id in source_entities:
                        key_weight = entity_weight_map.get(entity_id, 1.0)
                        ee_weight = event_entity_weights.get(
                            (event_id, entity_id), 1.0)
                        entity_weights[entity_id] = float(
                            key_weight) * float(ee_weight)

                    # è®°å½•ç»“æœ
                    event_result = {
                        "event_id": event_id,
                        "event": event,
                        "similarity_score": cosine_score,
                        "source_entities": source_entities,
                        "entity_weights": entity_weights
                    }
                    event_results.append(event_result)

                    # æ—¥å¿—è¾“å‡º
                    title_preview = event.title[:40] if event.title else "æ— æ ‡é¢˜"
                    self.logger.info(
                        f"äº‹é¡¹ {event_id[:8]}... | "
                        f"Cosine={cosine_score:.4f} | "
                        f"å…³è”å®ä½“æ•°={len(source_entities)} | "
                        f"æ ‡é¢˜: {title_preview}"
                    )

                self.logger.info("=" * 80)

                # 8. æŒ‰ç›¸ä¼¼åº¦æ’åº
                event_results.sort(
                    key=lambda x: x["similarity_score"], reverse=True)

                # 9. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
                original_count = len(event_results)
                if config and config.rerank.score_threshold:
                    filtered_results = [
                        r for r in event_results
                        if r["similarity_score"] >= config.rerank.score_threshold
                    ]

                    if len(filtered_results) < original_count:
                        self.logger.info(
                            f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªäº‹é¡¹ "
                            f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                        )

                        # å±•ç¤ºè¿‡æ»¤åä¿ç•™çš„äº‹é¡¹ä¿¡æ¯
                        if filtered_results:
                            self.logger.info("=" * 80)
                            self.logger.info(
                                f"è¿‡æ»¤åä¿ç•™çš„ {len(filtered_results)} ä¸ªäº‹é¡¹:")
                            self.logger.info("-" * 80)
                            for result in filtered_results:
                                title_preview = result["event"].title[:
                                                                      40] if result["event"].title else "æ— æ ‡é¢˜"
                                self.logger.info(
                                    f"äº‹é¡¹ {result['event_id'][:8]}... | "
                                    f"Cosine={result['similarity_score']:.4f} | "
                                    f"æ ‡é¢˜: {title_preview}"
                                )
                            self.logger.info("=" * 80)

                    event_results = filtered_results
                else:
                    self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

                self.logger.info(
                    f"[äº‹é¡¹çº§Step1] å®Œæˆ: å¤„ç†äº† {len(event_results)} ä¸ªäº‹é¡¹",
                    extra={
                        "avg_cosine_score": np.mean([r["similarity_score"] for r in event_results]) if event_results else 0.0
                    }
                )

                # ğŸ†• æ ¹æ® max_key_recall_results æˆªæ–­ï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
                max_key_results = config.rerank.max_key_recall_results if config else 30
                if len(event_results) > max_key_results:
                    self.logger.warning(
                        f"âš ï¸  [äº‹é¡¹çº§Step1] Keyå¬å›äº‹é¡¹æ•°({len(event_results)})è¶…è¿‡max_key_recall_results({max_key_results})ï¼Œ"
                        f"å°†æŒ‰ç›¸ä¼¼åº¦æ’åºåæˆªæ–­"
                    )

                    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
                    event_results.sort(
                        key=lambda x: x["similarity_score"],
                        reverse=True
                    )

                    # æˆªæ–­
                    truncated_results = event_results[:max_key_results]

                    self.logger.info(
                        f"ğŸ“Š [äº‹é¡¹çº§Step1] æˆªæ–­ç»Ÿè®¡: "
                        f"ä¿ç•™{len(truncated_results)}ä¸ª, "
                        f"ä¸¢å¼ƒ{len(event_results) - len(truncated_results)}ä¸ª"
                    )

                    event_results = truncated_results

                # æ˜¾ç¤ºTop 5ç»“æœ
                top_results = event_results[:5]
                for i, result in enumerate(top_results, 1):
                    self.logger.debug(
                        f"Top {i}: {result['event'].title[:50]} - "
                        f"Cosine:{result['similarity_score']:.3f}"
                    )

                return event_results

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step1] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step2_query_to_events(
        self,
        query: str,
        source_config_ids: List[str],
        k: int = 30,  # é»˜è®¤å€¼æ”¹ä¸º30ï¼Œä¸max_query_recall_resultsä¸€è‡´
        query_vector: Optional[List[float]] = None,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤2ï¼ˆäº‹é¡¹çº§ï¼‰: queryæ‰¾eventï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰

        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦åœ¨ESä¸­æ‰¾åˆ°ç›¸ä¼¼äº‹é¡¹ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¾—åˆ†

        æµç¨‹ï¼š
        1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼‰
        2. ä½¿ç”¨ES KNNæœç´¢æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„äº‹é¡¹
        3. è®¡ç®—æ¯ä¸ªäº‹é¡¹ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦
        4. æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤äº‹é¡¹
        5. è¿”å›äº‹é¡¹ç»“æœ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: ESå¬å›æ•°é‡ï¼ˆå»ºè®®ä½¿ç”¨config.rerank.max_query_recall_resultsï¼‰
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            config: æœç´¢é…ç½®ï¼ˆç”¨äºç¼“å­˜å’Œé˜ˆå€¼è¿‡æ»¤ï¼‰

        Returns:
            äº‹é¡¹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªäº‹é¡¹åŒ…å«ï¼š
            {
                "event_id": str,
                "event": SourceEventå¯¹è±¡ï¼ˆä»SQLæŸ¥è¯¢è·å–ï¼‰,
                "similarity_score": float,  # ä½™å¼¦ç›¸ä¼¼åº¦
                "source": str  # "query"ï¼ˆç”¨äºåŒºåˆ†æ¥è‡ªStep1è¿˜æ˜¯Step2ï¼‰
            }
        """
        try:
            self.logger.info(
                f"[äº‹é¡¹çº§Step2] å¼€å§‹: query='{query}', source_config_ids={source_config_ids}")

            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼‰
            if query_vector is None:
                query_vector = await self._generate_query_vector(query, config)
            if config and config.has_query_embedding:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}")
            else:
                self.logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # 2. ä½¿ç”¨ES KNNæœç´¢æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„äº‹é¡¹
            self.logger.info(
                f"ä» ES æœç´¢ç›¸ä¼¼äº‹é¡¹ï¼Œsource_config_ids={source_config_ids}, k={k}")

            similar_events = await self.event_repo.search_similar_by_content(
                query_vector=query_vector,
                k=k,
                source_config_ids=source_config_ids
            )

            self.logger.info(f"ESæœç´¢æ‰¾åˆ° {len(similar_events)} ä¸ªç›¸ä¼¼äº‹é¡¹")

            if not similar_events:
                self.logger.warning("æœªæ‰¾åˆ°ç›¸ä¼¼äº‹é¡¹")
                return []

            # 3. æå–äº‹é¡¹IDåˆ—è¡¨ï¼Œå¹¶ä»SQLæŸ¥è¯¢å®Œæ•´äº‹é¡¹ä¿¡æ¯
            event_ids_from_es = [e.get("event_id")
                                 for e in similar_events if e.get("event_id")]

            if not event_ids_from_es:
                self.logger.warning("ESè¿”å›çš„äº‹é¡¹ä¸­æ²¡æœ‰æœ‰æ•ˆçš„event_id")
                return []

            self.logger.info(f"ä»SQLæŸ¥è¯¢ {len(event_ids_from_es)} ä¸ªäº‹é¡¹çš„è¯¦ç»†ä¿¡æ¯...")

            async with self.session_factory() as session:
                # ä»SQLè·å–äº‹é¡¹è¯¦ç»†ä¿¡æ¯ï¼ˆé¢„åŠ è½½ source å’Œ article å…³ç³»ï¼‰
                event_detail_query = (
                    select(SourceEvent)
                    .options(
                        selectinload(SourceEvent.source),  # é¢„åŠ è½½ SourceConfig
                        selectinload(SourceEvent.article)  # é¢„åŠ è½½ Article
                    )
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(
                                source_config_ids),
                            SourceEvent.id.in_(event_ids_from_es)
                        )
                    )
                )
                event_detail_result = await session.execute(event_detail_query)
                events_from_sql = event_detail_result.scalars().all()

                if not events_from_sql:
                    self.logger.warning("SQLä¸­æœªæ‰¾åˆ°å¯¹åº”çš„äº‹é¡¹")
                    return []

                # ä¸ºæ¯ä¸ªäº‹é¡¹æ·»åŠ  document_name å’Œ source_name å±æ€§
                for event in events_from_sql:
                    # æ·»åŠ  source_name (from SourceConfig.name)
                    event.source_name = event.source.name if event.source else ""

                    # æ·»åŠ  document_name (from Article.title)
                    event.document_name = event.article.title if event.article else ""

                # æ„å»º event_id -> SourceEvent æ˜ å°„
                event_map = {event.id: event for event in events_from_sql}

                self.logger.info(f"ä»SQLè·å–åˆ° {len(event_map)} ä¸ªäº‹é¡¹çš„è¯¦ç»†ä¿¡æ¯")

            # 4. æ„å»º event_id -> content_vector æ˜ å°„
            event_vector_map = {}
            for es_event in similar_events:
                event_id = es_event.get("event_id")
                content_vector = es_event.get("content_vector")
                if event_id and content_vector:
                    event_vector_map[event_id] = content_vector

            # 5. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†
            self.logger.info("=" * 80)
            self.logger.info("[äº‹é¡¹çº§Step2] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—:")
            self.logger.info("-" * 80)

            event_results = []

            for es_event in similar_events:
                event_id = es_event.get("event_id")

                # æ£€æŸ¥è¯¥äº‹é¡¹æ˜¯å¦åœ¨SQLä¸­æ‰¾åˆ°
                if event_id not in event_map:
                    self.logger.debug(
                        f"äº‹é¡¹ {event_id[:8] if event_id else 'None'}... åœ¨SQLä¸­æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
                    continue

                event = event_map[event_id]

                # è·å–äº‹é¡¹å‘é‡
                event_vector = event_vector_map.get(event_id)

                if not event_vector:
                    self.logger.warning(f"äº‹é¡¹ {event_id[:8]}... æ²¡æœ‰å‘é‡ä¿¡æ¯ï¼Œè·³è¿‡")
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                try:
                    # è½¬æ¢ä¸ºnumpyæ•°ç»„
                    query_np = np.array(query_vector, dtype=np.float32)
                    event_np = np.array(event_vector, dtype=np.float32)

                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    cosine_score = float(
                        np.dot(query_np, event_np) /
                        (np.linalg.norm(query_np) * np.linalg.norm(event_np))
                    )
                except Exception as e:
                    self.logger.warning(f"äº‹é¡¹ {event_id[:8]}... ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                    cosine_score = 0.0

                # è®°å½•ç»“æœ
                event_result = {
                    "event_id": event_id,
                    "event": event,
                    "similarity_score": cosine_score,
                    "source": "query"  # æ ‡è®°æ¥è‡ªqueryç›´æ¥å¬å›
                }
                event_results.append(event_result)

                # æ—¥å¿—è¾“å‡º
                title_preview = event.title[:40] if event.title else "æ— æ ‡é¢˜"
                self.logger.info(
                    f"äº‹é¡¹ {event_id[:8]}... | "
                    f"Cosine={cosine_score:.4f} | "
                    f"æ ‡é¢˜: {title_preview}"
                )

            self.logger.info("=" * 80)

            # 6. æŒ‰ç›¸ä¼¼åº¦æ’åº
            event_results.sort(
                key=lambda x: x["similarity_score"], reverse=True)

            # 7. ä½¿ç”¨ config.rerank.score_threshold è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            original_count = len(event_results)
            if config and config.rerank.score_threshold:
                filtered_results = [
                    r for r in event_results
                    if r["similarity_score"] >= config.rerank.score_threshold
                ]

                if len(filtered_results) < original_count:
                    self.logger.info(
                        f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ªäº‹é¡¹ "
                        f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                    )

                    # å±•ç¤ºè¿‡æ»¤åä¿ç•™çš„äº‹é¡¹ä¿¡æ¯
                    if filtered_results:
                        self.logger.info("=" * 80)
                        self.logger.info(
                            f"è¿‡æ»¤åä¿ç•™çš„ {len(filtered_results)} ä¸ªäº‹é¡¹:")
                        self.logger.info("-" * 80)
                        for result in filtered_results:
                            title_preview = result["event"].title[:
                                                                  40] if result["event"].title else "æ— æ ‡é¢˜"
                            self.logger.info(
                                f"äº‹é¡¹ {result['event_id'][:8]}... | "
                                f"Cosine={result['similarity_score']:.4f} | "
                                f"æ ‡é¢˜: {title_preview}"
                            )
                        self.logger.info("=" * 80)

                event_results = filtered_results
            else:
                self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")

            self.logger.info(
                f"[äº‹é¡¹çº§Step2] å®Œæˆ: å¤„ç†äº† {len(event_results)} ä¸ªäº‹é¡¹",
                extra={
                    "avg_cosine_score": np.mean([r["similarity_score"] for r in event_results]) if event_results else 0.0
                }
            )

            # æ˜¾ç¤ºTop 5ç»“æœ
            top_results = event_results[:5]
            for i, result in enumerate(top_results, 1):
                self.logger.debug(
                    f"Top {i}: {result['event'].title[:50]} - "
                    f"Cosine:{result['similarity_score']:.3f}"
                )

            return event_results

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step2] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step3_merge_events(
        self,
        step1_events: List[Dict[str, Any]],
        step2_events: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤3ï¼ˆäº‹é¡¹çº§ï¼‰: åˆå¹¶eventå»é‡ï¼ˆä¿ç•™step1ç»“æœï¼‰

        åˆå¹¶Step1å’ŒStep2çš„äº‹é¡¹ç»“æœï¼Œå¦‚æœåŒä¸€äº‹é¡¹åŒæ—¶å‡ºç°åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­ï¼Œåªä¿ç•™Step1çš„ç»“æœ

        æµç¨‹ï¼š
        1. ä»¥Step1ç»“æœä¸ºåŸºç¡€
        2. æ·»åŠ Step2ä¸­ä¸åœ¨Step1ä¸­çš„äº‹é¡¹
        3. è®°å½•æ¯ä¸ªäº‹é¡¹çš„å¬å›æ¥æº

        Args:
            step1_events: Step1è¿”å›çš„äº‹é¡¹åˆ—è¡¨
            step2_events: Step2è¿”å›çš„äº‹é¡¹åˆ—è¡¨

        Returns:
            åˆå¹¶åçš„äº‹é¡¹åˆ—è¡¨ï¼Œæ¯ä¸ªäº‹é¡¹åŒ…å«ï¼š
            {
                "event_id": str,
                "event": SourceEventå¯¹è±¡,
                "similarity_score": float,  # æ¥è‡ªStep1æˆ–Step2çš„ç›¸ä¼¼åº¦
                "source": str,  # "entity" æˆ– "query"
                "source_entities": List[str],  # ä»…Step1æœ‰ï¼Œå¬å›è¯¥äº‹é¡¹çš„å®ä½“IDåˆ—è¡¨
                "entity_weights": Dict[str, float]  # ä»…Step1æœ‰ï¼Œå®ä½“ID -> æƒé‡æ˜ å°„
            }
        """
        try:
            self.logger.info(
                f"[äº‹é¡¹çº§Step3] å¼€å§‹åˆå¹¶äº‹é¡¹: Step1={len(step1_events)}ä¸ª, Step2={len(step2_events)}ä¸ª")

            # 1. ä»¥Step1ä¸ºåŸºç¡€ï¼Œæ„å»ºevent_idé›†åˆ
            step1_event_ids = {e["event_id"] for e in step1_events}

            # 2. åˆ›å»ºåˆå¹¶åçš„åˆ—è¡¨ï¼Œå…ˆæ·»åŠ æ‰€æœ‰Step1ç»“æœ
            merged_events = []

            for event_data in step1_events:
                # Step1çš„äº‹é¡¹ï¼Œsourceåº”ä¸º"entity"
                merged_event = {
                    "event_id": event_data["event_id"],
                    "event": event_data["event"],
                    "similarity_score": event_data["similarity_score"],
                    "source": "entity",  # Step1æ¥è‡ªå®ä½“å¬å›
                    "source_entities": event_data.get("source_entities", []),
                    "entity_weights": event_data.get("entity_weights", {})
                }
                merged_events.append(merged_event)

            # 3. æ·»åŠ Step2ä¸­ä¸åœ¨Step1ä¸­çš„äº‹é¡¹
            added_from_step2 = 0
            for event_data in step2_events:
                event_id = event_data["event_id"]

                # å¦‚æœè¯¥äº‹é¡¹å·²ç»åœ¨Step1ä¸­ï¼Œè·³è¿‡ï¼ˆä¿ç•™Step1ç»“æœï¼‰
                if event_id in step1_event_ids:
                    self.logger.debug(
                        f"äº‹é¡¹ {event_id[:8]}... å·²åœ¨Step1ä¸­ï¼Œè·³è¿‡Step2ç»“æœ")
                    continue

                # Step2ç‹¬æœ‰çš„äº‹é¡¹
                merged_event = {
                    "event_id": event_id,
                    "event": event_data["event"],
                    "similarity_score": event_data["similarity_score"],
                    "source": "query",  # Step2æ¥è‡ªqueryå¬å›
                    "source_entities": [],  # Step2æ²¡æœ‰å®ä½“ä¿¡æ¯
                    "entity_weights": {}
                }
                merged_events.append(merged_event)
                added_from_step2 += 1

            self.logger.info("=" * 80)
            self.logger.info(f"[äº‹é¡¹çº§Step3] åˆå¹¶ç»Ÿè®¡:")
            self.logger.info("-" * 80)
            self.logger.info(f"  Step1äº‹é¡¹æ•°: {len(step1_events)}")
            self.logger.info(f"  Step2äº‹é¡¹æ•°: {len(step2_events)}")
            self.logger.info(f"  Step2ç‹¬æœ‰äº‹é¡¹æ•°: {added_from_step2}")
            self.logger.info(
                f"  Step1+Step2é‡å¤äº‹é¡¹æ•°: {len(step2_events) - added_from_step2}")
            self.logger.info(f"  åˆå¹¶åæ€»äº‹é¡¹æ•°: {len(merged_events)}")
            self.logger.info("-" * 80)

            # ç»Ÿè®¡å¬å›æ¥æº
            entity_source_count = sum(
                1 for e in merged_events if e["source"] == "entity")
            query_source_count = sum(
                1 for e in merged_events if e["source"] == "query")

            self.logger.info(f"  æŒ‰æ¥æºç»Ÿè®¡:")
            self.logger.info(f"    å®ä½“å¬å›(entity): {entity_source_count} ä¸ª")
            self.logger.info(f"    æŸ¥è¯¢å¬å›(query): {query_source_count} ä¸ª")
            self.logger.info("=" * 80)

            return merged_events

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step3] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step4_calculate_event_weights(
        self,
        merged_events: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        æ­¥éª¤4ï¼ˆäº‹é¡¹çº§ï¼‰: è®¡ç®—äº‹é¡¹æƒé‡å‘é‡

        ä½¿ç”¨å…¬å¼è®¡ç®—æ¯ä¸ªäº‹é¡¹çš„åˆå§‹æƒé‡ï¼Œç”¨äºåç»­PageRank

        å…¬å¼ï¼ˆå‚è€ƒåŸæ®µè½æƒé‡è®¡ç®—ï¼‰:
        weight = 0.5 Ã— similarity_score + ln(1 + Î£(entity_weight))

        è¯´æ˜ï¼š
        - similarity_score: äº‹é¡¹ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦
        - entity_weight: å¬å›è¯¥äº‹é¡¹çš„å®ä½“æƒé‡ï¼ˆä»…Step1æœ‰ï¼ŒStep2ä¸º0ï¼‰
        - å¯¹äºStep1äº‹é¡¹ï¼šä½¿ç”¨entity_weightsä¸­çš„æƒé‡æ±‚å’Œ
        - å¯¹äºStep2äº‹é¡¹ï¼šä»…ä½¿ç”¨similarity_score

        Args:
            merged_events: Step3åˆå¹¶åçš„äº‹é¡¹åˆ—è¡¨

        Returns:
            äº‹é¡¹æƒé‡å­—å…¸ {event_id: weight}
        """
        try:
            self.logger.info(f"[äº‹é¡¹çº§Step4] å¼€å§‹è®¡ç®—äº‹é¡¹æƒé‡ï¼Œå…± {len(merged_events)} ä¸ªäº‹é¡¹")

            event_weights = {}

            self.logger.info("=" * 80)
            self.logger.info("[äº‹é¡¹çº§Step4] æƒé‡è®¡ç®—è¯¦æƒ…:")
            self.logger.info("-" * 80)

            for event_data in merged_events:
                event_id = event_data["event_id"]
                similarity_score = event_data["similarity_score"]
                source = event_data["source"]
                entity_weights = event_data.get("entity_weights", {})

                # è®¡ç®—å®ä½“æƒé‡æ€»å’Œ
                if entity_weights:
                    entity_weight_sum = sum(entity_weights.values())
                else:
                    entity_weight_sum = 0.0

                # ä½¿ç”¨å…¬å¼è®¡ç®—æƒé‡
                # weight = 0.5 Ã— similarity_score + ln(1 + entity_weight_sum)
                weight = 0.5 * similarity_score + \
                    math.log(1 + entity_weight_sum)

                event_weights[event_id] = weight

                # æ—¥å¿—è¾“å‡ºï¼ˆæ˜¾ç¤ºå‰10ä¸ªï¼‰
                if len(event_weights) <= 10:
                    title_preview = event_data["event"].title[:
                                                              30] if event_data["event"].title else "æ— æ ‡é¢˜"
                    self.logger.info(
                        f"äº‹é¡¹ {event_id[:8]}... | "
                        f"æ¥æº={source} | "
                        f"Sim={similarity_score:.4f} | "
                        f"Entityæƒé‡å’Œ={entity_weight_sum:.4f} | "
                        f"æœ€ç»ˆæƒé‡={weight:.4f} | "
                        f"æ ‡é¢˜: {title_preview}"
                    )

            if len(merged_events) > 10:
                self.logger.info(f"... (è¿˜æœ‰ {len(merged_events) - 10} ä¸ªäº‹é¡¹æœªæ˜¾ç¤º)")

            self.logger.info("-" * 80)

            # ç»Ÿè®¡ä¿¡æ¯
            weights_list = list(event_weights.values())
            if weights_list:
                avg_weight = np.mean(weights_list)
                max_weight = np.max(weights_list)
                min_weight = np.min(weights_list)

                self.logger.info(f"æƒé‡ç»Ÿè®¡:")
                self.logger.info(f"  å¹³å‡æƒé‡: {avg_weight:.4f}")
                self.logger.info(f"  æœ€å¤§æƒé‡: {max_weight:.4f}")
                self.logger.info(f"  æœ€å°æƒé‡: {min_weight:.4f}")

            self.logger.info("=" * 80)

            self.logger.info(f"[äº‹é¡¹çº§Step4] å®Œæˆ: è®¡ç®—äº† {len(event_weights)} ä¸ªäº‹é¡¹çš„æƒé‡")

            return event_weights

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step4] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return {}

    async def _step5_build_event_graph_and_pagerank(
        self,
        merged_events: List[Dict[str, Any]],
        event_weights: Dict[str, float],
        key_final: List[Dict[str, Any]],
        damping: float = 0.85,
        max_iterations: int = 100
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤5ï¼ˆäº‹é¡¹çº§ï¼‰: æ„å»ºäº‹é¡¹å…³ç³»å›¾ + æ–¹å‘æ€§PageRankæ’åº

        ä½¿ç”¨äº‹é¡¹ä¹‹é—´çš„æ–¹å‘æ€§å…³è”å…³ç³»æ„å»ºå›¾ï¼Œå®ç°æ›´ç²¾ç»†çš„PageRankæ’åºï¼š
        - èƒ½åŒºåˆ†äº‹é¡¹é‡è¦æ€§å·®å¼‚ï¼šé‡è¦äº‹é¡¹æŠ•ç¥¨æƒé‡æ›´å¤§
        - åŸºäºå†…å®¹çš„æ–¹å‘æ€§ï¼šæŠ•ç¥¨æƒé‡è€ƒè™‘ç›®æ ‡äº‹é¡¹çš„å†…å®¹ä¸°å¯Œåº¦

        æ–¹å‘æ€§å…³ç³»å›¾æ„å»ºï¼š
        1. å®ä½“å…³è”ï¼ˆæ–¹å‘æ€§æƒé‡ï¼‰ï¼š
           - äº‹é¡¹iâ†’äº‹é¡¹jçš„æƒé‡ = keyæƒé‡ Ã— keyåœ¨äº‹é¡¹jçš„å‡ºç°æ¬¡æ•°
           - äº‹é¡¹jâ†’äº‹é¡¹içš„æƒé‡ = keyæƒé‡ Ã— keyåœ¨äº‹é¡¹içš„å‡ºç°æ¬¡æ•°
           - ä½“ç°"é‡è¦äº‹é¡¹æŠ•ç¥¨æ›´æœ‰ä»·å€¼"çš„æ€æƒ³
        2. ç±»åˆ«å…³è”ï¼ˆæ–¹å‘æ€§æƒé‡ï¼‰ï¼š
           - ç›¸åŒcategoryçš„äº‹é¡¹ä¹‹é—´å»ºç«‹æœ‰å‘è¾¹
           - æƒé‡åŸºäºç›®æ ‡äº‹é¡¹çš„å†…å®¹ä¸°å¯Œåº¦æ¯”ä¾‹
           - å†…å®¹æ›´ä¸°å¯Œçš„äº‹é¡¹è·å¾—æ›´å¤šæŠ•ç¥¨æƒé‡

        Args:
            merged_events: Step3åˆå¹¶åçš„äº‹é¡¹åˆ—è¡¨
            event_weights: Step4è®¡ç®—çš„äº‹é¡¹æƒé‡å­—å…¸
            key_final: Recallé˜¶æ®µçš„å…³é”®å®ä½“åˆ—è¡¨ï¼ˆåŒ…å«å®ä½“æƒé‡ï¼‰
            damping: é˜»å°¼ç³»æ•°ï¼ˆé»˜è®¤0.85ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤100ï¼‰

        Returns:
            æ’åºåçš„äº‹é¡¹åˆ—è¡¨ï¼ˆæŒ‰PageRankå¾—åˆ†é™åºï¼‰
        """
        try:
            n = len(merged_events)

            if n == 0:
                self.logger.warning("[äº‹é¡¹çº§Step5] äº‹é¡¹åˆ—è¡¨ä¸ºç©º")
                return []

            self.logger.info(f"[äº‹é¡¹çº§Step5] å¼€å§‹æ„å»ºäº‹é¡¹å…³ç³»å›¾ï¼Œå…± {n} ä¸ªäº‹é¡¹")

            # 1. åˆå§‹åŒ–PageRankå€¼ï¼ˆä½¿ç”¨Step4çš„æƒé‡ä½œä¸ºåˆå§‹å€¼ï¼‰
            weights = np.array([event_weights.get(e["event_id"], 0.0)
                               for e in merged_events])

            if weights.sum() > 0:
                pagerank = weights / weights.sum()  # å½’ä¸€åŒ–
            else:
                pagerank = np.ones(n) / n  # å‡åŒ€åˆ†å¸ƒ

            self.logger.info(
                f"åˆå§‹PageRankåˆ†å¸ƒ: min={pagerank.min():.6f}, max={pagerank.max():.6f}, sum={pagerank.sum():.6f}")

            # 2. æ„å»ºå…³ç³»å›¾
            # graph[i] = [(j, weight), ...] è¡¨ç¤ºä»èŠ‚ç‚¹iåˆ°èŠ‚ç‚¹jçš„è¾¹åŠå…¶æƒé‡
            graph = defaultdict(list)

            # æ„å»º entity_id -> å®ä½“æƒé‡ çš„æ˜ å°„
            entity_weight_map = {}
            for key in key_final:
                key_id = key.get("key_id") or key.get("id")
                if key_id:
                    entity_weight_map[key_id] = key.get("weight", 1.0)

            # 2.1 å®ä½“å…³è”ï¼ˆæ–¹å‘æ€§æƒé‡ï¼‰ï¼šåŸºäºkeyåœ¨äº‹é¡¹å†…å®¹ä¸­çš„å‡ºç°æ¬¡æ•°æ„å»ºæœ‰å‘è¾¹
            # æ ¸å¿ƒæ€æƒ³ï¼šé‡è¦äº‹é¡¹åº”è¯¥"æŠ•ç¥¨"ç»™å†…å®¹æ›´ä¸°å¯Œçš„äº‹é¡¹
            # äº‹é¡¹iâ†’äº‹é¡¹jçš„æƒé‡ = keyæƒé‡ Ã— keyåœ¨äº‹é¡¹jçš„å‡ºç°æ¬¡æ•°
            # è¿™æ ·ï¼Œå†…å®¹ä¸­åŒ…å«æ›´å¤škeyçš„äº‹é¡¹ä¼šè·å¾—æ›´å¤šæŠ•ç¥¨æƒé‡
            entity_to_events = defaultdict(list)
            for i, event in enumerate(merged_events):
                for entity_id in event.get("source_entities", []):
                    entity_to_events[entity_id].append(i)

            # æ„å»ºkeyåç§°åˆ°æƒé‡çš„æ˜ å°„ï¼Œç”¨äºåç»­å†…å®¹åŒ¹é…
            key_name_to_weight = {}
            for key in key_final:
                key_name = key.get("name", "")
                key_weight = key.get("weight", 1.0)
                if key_name:
                    key_name_to_weight[key_name] = key_weight

            entity_edges = 0
            for entity_id, event_indices in entity_to_events.items():
                if len(event_indices) > 1:
                    # è·å–è¯¥å®ä½“çš„æƒé‡
                    entity_weight = entity_weight_map.get(entity_id, 1.0)

                    # æ‰¾åˆ°å¯¹åº”çš„keyåç§°ï¼ˆç”¨äºå†…å®¹åŒ¹é…ï¼‰
                    # é€šè¿‡entity_idåå‘æŸ¥æ‰¾key_finalä¸­çš„å¯¹åº”å®ä½“åç§°
                    key_name = None
                    for key in key_final:
                        key_id = key.get("key_id") or key.get("id")
                        if key_id == entity_id:
                            key_name = key.get("name", "")
                            break

                    if not key_name:
                        continue  # æ‰¾ä¸åˆ°keyåç§°ï¼Œè·³è¿‡å†…å®¹åŒ¹é…

                    # è®¡ç®—æ¯ä¸ªäº‹é¡¹ä¸­keyçš„å‡ºç°æ¬¡æ•°
                    # ç»Ÿè®¡èŒƒå›´ï¼šæ ‡é¢˜ + æ‘˜è¦ + å†…å®¹ï¼Œå…¨é¢åæ˜ äº‹é¡¹ä¸keyçš„å…³è”å¼ºåº¦
                    event_key_counts = {}
                    for event_idx in event_indices:
                        event_obj = merged_events[event_idx]["event"]  # è·å–SourceEventå¯¹è±¡
                        # åˆå¹¶æ ‡é¢˜ã€æ‘˜è¦å’Œå†…å®¹è¿›è¡Œç»Ÿè®¡
                        event_text = f"{event_obj.title} {event_obj.summary} {event_obj.content}"
                        key_count = event_text.count(key_name)
                        event_key_counts[event_idx] = key_count

                    # å»ºç«‹æ–¹å‘æ€§è¾¹ï¼šå®ç°"é‡è¦äº‹é¡¹æŠ•ç¥¨ç»™å†…å®¹æ›´ä¸°å¯Œäº‹é¡¹"çš„æ€æƒ³
                    # äº‹é¡¹iâ†’äº‹é¡¹jçš„æƒé‡ = keyæƒé‡ Ã— keyåœ¨äº‹é¡¹jçš„å‡ºç°æ¬¡æ•°
                    # è¿™æ ·ï¼Œå†…å®¹ä¸­keyå‡ºç°æ¬¡æ•°å¤šçš„äº‹é¡¹ä¼šè·å¾—æ›´å¤šæŠ•ç¥¨æƒé‡
                    # ä½¿ç”¨ idx_i < idx_j é¿å…é‡å¤éå†
                    for idx_i, i in enumerate(event_indices):
                        for idx_j in range(idx_i + 1, len(event_indices)):
                            j = event_indices[idx_j]

                            # è®¡ç®—åŒå‘æƒé‡
                            # iâ†’jçš„æƒé‡åŸºäºjä¸­keyçš„å‡ºç°æ¬¡æ•°
                            weight_i_to_j = entity_weight * event_key_counts[j]
                            # jâ†’içš„æƒé‡åŸºäºiä¸­keyçš„å‡ºç°æ¬¡æ•°
                            weight_j_to_i = entity_weight * event_key_counts[i]

                            # åªå»ºç«‹æƒé‡>0çš„è¾¹ï¼Œé¿å…æ— æ•ˆè¿æ¥
                            # è¿™ç§æœºåˆ¶ç¡®ä¿ï¼š
                            # 1. å†…å®¹ä¸­ä¸åŒ…å«keyçš„äº‹é¡¹ä¸ä¼šå‚ä¸æŠ•ç¥¨
                            # 2. æŠ•ç¥¨æƒé‡ä¸å†…å®¹ä¸°å¯Œåº¦æˆæ­£æ¯”
                            # 3. å®ç°çœŸæ­£çš„"å†…å®¹é‡è¦æ€§"é©±åŠ¨çš„PageRank
                            if weight_i_to_j > 0:
                                graph[i].append((j, weight_i_to_j))
                                entity_edges += 1
                            if weight_j_to_i > 0:
                                graph[j].append((i, weight_j_to_i))
                                entity_edges += 1

            self.logger.info(
                f"å®ä½“å…³è”: {entity_edges} æ¡æœ‰å‘è¾¹ï¼Œæ¶‰åŠ {len([e for e in entity_to_events.values() if len(e) > 1])} ä¸ªå®ä½“")

            # 2.2 ç±»åˆ«å…³è”ï¼ˆæ–¹å‘æ€§æƒé‡ï¼‰ï¼šç›¸åŒcategoryçš„eventä¹‹é—´å»ºè¾¹ï¼Œæƒé‡è€ƒè™‘å†…å®¹ç›¸å…³æ€§
            # æ ¸å¿ƒæ€æƒ³ï¼šåŒç±»äº‹é¡¹ä¸­ï¼Œå†…å®¹æ›´ä¸°å¯Œçš„äº‹é¡¹åº”è¯¥è·å¾—æ›´å¤šæŠ•ç¥¨æƒé‡
            # æƒé‡è®¡ç®—ï¼šåŸºäºç›®æ ‡äº‹é¡¹çš„å†…å®¹é•¿åº¦æ¯”ä¾‹ï¼Œå†…å®¹è¶Šé•¿æƒé‡è¶Šå¤§
            # è¿™æ ·ç¡®ä¿é‡è¦çš„åŒç±»äº‹é¡¹åœ¨PageRankä¸­è·å¾—æ›´é«˜æ’å
            category_to_events = defaultdict(list)
            for i, event in enumerate(merged_events):
                category = event["event"].category
                if category:
                    category_to_events[category].append(i)

            category_edges = 0
            for category, event_indices in category_to_events.items():
                if len(event_indices) > 1:
                    # è®¡ç®—æ¯ä¸ªäº‹é¡¹çš„å†…å®¹é•¿åº¦ï¼ˆç”¨äºæƒé‡åˆ†é…ï¼‰
                    event_content_lengths = {}
                    for event_idx in event_indices:
                        event_obj = merged_events[event_idx]["event"]
                        content_length = len(f"{event_obj.title} {event_obj.summary} {event_obj.content}")
                        event_content_lengths[event_idx] = max(content_length, 1)  # é¿å…é™¤é›¶

                    # å»ºç«‹æ–¹å‘æ€§è¾¹ï¼šæƒé‡åŸºäºç›®æ ‡äº‹é¡¹çš„å†…å®¹ä¸°å¯Œåº¦
                    # ä½¿ç”¨ idx_i < idx_j é¿å…é‡å¤éå†
                    total_length = sum(event_content_lengths.values())
                    if total_length > 0:
                        for idx_i, i in enumerate(event_indices):
                            for idx_j in range(idx_i + 1, len(event_indices)):
                                j = event_indices[idx_j]

                                # è®¡ç®—åŒå‘æƒé‡ï¼šä½¿ç”¨ç›®æ ‡äº‹é¡¹çš„å†…å®¹é•¿åº¦æ¯”ä¾‹ä½œä¸ºæƒé‡
                                # iâ†’jçš„æƒé‡åŸºäºjçš„å†…å®¹é•¿åº¦
                                weight_i_to_j = 0.1 * (event_content_lengths[j] / total_length)
                                # jâ†’içš„æƒé‡åŸºäºiçš„å†…å®¹é•¿åº¦
                                weight_j_to_i = 0.1 * (event_content_lengths[i] / total_length)

                                # å»ºç«‹åŒå‘è¾¹
                                if weight_i_to_j > 0:
                                    graph[i].append((j, weight_i_to_j))
                                    category_edges += 1
                                if weight_j_to_i > 0:
                                    graph[j].append((i, weight_j_to_i))
                                    category_edges += 1

            self.logger.info(
                f"ç±»åˆ«å…³è”: {category_edges} æ¡æœ‰å‘è¾¹ï¼Œæ¶‰åŠ {len([c for c in category_to_events.values() if len(c) > 1])} ä¸ªç±»åˆ«")

            total_edges = entity_edges + category_edges
            self.logger.info(f"å…³ç³»å›¾æ„å»ºå®Œæˆ: æ€»è¾¹æ•°={total_edges}")

            # 3. é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ€»å‡ºæƒé‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
            out_weights = {}
            for j in range(n):
                edges = graph.get(j, [])
                out_weights[j] = sum(w for _, w in edges) if edges else 0.0

            nodes_with_edges = sum(1 for w in out_weights.values() if w > 0)
            self.logger.debug(
                f"é¢„è®¡ç®—å®Œæˆ: {nodes_with_edges}/{n} ä¸ªèŠ‚ç‚¹æœ‰å‡ºè¾¹")

            # 4. PageRankè¿­ä»£ï¼ˆä¼˜åŒ–ç‰ˆï¼šåå‘éå†å›¾ï¼Œé¿å…O(nÂ²)å¤æ‚åº¦ï¼‰
            self.logger.info("=" * 80)
            self.logger.info("[äº‹é¡¹çº§Step5] å¼€å§‹PageRankè¿­ä»£")
            self.logger.info("-" * 80)

            for iteration in range(max_iterations):
                # åˆå§‹åŒ–ä¸ºåŸºç¡€å€¼ (1-d)/n
                new_pagerank = np.ones(n) * (1 - damping) / n

                # éå†æ‰€æœ‰æºèŠ‚ç‚¹jï¼ˆè€Œä¸æ˜¯éå†ç›®æ ‡èŠ‚ç‚¹iï¼‰
                for j in range(n):
                    # è·³è¿‡æ²¡æœ‰PageRankå€¼æˆ–æ²¡æœ‰å‡ºè¾¹çš„èŠ‚ç‚¹
                    if pagerank[j] == 0 or out_weights[j] == 0:
                        continue

                    # è®¡ç®—jå¯¹æ¯æ¡å‡ºè¾¹çš„å•ä½æƒé‡è´¡çŒ®
                    # è´¡çŒ® = d Ã— PR(j) Ã— edge_weight / total_out_weight
                    contribution_per_weight = damping * pagerank[j] / out_weights[j]

                    # éå†jçš„æ‰€æœ‰å‡ºè¾¹ï¼Œå°†è´¡çŒ®åˆ†é…ç»™ç›®æ ‡èŠ‚ç‚¹
                    for target, edge_weight in graph.get(j, []):
                        new_pagerank[target] += contribution_per_weight * edge_weight

                # æ£€æŸ¥æ”¶æ•›
                diff = np.abs(new_pagerank - pagerank).sum()

                if (iteration + 1) % 10 == 0:
                    self.logger.debug(f"è¿­ä»£ {iteration + 1}: diff={diff:.8f}")

                if diff < 1e-6:
                    self.logger.info(
                        f"PageRankåœ¨ç¬¬ {iteration + 1} æ¬¡è¿­ä»£æ”¶æ•› (diff={diff:.8f})")
                    pagerank = new_pagerank
                    break

                pagerank = new_pagerank

            else:
                self.logger.warning(f"PageRankè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œæœªå®Œå…¨æ”¶æ•›")

            self.logger.info(
                f"æœ€ç»ˆPageRankåˆ†å¸ƒ: min={pagerank.min():.6f}, max={pagerank.max():.6f}, sum={pagerank.sum():.6f}")
            self.logger.info("=" * 80)

            # 4. å°†PageRankå€¼èµ‹ç»™äº‹é¡¹
            for i, event in enumerate(merged_events):
                event["pagerank"] = float(pagerank[i])

            # 5. æŒ‰PageRankæ’åºï¼ˆé™åºï¼‰
            sorted_events = sorted(
                merged_events, key=lambda x: x["pagerank"], reverse=True)

            # 6. æ—¥å¿—ï¼šæ˜¾ç¤ºTop 10
            self.logger.info("=" * 80)
            self.logger.info("[äº‹é¡¹çº§Step5] PageRankæ’åºç»“æœ (Top 10):")
            self.logger.info("-" * 80)

            for rank, event in enumerate(sorted_events[:10], 1):
                title_preview = event["event"].title[:
                                                     40] if event["event"].title else "æ— æ ‡é¢˜"
                self.logger.info(
                    f"Rank {rank}: {event['event_id'][:8]}... | "
                    f"PageRank={event['pagerank']:.6f} | "
                    f"Weight={event_weights.get(event['event_id'], 0.0):.4f} | "
                    f"æ¥æº={event['source']} | "
                    f"æ ‡é¢˜: {title_preview}"
                )

            if len(sorted_events) > 10:
                self.logger.info(f"... (è¿˜æœ‰ {len(sorted_events) - 10} ä¸ªäº‹é¡¹æœªæ˜¾ç¤º)")

            self.logger.info("=" * 80)

            self.logger.info(f"[äº‹é¡¹çº§Step5] å®Œæˆ: æ’åºäº† {len(sorted_events)} ä¸ªäº‹é¡¹")

            return sorted_events

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step5] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step6_get_topn_events(
        self,
        sorted_events: List[Dict[str, Any]],
        key_final: List[Dict[str, Any]],
        config: SearchConfig,
        tracker: Tracker
    ) -> List[SourceEvent]:
        """
        æ­¥éª¤6ï¼ˆäº‹é¡¹çº§ï¼‰: é€‰æ‹©Top-Näº‹é¡¹ï¼ˆä¿ç•™æº¯æºï¼‰

        ä»PageRankæ’åºåçš„äº‹é¡¹ä¸­é€‰æ‹©Top-Nä¸ªï¼Œå¹¶ä¸ºäº‹é¡¹ç”Ÿæˆæº¯æºçº¿ç´¢

        æµç¨‹ï¼š
        1. ä¸º Top-KÃ—3 çš„äº‹é¡¹ç”Ÿæˆ intermediate çº¿ç´¢ï¼ˆæ™®é€šæ¨¡å¼å¯è§ï¼‰
        2. ä¸º Top-K çš„äº‹é¡¹é¢å¤–ç”Ÿæˆ final çº¿ç´¢ï¼ˆç²¾ç®€æ¨¡å¼é«˜äº®æ˜¾ç¤ºï¼‰
        3. è¿”å› Top-K äº‹é¡¹åˆ—è¡¨ï¼ˆæŒ‰PageRanké¡ºåºï¼‰

        Args:
            sorted_events: Step5æ’åºåçš„äº‹é¡¹åˆ—è¡¨
            key_final: Recallé˜¶æ®µçš„keyåˆ—è¡¨ï¼ˆç”¨äºæ„å»ºå®ä½“èŠ‚ç‚¹ï¼‰
            config: æœç´¢é…ç½®
            tracker: çº¿ç´¢è¿½è¸ªå™¨

        Returns:
            Top-Näº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆSourceEventï¼‰
        """
        try:
            topn = config.rerank.max_results
            # æ‰€æœ‰äº‹é¡¹éƒ½ç”Ÿæˆ intermediate çº¿ç´¢
            intermediate_events = sorted_events  # æ”¹ä¸ºæ‰€æœ‰äº‹é¡¹
            # Top-K ç”¨äºç”Ÿæˆ final çº¿ç´¢å’Œæœ€ç»ˆè¿”å›
            final_events = sorted_events[:topn]

            self.logger.info(f"[äº‹é¡¹çº§Step6] å¼€å§‹å¤„ç†äº‹é¡¹")
            self.logger.info(
                f"  æ‰€æœ‰ {len(intermediate_events)} ä¸ªäº‹é¡¹ç”Ÿæˆ intermediate çº¿ç´¢")
            self.logger.info(f"  Top-{topn} äº‹é¡¹ç”Ÿæˆ final çº¿ç´¢")

            # 1. æ„å»º entity_id -> key å¯¹è±¡çš„æ˜ å°„
            entity_to_key = {}
            for key in key_final:
                key_id = key.get("key_id") or key.get("id")
                if key_id:
                    entity_to_key[key_id] = key

            # ========== ç¬¬ä¸€æ­¥ï¼šä¸ºæ‰€æœ‰äº‹é¡¹ç”Ÿæˆ intermediate çº¿ç´¢ ==========
            self.logger.info("")
            self.logger.info("=" * 80)
            self.logger.info(
                f"[äº‹é¡¹çº§Step6] ç”Ÿæˆ Intermediate çº¿ç´¢ (æ‰€æœ‰ {len(intermediate_events)} ä¸ªäº‹é¡¹)")
            self.logger.info("-" * 80)

            intermediate_entity_clue_count = 0
            intermediate_query_clue_count = 0

            for rank, event_data in enumerate(intermediate_events, 1):
                event_obj = event_data["event"]
                event_id = event_data["event_id"]
                source = event_data["source"]
                source_entities = event_data.get("source_entities", [])

                if source == "entity":
                    # Step1å¬å›çš„äº‹é¡¹ï¼šä¸ºæ¯ä¸ªsource_entityç”Ÿæˆçº¿ç´¢ï¼ˆentity â†’ eventï¼‰
                    for entity_id in source_entities:
                        # ä»key_finalä¸­è·å–å®ä½“ä¿¡æ¯
                        key_info = entity_to_key.get(entity_id)
                        if not key_info:
                            self.logger.warning(
                                f"å®ä½“ {entity_id[:8]}... åœ¨key_finalä¸­æœªæ‰¾åˆ°")
                            continue

                        # æ„å»ºå®ä½“èŠ‚ç‚¹
                        entity_node = Tracker.build_entity_node({
                            "key_id": entity_id,
                            "id": entity_id,
                            "name": key_info.get("name", ""),
                            "type": key_info.get("type", ""),
                            "description": key_info.get("description", "")
                        })

                        # æ„å»ºäº‹é¡¹èŠ‚ç‚¹ï¼ˆä½¿ç”¨trackerå®ä¾‹æ–¹æ³•ï¼‰
                        event_node = tracker.get_or_create_event_node(
                            event_obj,
                            "rerank",
                            recall_method="entity"
                        )

                        # æ·»åŠ  intermediate çº¿ç´¢
                        tracker.add_clue(
                            stage="rerank",
                            from_node=entity_node,
                            to_node=event_node,
                            confidence=event_data["similarity_score"],
                            relation="å®ä½“å¬å›",
                            display_level="intermediate",  # intermediate çº§åˆ«
                            metadata={
                                "method": "pagerank_entity",
                                "pagerank_score": event_data["pagerank"],
                                "similarity_score": event_data["similarity_score"],
                                "rank": rank
                            }
                        )
                        intermediate_entity_clue_count += 1

                elif source == "query":
                    # Step2å¬å›çš„äº‹é¡¹ï¼šç”Ÿæˆquery â†’ eventçº¿ç´¢
                    # æ„å»ºqueryèŠ‚ç‚¹
                    query_node = Tracker.build_query_node(config)

                    # æ„å»ºäº‹é¡¹èŠ‚ç‚¹
                    event_node = tracker.get_or_create_event_node(
                        event_obj,
                        "rerank",
                        recall_method="query"
                    )

                    # æ·»åŠ  intermediate çº¿ç´¢
                    tracker.add_clue(
                        stage="rerank",
                        from_node=query_node,
                        to_node=event_node,
                        confidence=event_data["similarity_score"],
                        relation="æŸ¥è¯¢å¬å›",
                        display_level="intermediate",  # intermediate çº§åˆ«
                        metadata={
                            "method": "pagerank_query",
                            "pagerank_score": event_data["pagerank"],
                            "similarity_score": event_data["similarity_score"],
                            "rank": rank
                        }
                    )
                    intermediate_query_clue_count += 1

                # æ—¥å¿—ï¼ˆåªæ˜¾ç¤ºå‰10ä¸ªï¼‰
                if rank <= 10:
                    title_preview = event_obj.title[:
                                                    40] if event_obj.title else "æ— æ ‡é¢˜"
                    self.logger.info(
                        f"  Rank {rank}: {event_id[:8]}... | "
                        f"æ¥æº={source} | "
                        f"å®ä½“æ•°={len(source_entities)} | "
                        f"æ ‡é¢˜: {title_preview}"
                    )

            if len(intermediate_events) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(intermediate_events) - 10} ä¸ªäº‹é¡¹)")

            self.logger.info("-" * 80)
            self.logger.info(f"Intermediate çº¿ç´¢ç»Ÿè®¡:")
            self.logger.info(f"  å®ä½“â†’äº‹é¡¹çº¿ç´¢: {intermediate_entity_clue_count} æ¡")
            self.logger.info(f"  æŸ¥è¯¢â†’äº‹é¡¹çº¿ç´¢: {intermediate_query_clue_count} æ¡")
            self.logger.info(
                f"  æ€»çº¿ç´¢æ•°: {intermediate_entity_clue_count + intermediate_query_clue_count} æ¡")
            self.logger.info("=" * 80)

            # ========== ç¬¬äºŒæ­¥ï¼šä¸º Top-K ç”Ÿæˆ final çº¿ç´¢ ==========
            self.logger.info("")
            self.logger.info(
                "ğŸ¯ [äº‹é¡¹çº§ Rerank Final] ç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level=final)")
            self.logger.info(f"   ä¸º Top-{topn} äº‹é¡¹ç”Ÿæˆ final çº¿ç´¢")

            final_clue_count = 0

            for rank, event_data in enumerate(final_events, 1):
                event_obj = event_data["event"]
                event_id = event_data["event_id"]
                source = event_data["source"]
                source_entities = event_data.get("source_entities", [])

                if source == "entity":
                    # ä¸º entity å¬å›çš„äº‹é¡¹ç”Ÿæˆ final çº¿ç´¢ï¼ˆentity â†’ eventï¼‰
                    for entity_id in source_entities:
                        key_info = entity_to_key.get(entity_id)
                        if not key_info:
                            continue

                        # æ„å»ºå®ä½“èŠ‚ç‚¹
                        entity_node = Tracker.build_entity_node({
                            "key_id": entity_id,
                            "id": entity_id,
                            "name": key_info.get("name", ""),
                            "type": key_info.get("type", ""),
                            "description": key_info.get("description", "")
                        })

                        # æ„å»ºäº‹é¡¹èŠ‚ç‚¹
                        event_node = tracker.get_or_create_event_node(
                            event_obj,
                            "rerank",
                            recall_method="entity"
                        )

                        # æ·»åŠ  final çº¿ç´¢
                        tracker.add_clue(
                            stage="rerank",
                            from_node=entity_node,
                            to_node=event_node,
                            confidence=event_data["similarity_score"],
                            relation="æœ€ç»ˆäº‹é¡¹",
                            display_level="final",  # final çº§åˆ«
                            metadata={
                                "method": "final_result",
                                "step": "step6",
                                "pagerank_score": event_data["pagerank"],
                                "similarity_score": event_data["similarity_score"],
                                "rank": rank,
                                "source": "entity"
                            }
                        )
                        final_clue_count += 1

                        self.logger.debug(
                            f"  Final: {entity_id[:8]}... ('{key_info.get('name', '')[:20]}') "
                            f"â†’ {event_id[:8]}... ('{event_obj.title[:30]}', PR={event_data['pagerank']:.4f})"
                        )

                elif source == "query":
                    # ä¸º query å¬å›çš„äº‹é¡¹ç”Ÿæˆ final çº¿ç´¢ï¼ˆquery â†’ eventï¼‰
                    query_node = Tracker.build_query_node(config)

                    event_node = tracker.get_or_create_event_node(
                        event_obj,
                        "rerank",
                        recall_method="query"
                    )

                    # æ·»åŠ  final çº¿ç´¢
                    tracker.add_clue(
                        stage="rerank",
                        from_node=query_node,
                        to_node=event_node,
                        confidence=event_data["similarity_score"],
                        relation="æœ€ç»ˆäº‹é¡¹",
                        display_level="final",  # final çº§åˆ«
                        metadata={
                            "method": "final_result",
                            "step": "step6",
                            "pagerank_score": event_data["pagerank"],
                            "similarity_score": event_data["similarity_score"],
                            "rank": rank,
                            "source": "query"
                        }
                    )
                    final_clue_count += 1

                    self.logger.debug(
                        f"  Final: query '{config.query[:20]}...' "
                        f"â†’ {event_id[:8]}... ('{event_obj.title[:30]}', PR={event_data['pagerank']:.4f})"
                    )

            self.logger.info(
                f"âœ… [äº‹é¡¹çº§ Rerank Final] ç”Ÿæˆäº† {final_clue_count} æ¡æœ€ç»ˆçº¿ç´¢"
            )
            self.logger.info(
                f"âœ… [äº‹é¡¹çº§ Rerank Final] å‰ç«¯å¯æ ¹æ®è¿™äº› final çº¿ç´¢åæ¨å®Œæ•´æ¨ç†è·¯å¾„ï¼š"
            )
            self.logger.info(f"   - Entityå¬å›: query â†’ entity â†’ event")
            self.logger.info(f"   - Queryå¬å›: query â†’ event")
            self.logger.info("")

            # 3. æå–äº‹é¡¹å¯¹è±¡åˆ—è¡¨ï¼ˆä¿æŒPageRanké¡ºåºï¼Œåªè¿”å›Top-Kï¼‰
            result_events = [e["event"] for e in final_events]

            self.logger.info(f"[äº‹é¡¹çº§Step6] å®Œæˆ: è¿”å› Top-{len(result_events)} ä¸ªäº‹é¡¹")

            return result_events

        except Exception as e:
            self.logger.error(f"[äº‹é¡¹çº§Step6] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _generate_query_vector(self, query: str, config: SearchConfig = None) -> List[float]:
        """
        å°†queryè½¬åŒ–æˆå‘é‡

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            config: æœç´¢é…ç½®ï¼ˆç”¨äºç¼“å­˜query_vectorï¼‰

        Returns:
            æŸ¥è¯¢å‘é‡
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_vectorï¼ˆå¦‚æœæœ‰configä¼ å…¥ï¼‰
            if config and config.has_query_embedding and config.query_embedding:
                self.logger.debug(
                    f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(config.query_embedding)}")
                return config.query_embedding

            # ä½¿ç”¨processorç”Ÿæˆå‘é‡
            query_vector = await self.processor.generate_embedding(query)
            self.logger.debug(f"Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}")

            # å¦‚æœæœ‰configï¼Œç¼“å­˜query_vector
            if config:
                config.query_embedding = query_vector
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")

            return query_vector
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

    async def _search_similar_paragraphs(
        self,
        query_vector: List[float],
        source_config_ids: List[str],
        k: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨KNNæœç´¢æŸ¥æ‰¾ç›¸ä¼¼çš„æ–‡æœ¬ç‰‡æ®µ

        éå†æ‰€æœ‰æ•°æ®æºï¼Œåˆå¹¶æœç´¢ç»“æœ

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: æ¯ä¸ªæ•°æ®æºè¿”å›çš„æ•°é‡

        Returns:
            ç›¸ä¼¼æ®µè½åˆ—è¡¨ï¼ˆåˆå¹¶æ‰€æœ‰æ•°æ®æºçš„ç»“æœï¼‰
        """
        try:
            all_paragraphs = []

            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æºï¼Œåˆ™æœç´¢æ‰€æœ‰æ•°æ®æº
            if not source_config_ids:
                self.logger.info("æœªæŒ‡å®šæ•°æ®æºï¼Œæœç´¢æ‰€æœ‰æ•°æ®æº")
                similar_paragraphs = await self.content_repo.search_similar_by_content(
                    query_vector=query_vector,
                    k=k,
                    source_id=None
                )
                all_paragraphs.extend(similar_paragraphs)
            else:
                # éå†æ¯ä¸ªæ•°æ®æºè¿›è¡Œæœç´¢
                self.logger.info(f"éå† {len(source_config_ids)} ä¸ªæ•°æ®æºè¿›è¡ŒKNNæœç´¢")
                for source_id in source_config_ids:
                    try:
                        similar_paragraphs = await self.content_repo.search_similar_by_content(
                            query_vector=query_vector,
                            k=k,
                            source_id=source_id  # ä½¿ç”¨å•ä¸ª source_id
                        )
                        all_paragraphs.extend(similar_paragraphs)
                        self.logger.debug(
                            f"æ•°æ®æº {source_id[:8]}... æ‰¾åˆ° {len(similar_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
                    except Exception as e:
                        self.logger.warning(
                            f"æ•°æ®æº {source_id[:8]}... æœç´¢å¤±è´¥: {e}")
                        continue

            self.logger.info(f"KNNæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_paragraphs)} ä¸ªç›¸ä¼¼æ®µè½")
            return all_paragraphs

        except Exception as e:
            self.logger.error(f"KNNæœç´¢å¤±è´¥: {e}")
            raise AIError(f"KNNæœç´¢å¤±è´¥: {e}") from e

    async def _calculate_cosine_scores(
        self,
        query_vector: List[float],
        paragraphs: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            paragraphs: æ®µè½åˆ—è¡¨

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†å­—å…¸ {section_id: score}
        """
        try:
            if not paragraphs:
                return {}

            cosine_scores = {}

            for paragraph in paragraphs:
                section_id = paragraph.get("section_id")

                # ä»æ®µè½ä¸­è·å–é¢„å­˜çš„å‘é‡
                content_vector = paragraph.get("content_vector")

                if content_vector:
                    # ç›´æ¥ä½¿ç”¨ESè¿”å›çš„é¢„å­˜å‘é‡
                    pass
                else:
                    # å¦‚æœæ²¡æœ‰é¢„å­˜å‘é‡ï¼Œç°åœºç”Ÿæˆï¼ˆåªå¯¹contentç”Ÿæˆï¼Œä¸åŒ…å«æ ‡é¢˜ï¼‰
                    content = paragraph.get(
                        'section_content') or paragraph.get('content', '')
                    if not content.strip():
                        # å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡
                        self.logger.warning(f"æ®µè½ {section_id} å†…å®¹ä¸ºç©ºä¸”æ— é¢„å­˜å‘é‡ï¼Œè·³è¿‡")
                        continue

                    content_vector = await self.processor.generate_embedding(content)

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = await self._cosine_similarity(query_vector, content_vector)
                cosine_scores[section_id] = similarity

            self.logger.info(
                f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ - å…± {len(cosine_scores)} ä¸ªæ®µè½, "
                f"å¹³å‡ç›¸ä¼¼åº¦: {np.mean(list(cosine_scores.values())):.4f}"
            )

            return cosine_scores

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {}

    async def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦
        """
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            v1 = np.array(vec1)
            v2 = np.array(vec2)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            dot_product = np.dot(v1, v2)
            norm_v1 = np.linalg.norm(v1)
            norm_v2 = np.linalg.norm(v2)

            if norm_v1 == 0 or norm_v2 == 0:
                return 0.0

            similarity = dot_product / (norm_v1 * norm_v2)
            return float(similarity)

        except Exception as e:
            self.logger.error(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—é”™è¯¯: {e}")
            return 0.0
    
    async def _build_response(
        self,
        config: SearchConfig,
        key_final: List[Dict[str, Any]],
        events: List[SourceEvent],
        event_to_clues: Dict[str, List[Dict]]
    ) -> Dict[str, Any]:
        """
        æ„å»ºæ–°çš„å“åº”æ ¼å¼

        Args:
            config: æœç´¢é…ç½®å¯¹è±¡
            key_final: å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey-finalï¼‰
            events: äº‹é¡¹åˆ—è¡¨
            event_to_clues: äº‹é¡¹IDåˆ°å®ä½“åˆ—è¡¨çš„æ˜ å°„ {event_id: [entity1, entity2, ...]}

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - events: äº‹é¡¹å¯¹è±¡åˆ—è¡¨
                - clues: å¬å›çº¿ç´¢ä¿¡æ¯
                    - origin_query: åŸå§‹æŸ¥è¯¢
                    - final_query: LLMé‡å†™åçš„æŸ¥è¯¢ï¼ˆå¦‚æœæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                    - query_entities: æŸ¥è¯¢å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities: å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œå»é™¤query_entitiesä¸­çš„å€¼ï¼‰
                    - event_entities: äº‹é¡¹ä¸å®ä½“çš„å…³è”æ˜ å°„è¡¨ {event_id: [entity1, entity2, ...]}
        """
        # 1. å¤„ç† query_entitiesï¼šå°† config.query_recalled_keys ä¸­çš„ key_id æ”¹ä¸º id
        query_entities = []
        query_key_ids = set()  # ç”¨äºåç»­è¿‡æ»¤

        for key in config.query_recalled_keys:
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_id = key_copy.pop("key_id")
                key_copy["id"] = key_id
                query_key_ids.add(key_id)
            query_entities.append(key_copy)

        # 2. å¤„ç† recall_entitiesï¼šå°† key_final ä¸­çš„ key_id æ”¹ä¸º idï¼Œå¹¶è¿‡æ»¤æ‰ query_entities ä¸­çš„å€¼
        recall_entities = []

        for key in key_final:
            # è·å– key_id ç”¨äºè¿‡æ»¤åˆ¤æ–­
            key_id = key.get("key_id")

            # å¦‚æœè¿™ä¸ª key_id åœ¨ query_recalled_keys ä¸­ï¼Œåˆ™è·³è¿‡
            if key_id in query_key_ids:
                continue

            # å¤åˆ¶å¹¶é‡å‘½å key_id ä¸º id
            key_copy = key.copy()
            if "key_id" in key_copy:
                key_copy["id"] = key_copy.pop("key_id")
            recall_entities.append(key_copy)

        # 3. åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿”å› final_query
        # å¦‚æœå¯ç”¨äº†queryé‡å†™åŠŸèƒ½ï¼ˆenable_query_rewrite=Trueï¼‰ï¼Œåˆ™è¿”å›é‡å†™åçš„query
        # å¦åˆ™è¿”å› None
        final_query = config.query if config.enable_query_rewrite and config.recall.use_fast_mode == False else None

        # 4. è¿‡æ»¤ event_to_cluesï¼Œåªä¿ç•™æœ€ç»ˆè¿”å›çš„äº‹é¡¹
        final_event_ids = {event.id for event in events}
        filtered_event_entities = {
            event_id: clues
            for event_id, clues in event_to_clues.items()
            if event_id in final_event_ids
        }

        # 5. æ„å»ºå“åº”
        response = {
            "events": events,  # äº‹é¡¹åˆ—è¡¨
            "clues": {
                "origin_query": config.original_query,  # åŸå§‹queryï¼ˆé‡å†™å‰ï¼‰
                "final_query": final_query,  # é‡å†™åçš„queryï¼ˆæ²¡æœ‰é‡å†™åˆ™ä¸ºNoneï¼‰
                "query_entities": query_entities,
                "recall_entities": recall_entities,
                "event_entities": filtered_event_entities  # åªåŒ…å«æœ€ç»ˆè¿”å›äº‹é¡¹çš„æº¯æºä¿¡æ¯
            }
        }

        self.logger.info(
            f"å“åº”æ„å»ºå®Œæˆ: origin_query='{config.original_query}', "
            f"final_query='{final_query}', "
            f"query_entities={len(query_entities)}ä¸ª, "
            f"recall_entities={len(recall_entities)}ä¸ª, "
            f"events={len(events)}ä¸ª, "
            f"event_entitiesæ˜ å°„={len(filtered_event_entities)}ä¸ª (è¿‡æ»¤å‰={len(event_to_clues)}ä¸ª)"
        )

        return response
