"""
å®ä½“å¬å›æ¨¡å—ï¼ˆRecallï¼‰

å®ç°8æ­¥éª¤çš„å¤åˆæœç´¢ç®—æ³•ï¼š
1. queryæ‰¾keyï¼šLLMæŠ½å–queryçš„ç»“æ„åŒ–å±æ€§ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°å…³è”å®ä½“
2. keyæ‰¾eventï¼šé€šè¿‡[key-query-related]ç”¨sqlæ‰¾åˆ°æ‰€æœ‰å…³è”äº‹é¡¹
3. queryå†æ‰¾eventï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦åœ¨æ‰¾åˆ°queryå…³è”äº‹é¡¹
4. è¿‡æ»¤Eventï¼š[Event-query-related]å’Œ[Event-key-query-related]å–äº¤é›†
5. è®¡ç®—event-keyæƒé‡å‘é‡ï¼šæ ¹æ®æ¯ä¸ªeventåŒ…å«keyçš„æƒ…å†µè®¡ç®—æƒé‡
6. è®¡ç®—event-key-queryæƒé‡å‘é‡ï¼šå°†(event-key)*(e1)å¾—åˆ°æ–°çš„æƒé‡å‘é‡
7. åå‘è®¡ç®—keyæƒé‡å‘é‡ï¼šæ ¹æ®eventæƒé‡åå‘è®¡ç®—keyé‡è¦æ€§
8. æå–é‡è¦çš„keyï¼šé€šè¿‡é˜ˆå€¼æˆ–top-næ–¹å¼æå–é‡è¦key
"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np

from sqlalchemy import select
from sqlalchemy.orm import selectinload

from sag.core.ai.base import BaseLLMClient
from sag.core.ai.models import LLMMessage, LLMRole
from sag.core.prompt.manager import PromptManager
from sag.core.storage.elasticsearch import get_es_client
from sag.core.storage.repositories.entity_repository import EntityVectorRepository
from sag.core.storage.repositories.event_repository import EventVectorRepository
from sag.db import SourceEvent, Entity, EventEntity, get_session_factory
from sag.exceptions import AIError
from sag.modules.load.processor import DocumentProcessor
from sag.modules.search.config import SearchConfig
from sag.modules.search.tracker import Tracker  # ğŸ†• ç»Ÿä¸€ä½¿ç”¨Tracker
from sag.utils import get_logger

logger = get_logger("search.recall")


@dataclass
class RecallResult:
    """å®ä½“å¬å›ç»“æœ"""
    # æŸ¥è¯¢è¿½è¸ªä¿¡æ¯
    original_query: str  # åŸå§‹æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•å’Œè¿½è¸ªï¼‰

    # æœ€ç»ˆç»“æœ
    # [{"key": str, "weight": float, "steps": List[int]}, ...]
    key_final: List[Dict[str, Any]]

    # ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    key_query_related: List[Dict[str, Any]]  # æ­¥éª¤1ç»“æœ
    event_key_query_related: List[str]       # æ­¥éª¤2ç»“æœ
    event_query_related: List[Dict[str, Any]]  # æ­¥éª¤3ç»“æœ
    event_related: List[str]                 # æ­¥éª¤4ç»“æœ
    key_related: List[str]                   # æ­¥éª¤4ç»“æœ
    event_key_weights: Dict[str, float]      # æ­¥éª¤5ç»“æœ
    event_key_query_weights: Dict[str, float]  # æ­¥éª¤6ç»“æœ
    key_event_weights: Dict[str, float]      # æ­¥éª¤7ç»“æœ


class RecallSearcher:
    """å®ä½“å¬å›æœç´¢å™¨ - å®ç°8æ­¥éª¤å¤åˆæœç´¢ç®—æ³•"""

    def __init__(
        self,
        llm_client: BaseLLMClient,
        prompt_manager: PromptManager,
    ):
        """
        åˆå§‹åŒ–å®ä½“å¬å›æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.session_factory = get_session_factory()
        self.logger = get_logger("search.recall")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.entity_repo = EntityVectorRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºç”Ÿæˆå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "å®ä½“å¬å›æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    async def search(self, config: SearchConfig) -> RecallResult:
        """
        æ‰§è¡Œ8æ­¥éª¤æœç´¢ç®—æ³•

        Args:
            config: æœç´¢é…ç½®

        Returns:
            å®ä½“å¬å›ç»“æœ
        """
        try:
            # ä¿å­˜åŸå§‹queryç”¨äºç»“æœè¿½è¸ªï¼ˆå¿…é¡»åœ¨step1ä¹‹å‰ï¼‰
            original_query = config.query

            # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨
            tracker = Tracker(config)

            self.logger.info(
                f"å¼€å§‹å®ä½“å¬å›ï¼šsource_config_ids={config.get_source_config_ids()}, query={config.query}"
            )

            # === æ­¥éª¤1: queryæ‰¾keyï¼ˆè¯­ä¹‰æ‰©å±•ï¼‰ ===
            key_query_related, k1_weights = await self._step1_query_to_keys(config)
            self.logger.info(f"æ­¥éª¤1å®Œæˆï¼šæ‰¾åˆ° {len(key_query_related)} ä¸ªç›¸å…³key")

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šquery â†’ entityï¼ˆä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹æ„å»ºï¼‰
            for entity in key_query_related:
                # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                entity_weight = entity.get("weight")
                metadata = {
                    "method": "vector_search",
                    "step": "step1",
                    # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                    "source_attribute": entity.get("source_attribute")
                }
                # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                if entity_weight is not None:
                    metadata["weight"] = entity_weight

                tracker.add_clue(
                    stage="recall",
                    from_node=Tracker.build_query_node(config),
                    to_node=Tracker.build_entity_node(entity),
                    confidence=entity.get("similarity", 0.0),  # ç»Ÿä¸€ä½¿ç”¨similarity
                    metadata=metadata
                )

            # ğŸ” æ˜¾ç¤ºå¬å›å®ä½“çš„è¯¦ç»†ä¿¡æ¯
            if key_query_related:
                self.logger.info(f"ğŸ“‹ æ­¥éª¤1å¬å›å®ä½“è¯¦æƒ… (å…±{len(key_query_related)}ä¸ª):")
                for idx, entity in enumerate(key_query_related, 1):
                    self.logger.info(
                        f"  {idx}. å®ä½“ID: {entity.get('entity_id')}, "
                        f"åç§°: '{entity.get('name')}', "
                        f"ç±»å‹: {entity.get('type')}, "
                        f"ç›¸ä¼¼åº¦: {entity.get('similarity', 0.0):.4f}, "
                        f"æ¥æºå±æ€§: '{entity.get('source_attribute')}'"
                    )

            # ğŸ” Step1è¯Šæ–­æ—¥å¿—
            if key_query_related:
                top3 = sorted(key_query_related, key=lambda x: x.get(
                    "similarity", 0), reverse=True)[:3]
                top3_info = [(e['name'], e.get('similarity', 0)) for e in top3]
                self.logger.info(
                    f"ğŸ” [Step1è¯Šæ–­] å¬å›å®ä½“æ•°={len(key_query_related)}, "
                    f"Top3: {top3_info}, "
                    f"çº¿ç´¢æ•°={len([c for c in config.all_clues if c.get('step') == 'step1'])}"
                )
            else:
                self.logger.warning("âš ï¸ [Step1è¯Šæ–­] æœªå¬å›ä»»ä½•å®ä½“ï¼Œåç»­æ­¥éª¤å¯èƒ½æ— ç»“æœ")

            # å­˜å‚¨queryå¬å›çš„æ‰€æœ‰keyåˆ°configä¸­
            config.query_recalled_keys = key_query_related
            self.logger.debug(
                f"å·²å°† {len(key_query_related)} ä¸ªqueryå¬å›çš„keyå­˜å‚¨åˆ°config.query_recalled_keys")

            # === æ­¥éª¤2: keyæ‰¾eventï¼ˆç²¾å‡†åŒ¹é…ï¼‰ ===
            event_key_query_related = await self._step2_keys_to_events(config, key_query_related)
            self.logger.info(
                f"æ­¥éª¤2å®Œæˆï¼šæ‰¾åˆ° {len(event_key_query_related)} ä¸ªkeyç›¸å…³event")

            # === æ­¥éª¤3: queryå†æ‰¾eventï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰ ===
            event_query_related, e1_weights = await self._step3_query_to_events(config)
            self.logger.info(
                f"æ­¥éª¤3å®Œæˆï¼šæ‰¾åˆ° {len(event_query_related)} ä¸ªqueryç›¸å…³event")

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šquery â†’ eventï¼ˆéœ€è¦æŸ¥è¯¢å®Œæ•´çš„eventå¯¹è±¡ï¼‰
            if event_query_related:
                async with self.session_factory() as session:
                    event_ids_step3 = [e["event_id"]
                                       for e in event_query_related]
                    events_query_step3 = select(SourceEvent).where(
                        SourceEvent.id.in_(event_ids_step3))
                    events_result_step3 = await session.execute(events_query_step3)
                    events_step3 = {
                        event.id: event for event in events_result_step3.scalars().all()}

                    for event_dict in event_query_related:
                        event_obj = events_step3.get(event_dict["event_id"])
                        if event_obj:
                            tracker.add_clue(
                                stage="recall",
                                from_node=Tracker.build_query_node(config),
                                to_node=tracker.get_or_create_event_node(event_obj, "recall"),
                                confidence=event_dict.get("similarity", 0.0),
                                display_level="intermediate",  # ğŸ†• ä¸­é—´ç»“æœ
                                metadata={"method": "vector_search", "step": "step3"}
                            )

            # === æ­¥éª¤4: è¿‡æ»¤Eventï¼ˆç²¾å‡†ç­›é€‰ï¼‰ ===
            event_related, key_related = await self._step4_filter_events(
                event_key_query_related, event_query_related, key_query_related
            )
            self.logger.info(
                f"æ­¥éª¤4å®Œæˆï¼šè¿‡æ»¤å {len(event_related)} ä¸ªevent, {len(key_related)} ä¸ªkey")

            # ğŸ” æ˜¾ç¤ºkeyè¿‡æ»¤æƒ…å†µ
            original_key_count = len(key_query_related)
            retained_keys_count = len(key_related)
            lost_keys_count = original_key_count - retained_keys_count

            # é¿å…é™¤é›¶é”™è¯¯
            if original_key_count > 0:
                retention_rate = (retained_keys_count / original_key_count * 100)
                self.logger.info(
                    f"ğŸ” [Step4] Keyè¿‡æ»¤ç»“æœ: "
                    f"æ­¥éª¤1å¬å›={original_key_count}ä¸ª â†’ "
                    f"æ­¥éª¤4è¿‡æ»¤å={retained_keys_count}ä¸ª "
                        f"(ä¿ç•™ç‡={retention_rate:.1f}%, "
                    f"è¿‡æ»¤æ‰{lost_keys_count}ä¸ª)"
                )
            else:
                self.logger.info(
                    f"ğŸ” [Step4] Keyè¿‡æ»¤ç»“æœ: "
                    f"æ­¥éª¤1å¬å›={original_key_count}ä¸ª â†’ "
                    f"æ­¥éª¤4è¿‡æ»¤å={retained_keys_count}ä¸ª"
                )

            if lost_keys_count > 0:
                self.logger.info(
                    f"ğŸ“Œ [Step4] è¿‡æ»¤æ‰çš„{lost_keys_count}ä¸ªkeyæ˜¯å› ä¸ºï¼šå®ƒä»¬å…³è”çš„eventsä¸åœ¨äº¤é›†ä¸­ "
                    f"(å³è¿™äº›keyçš„eventsä¸queryç›¸ä¼¼åº¦ä¸å¤Ÿé«˜)"
                )

            # ğŸ” æ˜¾ç¤ºæ­¥éª¤4ä¿ç•™çš„keyè¯¦æƒ…
            if key_related:
                # ä»æ­¥éª¤1çš„ç»“æœä¸­è¿‡æ»¤å‡ºä¿ç•™çš„keyä¿¡æ¯
                key_related_set = set(key_related)
                retained_key_infos = [
                    k for k in key_query_related if k["entity_id"] in key_related_set
                ]

                self.logger.info(f"ğŸ“‹ æ­¥éª¤4è¿‡æ»¤åä¿ç•™çš„keyè¯¦æƒ… (å…±{len(retained_key_infos)}ä¸ª):")
                for idx, key_info in enumerate(retained_key_infos, 1):
                    self.logger.info(
                        f"  {idx}. å®ä½“ID: {key_info['entity_id']}, "
                        f"åç§°: '{key_info['name']}', "
                        f"ç±»å‹: {key_info['type']}, "
                        f"åŸå§‹ç›¸ä¼¼åº¦: {key_info.get('similarity', 0.0):.4f}, "
                        f"æ¥æºå±æ€§: '{key_info.get('source_attribute', 'N/A')}'"
                    )
            else:
                self.logger.warning("âš ï¸ æ­¥éª¤4åæ²¡æœ‰ä¿ç•™ä»»ä½•keyï¼Œåç»­æ­¥éª¤å°†æ— ç»“æœ")

            # ğŸ” Step4è¯Šæ–­æ—¥å¿—
            query_event_ids = {event["event_id"]
                               for event in event_query_related}
            key_event_ids = set(event_key_query_related)
            self.logger.info(
                f"ğŸ” [Step4è¯Šæ–­] Eventäº¤é›†è¿‡æ»¤: "
                f"queryå¬å›={len(query_event_ids)}, "
                f"keyå¬å›={len(key_event_ids)}, "
                f"äº¤é›†={len(event_related)}, "
                f"äº¤é›†ç‡={len(event_related) / max(len(query_event_ids), 1):.1%}"
            )
            self.logger.info(
                f"ğŸ” [Step4è¯Šæ–­] Keyè¿‡æ»¤: "
                f"è¾“å…¥={len(key_query_related)} (æ­¥éª¤1å¬å›), "
                f"è¾“å‡º={len(key_related)} (eventsåœ¨äº¤é›†ä¸­çš„key)"
            )

            # === æ­¥éª¤5: è®¡ç®—event-keyæƒé‡å‘é‡ ===
            event_key_weights = await self._step5_calculate_event_key_weights(
                event_related, key_related, k1_weights
            )
            self.logger.info(
                f"æ­¥éª¤5å®Œæˆï¼šè®¡ç®—äº† {len(event_key_weights)} ä¸ªeventçš„keyæƒé‡")

            # === æ­¥éª¤6: è®¡ç®—event-key-queryæƒé‡å‘é‡ ===
            event_key_query_weights = await self._step6_calculate_event_key_query_weights(
                event_key_weights, e1_weights
            )
            self.logger.info(
                f"æ­¥éª¤6å®Œæˆï¼šè®¡ç®—äº† {len(event_key_query_weights)} ä¸ªeventçš„å¤åˆæƒé‡")

            # === æ­¥éª¤7: åå‘è®¡ç®—keyæƒé‡å‘é‡ ===
            key_event_weights = await self._step7_calculate_key_event_weights(
                event_related, key_related, event_key_query_weights
            )
            self.logger.info(f"æ­¥éª¤7å®Œæˆï¼šè®¡ç®—äº† {len(key_event_weights)} ä¸ªkeyçš„åå‘æƒé‡")

            # ğŸ” Step7è¯Šæ–­æ—¥å¿—
            if key_event_weights:
                weights = list(key_event_weights.values())
                self.logger.info(
                    f"ğŸ” [Step7è¯Šæ–­] Keyæƒé‡åˆ†å¸ƒ: "
                    f"æ€»æ•°={len(weights)}, "
                    f"æœ€å¤§={max(weights):.4f}, "
                    f"æœ€å°={min(weights):.4f}, "
                    f"å¹³å‡={sum(weights)/len(weights):.4f}"
                )
            else:
                self.logger.warning("âš ï¸ [Step7è¯Šæ–­] æœªè®¡ç®—å‡ºä»»ä½•keyæƒé‡ï¼ŒStep8å°†æ— ç»“æœ")

            # === æ­¥éª¤8: æå–é‡è¦çš„key ===
            key_final = await self._step8_extract_important_keys(
                key_event_weights, config
            )
            self.logger.info(f"æ­¥éª¤8å®Œæˆï¼šæå–äº† {len(key_final)} ä¸ªé‡è¦key")

            # ğŸ” åˆ†ææœ€ç»ˆkeyçš„è¿‡æ»¤æƒ…å†µ
            if key_final:
                self.logger.info(
                    f"ğŸ” [Step8] æœ€ç»ˆç»“æœ: "
                    f"æ­¥éª¤1å¬å›={len(key_query_related)}ä¸ª â†’ "
                    f"æ­¥éª¤4è¿‡æ»¤å={len(key_related)}ä¸ª â†’ "
                    f"æ­¥éª¤8æå–={len(key_final)}ä¸ª"
                )

            # ğŸ” Step8è¯Šæ–­æ—¥å¿—ï¼ˆæœ€å…³é”®ï¼ï¼‰
            input_keys = len(key_event_weights)
            output_keys = len(key_final)
            recall_rate = output_keys / max(input_keys, 1)

            self.logger.info(
                f"ğŸ” [Step8è¯Šæ–­] æœ€ç»ˆè¿‡æ»¤: "
                f"è¾“å…¥={input_keys}, è¾“å‡º={output_keys}, å¬å›ç‡={recall_rate:.1%}"
            )
            self.logger.info(
                f"ğŸ” [Step8è¯Šæ–­] é…ç½®å‚æ•°: "
                f"top_n_keys={config.recall.final_entity_count}, "
                f"final_key_threshold={config.recall.entity_weight_threshold}"
            )

            # === ğŸ†• æ­¥éª¤8å®Œæˆåï¼šç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level="final") ===
            # ä¸ºæœ€ç»ˆä¿ç•™çš„entityç”Ÿæˆ query â†’ entity çº¿ç´¢
            # å‰ç«¯ç²¾ç®€æ¨¡å¼ï¼šåªæ˜¾ç¤ºè¿™äº› final çº¿ç´¢
            # å‰ç«¯å¯ä»¥æ ¹æ® final çº¿ç´¢åæ¨å®Œæ•´è·¯å¾„ï¼ˆquery â†’ extracted_entity â†’ entityï¼‰
            if key_final:
                self.logger.info(f"ğŸ¯ [Step8] ç”Ÿæˆ {len(key_final)} æ¡æœ€ç»ˆçº¿ç´¢ (display_level=final)")

                for key in key_final:
                    # ä» key_query_related ä¸­æ‰¾åˆ°åŸå§‹entityä¿¡æ¯
                    original_entity = next(
                        (e for e in key_query_related if e["entity_id"] == key["key_id"]),
                        None
                    )

                    if original_entity:
                        # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        entity_weight = key.get("weight")
                        metadata = {
                            "method": "final_result",
                            "step": "step8",
                            "steps": key.get("steps", [1]),
                            "source_attribute": original_entity.get("source_attribute")
                        }
                        # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                        if entity_weight is not None:
                            metadata["weight"] = entity_weight

                        tracker.add_clue(
                            stage="recall",
                            from_node=Tracker.build_query_node(config),
                            to_node=Tracker.build_entity_node(original_entity),
                            confidence=original_entity.get("similarity", 0.0),  # ç»Ÿä¸€ä½¿ç”¨similarity
                            relation="è¯­ä¹‰ç›¸ä¼¼",
                            display_level="final",  # ğŸ†• æ ‡è®°ä¸ºæœ€ç»ˆç»“æœ
                            metadata=metadata
                        )
                    else:
                        self.logger.warning(
                            f"âš ï¸ [Step8] æ— æ³•ä¸º key_id={key['key_id']} ç”Ÿæˆæœ€ç»ˆçº¿ç´¢: "
                            f"åœ¨ key_query_related ä¸­æ‰¾ä¸åˆ°åŸå§‹ä¿¡æ¯"
                        )

                self.logger.info(
                    f"âœ… [Step8] æœ€ç»ˆçº¿ç´¢ç”Ÿæˆå®Œæˆï¼Œå‰ç«¯å¯æ ¹æ®è¿™äº› final çº¿ç´¢åæ¨å®Œæ•´æ¨ç†è·¯å¾„"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ [Step8] æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ€ç»ˆçº¿ç´¢ï¼key_final ä¸ºç©ºã€‚"
                    f"è¿™å¯èƒ½å¯¼è‡´å‰ç«¯ç²¾ç®€æ¨¡å¼å›¾è°±ä¸ºç©ºã€‚"
                    f"å»ºè®®æ£€æŸ¥é…ç½®å‚æ•°ï¼štop_n_keys={config.recall.final_entity_count}, "
                    f"final_key_threshold={config.recall.entity_weight_threshold}"
                )


            # å¦‚æœå¬å›ç‡è¿‡ä½ï¼Œå‘å‡ºè­¦å‘Šå¹¶æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„å®ä½“
            if recall_rate < 0.3 and input_keys > 0:
                self.logger.warning(
                    f"âš ï¸ [Step8è¯Šæ–­] å¬å›ç‡è¿‡ä½ ({recall_rate:.1%})ï¼"
                    f"å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°ï¼šå¢å¤§top_n_keysæˆ–é™ä½final_key_threshold"
                )

                # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„å®ä½“ï¼ˆæƒé‡æœ€é«˜çš„5ä¸ªï¼‰
                if input_keys - output_keys > 0:
                    sorted_weights = sorted(
                        key_event_weights.items(), key=lambda x: x[1], reverse=True)
                    filtered_out = sorted_weights[output_keys:output_keys + 5]

                    if filtered_out:
                        filtered_info = [(kid, w) for kid, w in filtered_out]
                        self.logger.warning(
                            f"âš ï¸ [Step8è¯Šæ–­] è¢«è¿‡æ»¤æ‰çš„é«˜æƒé‡å®ä½“ï¼ˆå‰5ä¸ªï¼‰: "
                            f"{filtered_info}"
                        )

            # === æ„å»ºRecallé˜¶æ®µçº¿ç´¢ ===
            # ä½¿ç”¨config.query_recalled_keysï¼ˆå·²åœ¨step8ä¸­è¿‡æ»¤å¹¶æ›´æ–°ä¸ºkey_finalæ ¼å¼ï¼‰
            recall_clues = await self._build_recall_clues(
                config, config.query_recalled_keys)
            config.recall_clues = recall_clues
            self.logger.info(
                f"âœ¨ æ„å»ºäº† {len(recall_clues)} æ¡Recallçº¿ç´¢ (query â†’ entity), "
                f"è¿™äº›æ˜¯æ­¥éª¤1ç›´æ¥å¬å›ä¸”åœ¨æœ€ç»ˆç»“æœä¸­çš„å®ä½“"
            )

            result = RecallResult(
                original_query=original_query,
                key_final=key_final,
                key_query_related=key_query_related,
                event_key_query_related=event_key_query_related,
                event_query_related=event_query_related,
                event_related=event_related,
                key_related=key_related,
                event_key_weights=event_key_weights,
                event_key_query_weights=event_key_query_weights,
                key_event_weights=key_event_weights,
            )

            self.logger.info(
                f"å®ä½“å¬å›å®Œæˆï¼šè¿”å› {len(key_final)} ä¸ªé‡è¦key",
                extra={
                    "source_config_ids": config.source_config_ids,
                    "query": config.query,
                    "final_keys_count": len(key_final),
                },
            )

            return result

        except Exception as e:
            self.logger.error(f"å®ä½“å¬å›å¤±è´¥: {e}", exc_info=True)
            raise

    # === æ­¥éª¤å®ç°æ–¹æ³• ===

    async def _step1_query_to_keys(
        self, config: SearchConfig
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        æ­¥éª¤1: queryæ‰¾keyï¼ˆè¯­ä¹‰æ‰©å±•ï¼‰
        LLMæŠ½å–queryçš„ç»“æ„åŒ–å±æ€§ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°å…³è”å®ä½“

        å¦‚æœå¯ç”¨äº†queryé‡å†™ï¼Œä¼šç›´æ¥ä¿®æ”¹config.queryä¸ºé‡å†™åçš„queryï¼Œ
        è¿™æ ·åç»­çš„æ¨¡å—éƒ½ä¼šè‡ªåŠ¨ä½¿ç”¨é‡å†™åçš„query

        Returns:
            Tuple[List[Dict[str, Any]], Dict[str, float]]:
                (key_query_related, k1_weights)
        """
        # TODO: å®Œå–„LLMå±æ€§æŠ½å–å®ç°
        # å½“å‰å®ç°ï¼š
        # 1. ä½¿ç”¨ç®€å•è§„åˆ™ä»queryä¸­æå–å±æ€§ï¼ˆå ä½ç¬¦ï¼‰
        # 2. å°†å±æ€§è½¬æ¢ä¸ºå‘é‡ï¼ˆå ä½ç¬¦å®ç°ï¼‰
        # 3. ä½¿ç”¨å‘é‡æœç´¢æ‰¾åˆ°ç›¸ä¼¼å®ä½“

        self.logger.info(
            f"æ­¥éª¤1å¼€å§‹: query='{config.query}', "
            f"key_similarity_threshold={config.recall.entity_similarity_threshold}, "
            f"max_keys={config.recall.max_entities}, "
            f"source_config_ids={config.get_source_config_ids()}, "
            f"use_fast_mode={config.recall.use_fast_mode}"
        )

        # å¿«é€Ÿæ¨¡å¼ï¼šç›´æ¥ç”¨queryçš„embeddingå¬å›keyï¼Œè·³è¿‡LLMå±æ€§æŠ½å–å’Œqueryé‡å†™
        if config.recall.use_fast_mode:
            self.logger.info("ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡LLMå±æ€§æŠ½å–ï¼Œç›´æ¥ä½¿ç”¨query embeddingå¬å›key")

            # å¿«é€Ÿæ¨¡å¼ä¸‹ä¹Ÿéœ€è¦è®¾ç½®origin_queryï¼ˆæœªé‡å†™ï¼‰
            config.original_query = config.query

            try:
                # ç”ŸæˆåŸå§‹queryçš„embedding
                self.logger.debug(f"å¼€å§‹ä¸ºquery '{config.query}' ç”Ÿæˆå‘é‡...")
                query_embedding = await self.processor.generate_embedding(config.query)
                self.logger.info(f"âœ… Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")

                # ç¼“å­˜query_embeddingåˆ°configï¼Œé¿å…é‡å¤ç”Ÿæˆ
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")

                # ç›´æ¥æœç´¢entityï¼ˆä¸é™åˆ¶entity_typeï¼‰
                self.logger.debug(
                    f"å¼€å§‹å‘é‡æœç´¢: k={config.recall.vector_top_k}, source_config_ids={config.get_source_config_ids()}")
                similar_entities = await self.entity_repo.search_similar(
                    query_vector=query_embedding,
                    k=config.recall.vector_top_k,
                    source_config_ids=config.get_source_config_ids(),  # ä½¿ç”¨å¤šæºæ”¯æŒ
                    entity_type=None,  # ä¸é™åˆ¶ç±»å‹
                    include_type_threshold=True,
                )

                self.logger.info(f"ğŸ“Š å¿«é€Ÿæ¨¡å¼æœç´¢åˆ° {len(similar_entities)} ä¸ªå€™é€‰å®ä½“")

                # è¿‡æ»¤é˜ˆå€¼
                key_query_related = []
                k1_weights = {}
                passed_count = 0

                for entity in similar_entities:
                    similarity = float(entity.get("_score", 0.0))
                    type_threshold = entity.get("type_threshold", 0.800)
                    final_threshold = max(
                        config.recall.entity_similarity_threshold, type_threshold)

                    if similarity >= final_threshold:
                        key_query_related.append({
                            "entity_id": entity["entity_id"],
                            "name": entity["name"],
                            "type": entity["type"],
                            "similarity": similarity,
                            "source_attribute": config.query,  # ç›´æ¥ä½¿ç”¨åŸå§‹query
                            "type_threshold": type_threshold,
                            "final_threshold": final_threshold,
                        })
                        k1_weights[entity["entity_id"]] = similarity
                        passed_count += 1

                self.logger.info(
                    f"ğŸ“ˆ å¿«é€Ÿæ¨¡å¼é˜ˆå€¼è¿‡æ»¤ç»“æœ: "
                    f"é€šè¿‡ {passed_count}/{len(similar_entities)}"
                )

                # å»é‡å¹¶é™åˆ¶æ•°é‡
                seen_entities = set()
                unique_keys = []
                for key_info in key_query_related:
                    entity_id = key_info["entity_id"]
                    if entity_id not in seen_entities:
                        seen_entities.add(entity_id)
                        unique_keys.append(key_info)

                key_query_related = unique_keys[:config.recall.max_entities]

                self.logger.info(
                    f"ğŸ“‹ å¿«é€Ÿæ¨¡å¼å®Œæˆ: æœ€ç»ˆè¿”å› {len(key_query_related)} ä¸ªkey"
                )

                if len(key_query_related) > 0:
                    top_entities = sorted(
                        key_query_related, key=lambda x: x["similarity"], reverse=True)[:3]
                    top_info = [
                        f"'{e['name']}'({e['type']}, {e['similarity']:.3f})"
                        for e in top_entities
                    ]
                    self.logger.info(f"ğŸ† Top 3 ç›¸ä¼¼å®ä½“: {', '.join(top_info)}")

                return key_query_related, k1_weights

            except Exception as e:
                self.logger.error(f"âŒ å¿«é€Ÿæ¨¡å¼å¤±è´¥: {e}")
                import traceback
                self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise

        # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨ï¼ˆç»Ÿä¸€æ–¹å¼ï¼‰
        tracker = Tracker(config)

        # 1. ä»queryä¸­æŠ½å–ç»“æ„åŒ–å±æ€§ï¼Œå¯é€‰åœ°è¿›è¡Œqueryé‡å†™
        # å‘åå…¼å®¹æ€§æ£€æŸ¥ï¼šç¡®ä¿é…ç½®é¡¹å­˜åœ¨
        enable_rewrite = getattr(config, 'enable_query_rewrite', True)

        # ä¿å­˜åŸå§‹query
        original_query = config.query

        query_attributes, rewritten_query = await self._extract_attributes_from_query(
            config.query,
            enable_rewrite=enable_rewrite
        )

        # å¦‚æœå¯ç”¨äº†é‡å†™åŠŸèƒ½
        if enable_rewrite:
            if rewritten_query:
                # ä¿å­˜åŸå§‹queryåˆ°origin_query
                config.original_query = original_query
                # å°†é‡å†™åçš„queryä¿å­˜åˆ°query
                config.query = rewritten_query
                self.logger.info(
                    f"ğŸ”„ Queryé‡å†™: origin='{original_query}' â†’ query='{rewritten_query}'")

                # ğŸ†• è®°å½• prepare é˜¶æ®µçº¿ç´¢ï¼šqueryé‡å†™
                tracker.add_clue(
                    stage="prepare",
                    from_node=Tracker.build_query_node(
                        config, use_origin=True),  # åŸå§‹query
                    to_node=Tracker.build_query_node(
                        config, use_origin=False),   # é‡å†™åquery
                    confidence=1.0,
                    relation="é‡å†™ç”¨æˆ·è¯·æ±‚",
                    metadata={"method": "llm_rewrite"}
                )
            else:
                # æ²¡æœ‰é‡å†™ç»“æœï¼Œorigin_queryå’Œqueryéƒ½ä½¿ç”¨åŸå§‹query
                config.original_query = original_query
                self.logger.debug(f"ğŸ“ Queryæœªé‡å†™ï¼Œä¿æŒåŸæ ·: '{config.query}'")
        else:
            # æœªå¯ç”¨é‡å†™åŠŸèƒ½ï¼Œorigin_queryå’Œqueryéƒ½ä½¿ç”¨åŸå§‹query
            config.original_query = original_query
            self.logger.debug(f"ğŸ“ Queryé‡å†™åŠŸèƒ½æœªå¯ç”¨ï¼Œä½¿ç”¨åŸå§‹query: '{config.query}'")

        self.logger.info(
            f"æŠ½å–åˆ° {len(query_attributes)} ä¸ªå±æ€§: {[attr['name'] for attr in query_attributes]}")

        # ğŸ†• è®°å½• prepare é˜¶æ®µçº¿ç´¢ï¼šå±æ€§æå–
        if query_attributes:
            query_node = Tracker.build_query_node(config)
            for attr in query_attributes:
                entity_node = Tracker.build_extracted_entity_node(attr)
                tracker.add_clue(
                    stage="prepare",
                    from_node=query_node,
                    to_node=entity_node,
                    confidence=await self._importance_to_confidence(
                        attr.get("importance", "medium")),
                    relation="è¯·æ±‚çš„å±æ€§æå–",
                    metadata={
                        "method": "llm_extraction",
                        "attribute_type": attr.get("type"),
                        "importance": attr.get("importance", "medium")
                    }
                )

        # è¯¦ç»†è®°å½•æŠ½å–çš„å±æ€§ + æ”¶é›†å±æ€§ä¿¡æ¯ï¼Œä¸ºåé¢æ‰¹é‡embeddingåšå‡†å¤‡
        attribute_names = []
        if query_attributes:
            self.logger.info("è¯¦ç»†å±æ€§ä¿¡æ¯:")
            for i, attr in enumerate(query_attributes, 1):
                self.logger.info(
                    f"  {i}. åç§°: '{attr['name']}', ç±»å‹: {attr['type']}, é‡è¦æ€§: {attr.get('importance', 'N/A')}")
                # æå–æ‰€æœ‰å±æ€§åç§°
                attribute_names.append(attr["name"])

        else:
            self.logger.warning("âš ï¸ æœªæŠ½å–åˆ°ä»»ä½•å±æ€§ï¼Œè¿™å¯èƒ½å¯¼è‡´æ— æ³•æ‰¾åˆ°Keys")

        key_query_related = []
        k1_weights = {}
        total_searched = 0

        # 2. æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å±æ€§çš„ embedding å‘é‡ï¼ˆä¼˜åŒ–ï¼šå‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼‰
        if query_attributes:
            import time
            self.logger.info(
                f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(query_attributes)} ä¸ªå±æ€§çš„embeddingå‘é‡...")
            batch_embedding_start = time.perf_counter()

            # æ‰¹é‡ç”Ÿæˆ embedding
            from sag.core.ai.embedding import batch_generate_embedding
            attribute_vectors = await batch_generate_embedding(attribute_names)

            batch_embedding_time = time.perf_counter() - batch_embedding_start
            self.logger.info(
                f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆï¼Œå…± {len(attribute_vectors)} ä¸ªå‘é‡ï¼Œ"
                f"è€—æ—¶: {batch_embedding_time:.3f}ç§’ï¼Œ"
                f"å¹³å‡æ¯ä¸ª: {batch_embedding_time/len(attribute_vectors):.3f}ç§’"
            )

            # å°†å‘é‡é™„åŠ åˆ°å±æ€§ä¿¡æ¯ä¸­
            for attr, vector in zip(query_attributes, attribute_vectors):
                attr["vector"] = vector
        else:
            self.logger.warning("âš ï¸ æ²¡æœ‰å±æ€§éœ€è¦ç”Ÿæˆå‘é‡")

        # 3. å¯¹æ¯ä¸ªå±æ€§è¿›è¡Œå‘é‡æœç´¢
        for i, attribute_info in enumerate(query_attributes, 1):
            self.logger.info(
                f"ğŸ” æ­£åœ¨æœç´¢å±æ€§ {i}/{len(query_attributes)}: '{attribute_info['name']}' (ç±»å‹: {attribute_info['type']})")

            try:
                # ä½¿ç”¨é¢„å…ˆç”Ÿæˆçš„å‘é‡
                query_embedding = attribute_info["vector"]
                self.logger.debug(f"ä½¿ç”¨é¢„ç”Ÿæˆçš„å‘é‡ï¼Œç»´åº¦: {len(query_embedding)}")

                # ä½¿ç”¨å‘é‡æœç´¢æ‰¾ç›¸ä¼¼å®ä½“ï¼ŒåŒ…å«å®ä½“ç±»å‹é˜ˆå€¼ä¿¡æ¯
                self.logger.debug(
                    f"å¼€å§‹å‘é‡æœç´¢: k={config.recall.vector_top_k}, source_config_ids={config.get_source_config_ids()}, entity_type={attribute_info['type']}")
                similar_entities = await self.entity_repo.search_similar(
                    query_vector=query_embedding,
                    k=config.recall.vector_top_k,
                    source_config_ids=config.get_source_config_ids(),  # ä½¿ç”¨å¤šæºæ”¯æŒ
                    entity_type=attribute_info["type"],
                    include_type_threshold=True,
                )

                total_searched += len(similar_entities)
                self.logger.info(
                    f"ğŸ“Š å±æ€§ '{attribute_info['name']}' æœç´¢åˆ° {len(similar_entities)} ä¸ªå€™é€‰å®ä½“")

                # å¦‚æœæ²¡æœ‰æœç´¢åˆ°ä»»ä½•å®ä½“ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                if len(similar_entities) == 0:
                    self.logger.warning(
                        f"âš ï¸ å±æ€§ '{attribute_info['name']}' æœªæ‰¾åˆ°ä»»ä½•å€™é€‰å®ä½“")
                    self.logger.warning(
                        f"   å¯èƒ½åŸå› : 1) source_config_ids '{config.source_config_ids}' æ— æ•°æ® 2) entity_type '{attribute_info['type']}' æ— æ•°æ® 3) ESç´¢å¼•é—®é¢˜")
                    continue

                # è®°å½•æœç´¢åˆ°çš„åŸå§‹ç»“æœï¼ˆå‰3ä¸ªï¼‰
                self.logger.debug("æœç´¢åˆ°çš„å€™é€‰å®ä½“ï¼ˆå‰3ä¸ªï¼‰:")
                for j, entity in enumerate(similar_entities[:3], 1):
                    similarity = float(entity.get("_score", 0.0))
                    type_threshold = entity.get("type_threshold", 0.800)
                    self.logger.debug(
                        f"  {j}. '{entity['name']}' [{entity['type']}] - similarity: {similarity:.3f}, type_threshold: {type_threshold:.3f}")

                passed_count = 0
                failed_count = 0
                # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼å¹¶è®°å½•æƒé‡
                for entity in similar_entities:
                    similarity = float(entity.get("_score", 0.0))
                    type_threshold = entity.get("type_threshold", 0.800)

                    # ä½¿ç”¨é…ç½®é˜ˆå€¼å’Œç±»å‹é˜ˆå€¼ä¸­çš„æœ€å¤§å€¼
                    final_threshold = max(
                        config.recall.entity_similarity_threshold, type_threshold)

                    if similarity >= final_threshold:
                        key_query_related.append({
                            "entity_id": entity["entity_id"],
                            "name": entity["name"],
                            "type": entity["type"],
                            "similarity": similarity,
                            "source_attribute": attribute_info["name"],
                            "type_threshold": type_threshold,
                            "final_threshold": final_threshold,
                        })
                        k1_weights[entity["entity_id"]] = similarity
                        passed_count += 1

                        # ğŸ†• è®°å½•çº¿ç´¢ï¼šextracted_entity â†’ real_entity
                        extracted_node = Tracker.build_extracted_entity_node(
                            attribute_info)
                        real_entity_dict = {
                            "entity_id": entity["entity_id"],
                            "name": entity["name"],
                            "type": entity["type"],
                            "description": entity.get("description", "")
                        }
                        # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                        entity_weight = real_entity_dict.get("weight")
                        metadata = {
                            "method": "vector_search",
                            "step": "step1",
                            "source_attribute": attribute_info["name"],
                            "attribute_type": attribute_info.get("type"),
                            "type_threshold": type_threshold,
                            "final_threshold": final_threshold,
                        }
                        # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                        if entity_weight is not None:
                            metadata["weight"] = entity_weight

                        tracker.add_clue(
                            stage="recall",
                            from_node=extracted_node,
                            to_node=Tracker.build_entity_node(
                                real_entity_dict),
                            confidence=similarity,  # ç»Ÿä¸€ä½¿ç”¨similarity
                            relation="å‘é‡å¬å›",
                            display_level="intermediate",  # ğŸ†• ä¸­é—´ç»“æœ
                            metadata=metadata
                        )

                        self.logger.debug(
                            f"âœ… å®ä½“ '{entity['name']}' é€šè¿‡é˜ˆå€¼æ£€æŸ¥: "
                            f"similarity={similarity:.3f} >= final_threshold={final_threshold:.3f} "
                            f"(type_threshold={type_threshold:.3f}, config_threshold={config.recall.entity_similarity_threshold:.3f})"
                        )
                    else:
                        failed_count += 1
                        self.logger.debug(
                            f"âŒ å®ä½“ '{entity['name']}' æœªé€šè¿‡é˜ˆå€¼æ£€æŸ¥: "
                            f"similarity={similarity:.3f} < final_threshold={final_threshold:.3f} "
                            f"(type_threshold={type_threshold:.3f}, config_threshold={config.recall.entity_similarity_threshold:.3f})"
                        )

                self.logger.info(
                    f"ğŸ“ˆ å±æ€§ '{attribute_info['name']}' é˜ˆå€¼è¿‡æ»¤ç»“æœ: "
                    f"é€šè¿‡ {passed_count}/{len(similar_entities)}, å¤±è´¥ {failed_count}/{len(similar_entities)}"
                )

            except AIError as e:
                self.logger.error(
                    f"âŒ å±æ€§ '{attribute_info['name']}' å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
                self.logger.error(
                    f"   å¯èƒ½åŸå› : 1) Embedding APIé—®é¢˜ 2) ç½‘ç»œè¿æ¥é—®é¢˜ 3) APIå¯†é’¥é—®é¢˜")
                # å‘é‡ç”Ÿæˆå¤±è´¥æ—¶è·³è¿‡è¯¥å±æ€§ï¼Œç»§ç»­å¤„ç†å…¶ä»–å±æ€§
                continue
            except Exception as e:
                self.logger.error(f"âŒ æœç´¢å®ä½“å¤±è´¥: {attribute_info['name']} - {e}")
                import traceback
                self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                continue

        # 3. å»é‡ï¼ˆåŸºäºentity_idï¼‰å¹¶é™åˆ¶æ•°é‡
        seen_entities = set()
        unique_keys = []
        for key_info in key_query_related:
            entity_id = key_info["entity_id"]
            if entity_id not in seen_entities:
                seen_entities.add(entity_id)
                unique_keys.append(key_info)

        before_limit = len(unique_keys)
        key_query_related = unique_keys[: config.recall.max_entities]

        # æ±‡æ€»æ—¥å¿—
        self.logger.info(
            f"ğŸ“‹ æ­¥éª¤1å®Œæˆ: æ€»æœç´¢={total_searched}, "
            f"é€šè¿‡é˜ˆå€¼={before_limit}, "
            f"å»é‡å={len(key_query_related)}, "
            f"é™åˆ¶max_keys={config.recall.max_entities}"
        )

        if len(key_query_related) > 0:
            # æ˜¾ç¤ºæœ€é«˜ç›¸ä¼¼åº¦çš„å‡ ä¸ªå®ä½“
            top_entities = sorted(
                key_query_related, key=lambda x: x["similarity"], reverse=True)[:3]
            top_info = [
                f"'{e['name']}'({e['type']}, {e['similarity']:.3f})"
                for e in top_entities
            ]
            self.logger.info(f"ğŸ† Top 3 ç›¸ä¼¼å®ä½“: {', '.join(top_info)}")
        else:
            self.logger.error("âŒ æ­¥éª¤1æœ€ç»ˆç»“æœ: æœªæ‰¾åˆ°ä»»ä½•Keysï¼")
            self.logger.error("   å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            self.logger.error("   1. é™ä½ key_similarity_threshold (å½“å‰: {:.1f})".format(
                config.recall.entity_similarity_threshold))
            self.logger.error("   2. æ£€æŸ¥ source_config_ids '{}' æ˜¯å¦æœ‰å®ä½“æ•°æ®".format(
                config.source_config_ids))
            self.logger.error(
                "   3. æ£€æŸ¥ Elasticsearch ç´¢å¼• 'entity_vectors' æ˜¯å¦æœ‰æ•°æ®")
            self.logger.error("   4. æ£€æŸ¥å®ä½“ç±»å‹ç›¸ä¼¼åº¦é˜ˆå€¼è®¾ç½®æ˜¯å¦è¿‡é«˜")

        return key_query_related, k1_weights

    async def _extract_attributes_from_query(self, query: str, enable_rewrite: bool = True) -> Tuple[List[Dict[str, Any]], Optional[str]]:
        """
        ä»queryä¸­æŠ½å–ç»“æ„åŒ–å±æ€§ï¼Œå¯é€‰æ‹©æ€§åœ°è¿›è¡Œqueryé‡å†™

        Args:
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            enable_rewrite: æ˜¯å¦å¯ç”¨queryé‡å†™åŠŸèƒ½

        Returns:
            Tuple[List[Dict[str, Any]], Optional[str]]: (å±æ€§åˆ—è¡¨, é‡å†™åçš„query)
        """
        self.logger.debug(
            f"å¼€å§‹ä»queryä¸­æŠ½å–å±æ€§: {query}, enable_rewrite={enable_rewrite}")

        try:
            if enable_rewrite:
                # ä½¿ç”¨å¸¦é‡å†™åŠŸèƒ½çš„æç¤ºè¯æ¨¡æ¿
                prompt = self.prompt_manager.render(
                    "extract_attributes_with_rewrite",
                    query=query
                )

                # è°ƒç”¨LLMè¿›è¡Œå±æ€§æŠ½å–å’ŒæŸ¥è¯¢é‡å†™
                messages = [
                    LLMMessage(role=LLMRole.USER, content=prompt)
                ]

                # æ„å»ºæ”¯æŒé‡å†™çš„JSON Schema
                schema = self._build_attribute_extraction_with_rewrite_schema()

                response = await self.llm_client.chat_with_schema(
                    messages, response_schema=schema, temperature=0.2, max_tokens=2000
                )

                # è§£æLLMå“åº”ï¼Œæå–ç»“æ„åŒ–å±æ€§å’Œé‡å†™åçš„æŸ¥è¯¢
                attributes, rewritten_query = await self._parse_attribute_extraction_with_rewrite_response(
                    response)

                if not attributes:
                    self.logger.debug("LLMæœªæå–åˆ°å±æ€§ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                    return await self._fallback_attribute_extraction(query), None

                self.logger.debug(
                    f"LLMæŠ½å–åˆ° {len(attributes)} ä¸ªå±æ€§: {[attr['name'] for attr in attributes]}")

                # è®°å½•é‡å†™ä¿¡æ¯
                if rewritten_query and rewritten_query != query:
                    self.logger.info(
                        f"ğŸ“ Queryé‡å†™: '{query}' â†’ '{rewritten_query}'")
                elif rewritten_query:
                    self.logger.debug(f"ğŸ“ Queryä¿æŒä¸å˜: '{query}' (è´¨é‡åˆ†æ•°æœªè¾¾åˆ°é˜ˆå€¼)")

                return attributes, rewritten_query
            else:
                # ä½¿ç”¨åŸæœ‰çš„å±æ€§æŠ½å–é€»è¾‘
                prompt = self.prompt_manager.render(
                    "extract_attributes",
                    query=query
                )

                messages = [
                    LLMMessage(role=LLMRole.USER, content=prompt)
                ]

                schema = await self._build_attribute_extraction_schema()

                response = await self.llm_client.chat_with_schema(
                    messages, response_schema=schema, temperature=0.2, max_tokens=2000
                )

                attributes = await self._parse_attribute_extraction_response(
                    response)

                if not attributes:
                    self.logger.debug("LLMæœªæå–åˆ°å±æ€§ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                    return await self._fallback_attribute_extraction(query), None

                self.logger.debug(
                    f"LLMæŠ½å–åˆ° {len(attributes)} ä¸ªå±æ€§: {[attr['name'] for attr in attributes]}")
                return attributes, None

        except Exception as e:
            self.logger.warning(f"LLMå±æ€§æŠ½å–å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ: {e}")
            # å›é€€åˆ°ç®€å•çš„è§„åˆ™åŒ¹é…
            return await self._fallback_attribute_extraction(query), None

    async def _build_attribute_extraction_schema(self) -> Dict[str, Any]:
        """
        æ„å»ºå±æ€§æå–çš„JSON Schemaï¼ŒåŒ¹é…ç°æœ‰æç¤ºè¯æ¨¡æ¿çš„è¾“å‡ºæ ¼å¼
        """
        return {
            "type": "object",
            "properties": {
                "attributes": {
                    "type": "array",
                    "description": "æå–çš„å±æ€§åˆ—è¡¨",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "description": "å±æ€§åç§°"},
                            "type": {"type": "string", "description": "å±æ€§ç±»å‹ï¼ˆperson/location/time/topic/action/organization/productç­‰ï¼‰"},
                            "context": {"type": "string", "description": "åœ¨æŸ¥è¯¢ä¸­çš„ä¸Šä¸‹æ–‡"},
                            "importance": {"type": "string", "description": "é‡è¦æ€§ï¼ˆhigh/medium/lowï¼‰"}
                        },
                        "required": ["name", "type", "importance"]
                    }
                }
            },
            "required": ["attributes"],
        }

    async def _build_attribute_extraction_with_rewrite_schema(self) -> Dict[str, Any]:
        """
        æ„å»ºæ”¯æŒqueryé‡å†™çš„å±æ€§æå–JSON Schema
        """
        return {
            "type": "object",
            "properties": {
                "attributes": {
                    "type": "array",
                    "description": "æå–çš„å±æ€§åˆ—è¡¨",
                    "items": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "description": "å±æ€§åç§°"},
                            "type": {"type": "string", "description": "å±æ€§ç±»å‹ï¼ˆperson/location/time/topic/action/organization/productç­‰ï¼‰"},
                            "context": {"type": "string", "description": "åœ¨æŸ¥è¯¢ä¸­çš„ä¸Šä¸‹æ–‡"},
                            "importance": {"type": "string", "description": "é‡è¦æ€§ï¼ˆhigh/medium/lowï¼‰"}
                        },
                        "required": ["name", "type", "importance"]
                    }
                },
                "rewritten_query": {
                    "type": "string",
                    "description": "é‡å†™åçš„æŸ¥è¯¢æ–‡æœ¬"
                }
            },
            "required": ["attributes", "rewritten_query"],
        }

    async def _parse_attribute_extraction_response(self, response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è§£æå±æ€§æå–å“åº”ï¼ŒåŒ¹é…ç°æœ‰æç¤ºè¯æ¨¡æ¿çš„è¾“å‡ºæ ¼å¼
        """
        attributes = []
        attributes_data = response.get("attributes", [])

        if not isinstance(attributes_data, list):
            return attributes

        for attr_item in attributes_data:
            if not isinstance(attr_item, dict):
                continue

            name = attr_item.get("name", "").strip()
            attr_type = attr_item.get("type", "").strip()
            context = attr_item.get("context", "").strip()
            importance = attr_item.get("importance", "medium").strip()

            if name and attr_type:  # ç¡®ä¿åç§°å’Œç±»å‹éƒ½ä¸ä¸ºç©º
                # éªŒè¯é‡è¦æ€§å­—æ®µ
                if importance not in ["high", "medium", "low"]:
                    importance = "medium"

                attributes.append({
                    "name": name,
                    "type": attr_type,
                    "context": context,
                    "importance": importance,
                    "confidence": await self._importance_to_confidence(importance)
                })

        return attributes

    async def _parse_attribute_extraction_with_rewrite_response(
        self, response: Dict[str, Any]
    ) -> Tuple[List[Dict[str, Any]], Optional[str]]:
        """
        è§£ææ”¯æŒqueryé‡å†™çš„å±æ€§æå–å“åº”

        Args:
            response: LLMå“åº”

        Returns:
            Tuple[List[Dict[str, Any]], Optional[str]]: (å±æ€§åˆ—è¡¨, é‡å†™åçš„queryæˆ–None)
        """
        # è§£æå±æ€§éƒ¨åˆ†
        attributes = await self._parse_attribute_extraction_response(response)

        # è§£æé‡å†™éƒ¨åˆ†
        rewritten_query = response.get("rewritten_query", "").strip()

        if rewritten_query:
            self.logger.debug(f"è·å–åˆ°é‡å†™åçš„query: '{rewritten_query}'")
        else:
            self.logger.debug("æœªè·å–åˆ°é‡å†™åçš„queryï¼Œå°†ä½¿ç”¨åŸquery")

        return attributes, rewritten_query if rewritten_query else None

    async def _importance_to_confidence(self, importance: str) -> float:
        """
        å°†é‡è¦æ€§è½¬æ¢ä¸ºç½®ä¿¡åº¦
        """
        importance_confidence_map = {
            "high": 0.9,
            "medium": 0.7,
            "low": 0.5
        }
        return importance_confidence_map.get(importance, 0.7)

    async def _parse_llm_attributes_response(self, response: str) -> List[Dict[str, Any]]:
        """
        è§£æLLMè¿”å›çš„å±æ€§ä¿¡æ¯ï¼ˆä¿ç•™åŸæœ‰æ–¹æ³•ä½œä¸ºå…¼å®¹ï¼‰
        """
        import json
        import re

        try:
            # å°è¯•ç›´æ¥è§£æJSON
            if response.strip().startswith('[') or response.strip().startswith('{'):
                return json.loads(response)
        except json.JSONDecodeError:
            pass

        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
        attributes = []

        # ç®€å•çš„æ­£åˆ™åŒ¹é…æå–
        patterns = [
            r'åç§°[ï¼š:]\s*([^\n,ï¼Œ]+)\s*ç±»å‹[ï¼š:]\s*([^\n,ï¼Œ]+)',
            r'å±æ€§[ï¼š:]\s*([^\n,ï¼Œ]+)\s*\(([^)]+)\)',
            r'([^\n,ï¼Œ]+)\s*-\s*([^\n,ï¼Œ]+)',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, response)
            for name, attr_type in matches:
                # æ ‡å‡†åŒ–å±æ€§ç±»å‹
                attr_type = attr_type.strip().lower()
                if any(word in attr_type for word in ['äºº', 'person', 'äººç‰©', 'ä¸“å®¶']):
                    standardized_type = 'person'
                elif any(word in attr_type for word in ['ç»„ç»‡', 'org', 'ä¼ä¸š', 'å…¬å¸', 'æœºæ„']):
                    standardized_type = 'organization'
                elif any(word in attr_type for word in ['åœ°', 'location', 'åœ°ç‚¹', 'åœ°æ–¹']):
                    standardized_type = 'location'
                elif any(word in attr_type for word in ['æ—¶', 'time', 'æ—¶é—´', 'æ—¥æœŸ']):
                    standardized_type = 'time'
                else:
                    standardized_type = 'topic'

                attributes.append({
                    "name": name.strip(),
                    "type": standardized_type
                })

        return attributes[:10]  # é™åˆ¶æœ€å¤š10ä¸ªå±æ€§

    async def _fallback_attribute_extraction(self, query: str) -> List[Dict[str, Any]]:
        """
        å›é€€çš„å±æ€§æŠ½å–æ–¹æ¡ˆï¼ˆåŸºäºè§„åˆ™ï¼‰
        """
        attributes = []

        # åŸºäºä¸€äº›ç®€å•çš„è§„åˆ™æå–å±æ€§
        if any(word in query.lower() for word in ["ai", "artificial intelligence", "äººå·¥æ™ºèƒ½"]):
            attributes.append({"name": "AI", "type": "topic"})
        if any(word in query.lower() for word in ["tech", "technology", "æŠ€æœ¯", "ç§‘æŠ€"]):
            attributes.append({"name": "ç§‘æŠ€", "type": "topic"})
        if any(word in query.lower() for word in ["innovation", "åˆ›æ–°"]):
            attributes.append({"name": "åˆ›æ–°", "type": "topic"})
        if any(word in query.lower() for word in ["medical", "health", "åŒ»ç–—", "å¥åº·"]):
            attributes.append({"name": "åŒ»ç–—", "type": "topic"})
        if any(word in query.lower() for word in ["company", "ä¼ä¸š", "å…¬å¸"]):
            attributes.append({"name": "ä¼ä¸š", "type": "organization"})
        if any(word in query.lower() for word in ["person", "people", "äººç‰©", "ä¸“å®¶"]):
            attributes.append({"name": "äººç‰©", "type": "person"})

        # å¦‚æœæ²¡æœ‰æå–åˆ°å±æ€§ï¼Œä½¿ç”¨é»˜è®¤
        if not attributes:
            attributes = [
                {"name": "AI", "type": "topic"},
                {"name": "ç§‘æŠ€", "type": "topic"},
            ]

        return attributes

    async def _step2_keys_to_events(
        self, config: SearchConfig, key_query_related: List[Dict[str, Any]]
    ) -> List[str]:
        """
        æ­¥éª¤2: keyæ‰¾eventï¼ˆç²¾å‡†åŒ¹é…ï¼‰
        é€šè¿‡[key-query-related]ç”¨sqlæ‰¾åˆ°æ‰€æœ‰å…³è”äº‹é¡¹

        åŒæ—¶è®°å½•çº¿ç´¢ï¼šentity â†’ event
        """
        if not key_query_related:
            return []

        key_entity_ids = [key["entity_id"] for key in key_query_related]

        # ğŸ†• æ„å»º entity_id â†’ source_attribute æ˜ å°„
        entity_source_map = {
            key["entity_id"]: key.get("source_attribute")
            for key in key_query_related
        }

        # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨è®°å½•çº¿ç´¢
        tracker = Tracker(config)

        async with self.session_factory() as session:
            # æŸ¥è¯¢entity-eventå…³ç³»ï¼ˆè¿”å›å®Œæ•´æ˜ å°„ï¼Œç”¨äºè®°å½•çº¿ç´¢ï¼‰
            query = (
                select(EventEntity.entity_id, EventEntity.event_id)
                .where(EventEntity.entity_id.in_(key_entity_ids))
            )

            result = await session.execute(query)
            entity_event_pairs = result.fetchall()

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šentity â†’ eventï¼ˆä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹ï¼ŒæŸ¥è¯¢eventå¯¹è±¡è·å–å®Œæ•´ä¿¡æ¯ï¼‰
            # å…ˆæ‰¹é‡æŸ¥è¯¢eventå¯¹è±¡
            event_ids_for_query = list(
                set(event_id for _, event_id in entity_event_pairs))
            events_query = select(SourceEvent).where(
                SourceEvent.id.in_(event_ids_for_query))
            events_result = await session.execute(events_query)
            events = {event.id: event for event in events_result.scalars().all()}

            # åŒæ—¶æŸ¥è¯¢entityå¯¹è±¡
            entities_query = select(Entity).where(
                Entity.id.in_(key_entity_ids))
            entities_result = await session.execute(entities_query)
            entities = {
                entity.id: entity for entity in entities_result.scalars().all()}

            # è®°å½•æ¯ä¸ªentityâ†’eventçš„çº¿ç´¢
            for entity_id, event_id in entity_event_pairs:
                entity_obj = entities.get(entity_id)
                event_obj = events.get(event_id)

                # æ„å»ºentityå’ŒeventèŠ‚ç‚¹
                if entity_obj:
                    entity_dict = {
                        "id": entity_obj.id,
                        "entity_id": entity_obj.id,  # å…¼å®¹å­—æ®µ
                        "name": entity_obj.name,
                        "type": entity_obj.type,
                        "description": entity_obj.description or "",
                        # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                        "source_attribute": entity_source_map.get(entity_id)
                    }
                else:
                    # Fallback
                    entity_dict = {
                        "id": entity_id,
                        "entity_id": entity_id,
                        # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                        "source_attribute": entity_source_map.get(entity_id)
                    }

                if event_obj:
                    # ä»å®ä½“å­—å…¸ä¸­è·å–ç›¸ä¼¼åº¦ä½œä¸ºconfidenceï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    entity_similarity = entity_dict.get("similarity", 1.0)
                    metadata = {
                        "method": "database_lookup",
                        "step": "step2",
                        # ğŸ†• æ·»åŠ åˆ°metadata
                        "source_attribute": entity_dict.get("source_attribute")
                    }
                    # toèŠ‚ç‚¹æ˜¯äº‹ä»¶ï¼Œä¸å­˜å‚¨weight

                    tracker.add_clue(
                        stage="recall",
                        from_node=Tracker.build_entity_node(entity_dict),
                        to_node=tracker.get_or_create_event_node(event_obj, "recall"),
                        confidence=entity_similarity,  # ä½¿ç”¨å®ä½“çš„ç›¸ä¼¼åº¦
                        display_level="intermediate",  # ğŸ†• ä¸­é—´ç»“æœ
                        metadata=metadata
                    )

            # è¿”å›å»é‡çš„event_ids
            event_ids = list(
                set(event_id for _, event_id in entity_event_pairs))

        return event_ids

    async def _step3_query_to_events(
        self, config: SearchConfig
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        æ­¥éª¤3: queryå†æ‰¾eventï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰
        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦åœ¨æ‰¾åˆ°queryå…³è”äº‹é¡¹
        """
        self.logger.debug(f"æ­¥éª¤3: é€šè¿‡è¯­ä¹‰åŒ¹é…æœç´¢ç›¸å…³äº‹ä»¶ - {config.query}")

        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_embedding
            if config.has_query_embedding and config.query_embedding:
                query_embedding = config.query_embedding
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_embedding)}")
            else:
                # ä½¿ç”¨çœŸå®Embedding APIç”ŸæˆæŸ¥è¯¢å‘é‡
                query_embedding = await self.processor.generate_embedding(config.query)
                self.logger.debug(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")

                # ç¼“å­˜query_embeddingåˆ°config
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")
        except Exception as e:
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

        content_similar_events = []

        try:
            # é€šè¿‡å†…å®¹å‘é‡æœç´¢
            content_similar_events = await self.event_repo.search_similar_by_content(
                query_vector=query_embedding,
                k=config.recall.vector_top_k,
                source_config_ids=config.get_source_config_ids(),  # ä½¿ç”¨å¤šæºæ”¯æŒ
            )
        except Exception as e:
            self.logger.warning(f"å†…å®¹å‘é‡æœç´¢å¤±è´¥: {e}")
            return [], {}

        # ç›´æ¥ä½¿ç”¨å†…å®¹æœç´¢ç»“æœ
        event_query_related = []
        for event in content_similar_events:
            event_query_related.append({
                "event_id": event["event_id"],
                "title": event["title"],
                "summary": event.get("summary", ""),
                "similarity": float(event.get("_score", 0.0)),
                "match_type": "content",
            })

        # è¿‡æ»¤ç›¸ä¼¼åº¦é˜ˆå€¼
        before_threshold = len(event_query_related)
        event_query_related = [
            event for event in event_query_related
            if event["similarity"] >= config.recall.event_similarity_threshold
        ]

        # é™åˆ¶æ•°é‡
        event_query_related = event_query_related[: config.recall.max_events]

        # æ„å»ºæƒé‡å‘é‡
        e1_weights = {event["event_id"]: event["similarity"]
                      for event in event_query_related}

        self.logger.debug(
            f"æ­¥éª¤3å®Œæˆ(ä»…å†…å®¹æœç´¢): æ‰¾åˆ° {len(event_query_related)} ä¸ªç›¸å…³äº‹ä»¶ "
            f"(é˜ˆå€¼è¿‡æ»¤å‰: {before_threshold})"
        )

        return event_query_related, e1_weights

    async def _step4_filter_events(
        self,
        event_key_query_related: List[str],
        event_query_related: List[Dict[str, Any]],
        key_query_related: List[Dict[str, Any]],
    ) -> Tuple[List[str], List[str]]:
        """
        æ­¥éª¤4: è¿‡æ»¤Eventï¼ˆç²¾å‡†ç­›é€‰ï¼‰
        [Event-query-related]å’Œ[Event-key-query-related]å–äº¤é›†
        ç„¶ååªä¿ç•™æ­¥éª¤1å¬å›çš„keyä¸­ï¼Œé‚£äº›eventsåœ¨äº¤é›†ä¸­çš„key
        """
        # æå–event_query_relatedä¸­çš„event_id
        query_event_ids = {event["event_id"] for event in event_query_related}
        key_event_ids = set(event_key_query_related)

        # å–äº¤é›†
        event_related = list(query_event_ids.intersection(key_event_ids))
        event_related_set = set(event_related)

        self.logger.debug(
            f"ğŸ“Š [Step4å†…éƒ¨] Eventsäº¤é›†: "
            f"keyæ‰¾åˆ°çš„events={len(key_event_ids)}, "
            f"queryæ‰¾åˆ°çš„events={len(query_event_ids)}, "
            f"äº¤é›†={len(event_related)}"
        )

        # åªä¿ç•™æ­¥éª¤1å¬å›çš„keyä¸­ï¼Œé‚£äº›å…³è”çš„eventsåœ¨äº¤é›†ä¸­çš„key
        key_related = []
        if event_related:
            async with self.session_factory() as session:
                for key_info in key_query_related:
                    key_id = key_info["entity_id"]

                    # æŸ¥è¯¢è¿™ä¸ªkeyå…³è”çš„æ‰€æœ‰events
                    query = (
                        select(EventEntity.event_id)
                        .where(EventEntity.entity_id == key_id)
                    )
                    result = await session.execute(query)
                    key_events = {row[0] for row in result.fetchall()}

                    # æ£€æŸ¥è¿™ä¸ªkeyçš„eventsæ˜¯å¦ä¸äº¤é›†æœ‰äº¤é›†
                    if key_events.intersection(event_related_set):
                        key_related.append(key_id)

                self.logger.debug(
                    f"ğŸ“Š [Step4å†…éƒ¨] ä»æ­¥éª¤1å¬å›çš„{len(key_query_related)}ä¸ªkeyä¸­ï¼Œ"
                    f"ä¿ç•™äº†{len(key_related)}ä¸ªï¼ˆå®ƒä»¬çš„eventsåœ¨äº¤é›†ä¸­ï¼‰"
                )

        return event_related, key_related

    async def _step5_calculate_event_key_weights(
        self,
        event_related: List[str],
        key_related: List[str],
        k1_weights: Dict[str, float],
    ) -> Dict[str, float]:
        """
        æ­¥éª¤5: è®¡ç®—event-keyæƒé‡å‘é‡
        æ ¹æ®æ¯ä¸ªeventåŒ…å«keyçš„æƒ…å†µï¼Œå°†å¯¹åº”keyçš„æƒé‡ç›¸åŠ 
        """
        if not event_related or not key_related:
            return {}

        event_key_weights = {}

        try:
            async with self.session_factory() as session:
                for event_id in event_related:
                    # æŸ¥è¯¢è¯¥eventåŒ…å«çš„æ‰€æœ‰key
                    query = (
                        select(EventEntity.entity_id)
                        .where(EventEntity.event_id == event_id)
                        .where(EventEntity.entity_id.in_(key_related))
                    )
                    result = await session.execute(query)
                    event_keys = [row[0] for row in result.fetchall()]

                    # è®¡ç®—æƒé‡ï¼šW_event-key(ej) = Î£(k1)i (k_i âˆˆ e_j)
                    total_weight = sum(k1_weights.get(key_id, 0.0)
                                       for key_id in event_keys)
                    event_key_weights[event_id] = total_weight
        except Exception as e:
            self.logger.error(f"æ­¥éª¤5è®¡ç®—event-keyæƒé‡å¤±è´¥: {e}", exc_info=True)
            raise

        return event_key_weights

    async def _step6_calculate_event_key_query_weights(
        self,
        event_key_weights: Dict[str, float],
        e1_weights: Dict[str, float],
    ) -> Dict[str, float]:
        """
        æ­¥éª¤6: è®¡ç®—event-key-queryæƒé‡å‘é‡
        å°†(event-key)*(e1)ï¼Œå¾—åˆ°æ–°çš„ï¼ˆe2ï¼‰å‘é‡
        """
        event_key_query_weights = {}

        for event_id in event_key_weights:
            key_weight = event_key_weights[event_id]
            query_weight = e1_weights.get(event_id, 0.0)

            # W_e2(ej) = W_event-key(ej) Ã— (e1)j
            event_key_query_weights[event_id] = key_weight * query_weight

        return event_key_query_weights

    async def _step7_calculate_key_event_weights(
        self,
        event_related: List[str],
        key_related: List[str],
        event_key_query_weights: Dict[str, float],
    ) -> Dict[str, float]:
        """
        æ­¥éª¤7: åå‘è®¡ç®—keyæƒé‡å‘é‡
        æ ¹æ®æ¯ä¸ªeventçš„æƒé‡åå‘è®¡ç®—keyçš„é‡è¦æ€§
        """
        if not event_related or not key_related:
            return {}

        key_event_weights = {}

        async with self.session_factory() as session:
            for key_id in key_related:
                # æŸ¥è¯¢åŒ…å«è¯¥keyçš„æ‰€æœ‰event
                query = (
                    select(EventEntity.event_id)
                    .where(EventEntity.entity_id == key_id)
                    .where(EventEntity.event_id.in_(event_related))
                )
                result = await session.execute(query)
                key_events = [row[0] for row in result.fetchall()]

                # è®¡ç®—æƒé‡ï¼šW_key-event(ki) = Î£ W_e2(ej) (e_j contains k_i)
                total_weight = sum(
                    event_key_query_weights.get(event_id, 0.0) for event_id in key_events
                )
                key_event_weights[key_id] = total_weight

        return key_event_weights

    async def _step8_extract_important_keys(
        self,
        key_event_weights: Dict[str, float],
        config: SearchConfig,
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤8: æå–é‡è¦çš„key
        è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–æå–top-né‡è¦çš„key
        """
        # è·å–keyçš„è¯¦ç»†ä¿¡æ¯
        key_final = []

        if not key_event_weights:
            return key_final

        # æŒ‰æƒé‡æ’åº
        sorted_keys = sorted(key_event_weights.items(),
                             key=lambda x: x[1], reverse=True)

        # åº”ç”¨é˜ˆå€¼æˆ–top-nç­›é€‰
        if config.recall.final_entity_count:
            # Top-Næ¨¡å¼
            selected_keys = sorted_keys[: config.recall.final_entity_count]
        else:
            # é˜ˆå€¼æ¨¡å¼
            selected_keys = [
                (key_id, weight) for key_id, weight in sorted_keys
                if weight >= config.recall.entity_weight_threshold
            ]

        # è·å–keyçš„è¯¦ç»†ä¿¡æ¯
        if selected_keys:
            key_ids = [key_id for key_id, _ in selected_keys]

            try:
                async with self.session_factory() as session:
                    query = select(Entity).where(Entity.id.in_(key_ids))
                    result = await session.execute(query)
                    entities = {
                        entity.id: entity for entity in result.scalars().all()}

                for key_id, weight in selected_keys:
                    entity = entities.get(key_id)
                    if entity:
                        key_final.append({
                            "key_id": key_id,
                            "name": entity.name,
                            "type": entity.type,
                            "weight": weight,
                            "steps": [1],  # ç¬¬ä¸€é˜¶æ®µï¼Œæ‰€æœ‰å€¼éƒ½ä¸º1
                        })
            except Exception as e:
                self.logger.error(f"æ­¥éª¤8æå–é‡è¦keyså¤±è´¥: {e}", exc_info=True)
                raise

        # ç­›é€‰å‡ºæœ€ç»ˆè¢«ä½¿ç”¨çš„queryå¬å›çš„key
        if key_final and config.query_recalled_keys:
            # æ„å»ºkey_finalçš„key_idåˆ°keyå¯¹è±¡çš„æ˜ å°„
            key_final_map = {key["key_id"]: key for key in key_final}

            # è®°å½•åŸå§‹æ•°é‡
            original_count = len(config.query_recalled_keys)

            # ç­›é€‰å‡ºåœ¨key_finalä¸­çš„queryå¬å›çš„keyï¼Œå¹¶ä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡
            used_query_keys = []
            for query_key in config.query_recalled_keys:
                entity_id = query_key["entity_id"]
                if entity_id in key_final_map:
                    # ä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡ï¼ˆåŒ…å«weightå’Œstepsç­‰ä¿¡æ¯ï¼‰
                    used_query_keys.append(key_final_map[entity_id])

            # æ›´æ–°config.query_recalled_keysï¼Œåªä¿ç•™æœ€ç»ˆè¢«ä½¿ç”¨çš„keyï¼ˆæ¥è‡ªkey_finalï¼‰
            config.query_recalled_keys = used_query_keys

            self.logger.info(
                f"æ­¥éª¤8: queryå¬å›çš„keyä¸­æ€»å…±{original_count}ä¸ª "
                f"æœ‰{len(used_query_keys)}ä¸ªè¢«ä¿ç•™åœ¨key_finalä¸­ï¼ˆä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡ï¼‰"
            )

            if used_query_keys:
                # æ˜¾ç¤ºè¢«ä¿ç•™çš„queryå¬å›çš„key
                used_key_names = [key["name"] for key in used_query_keys[:5]]
                self.logger.debug(
                    f"è¢«ä¿ç•™çš„queryå¬å›keyï¼ˆå‰5ä¸ªï¼‰: {', '.join(used_key_names)}")

        return key_final

    async def _generate_vector_unified(
        self,
        text: str,
        context: str = "unknown",
        use_cache: bool = True
    ) -> List[float]:
        """
        ç»Ÿä¸€çš„å‘é‡ç”Ÿæˆæ–¹æ³•

        Args:
            text: éœ€è¦ç”Ÿæˆå‘é‡çš„æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡æè¿°ï¼Œç”¨äºæ—¥å¿—è®°å½•
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé¢„ç•™æ‰©å±•ï¼‰

        Returns:
            ç”Ÿæˆçš„å‘é‡æ•°ç»„

        Raises:
            AIError: å‘é‡ç”Ÿæˆå¤±è´¥
        """
        if not text or not text.strip():
            raise AIError(f"å‘é‡ç”Ÿæˆå¤±è´¥ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©º (context: {context})")

        try:
            self.logger.debug(
                f"å¼€å§‹ç”Ÿæˆå‘é‡ - context: {context}, "
                f"æ–‡æœ¬é•¿åº¦: {len(text)}å­—ç¬¦, "
                f"æ–‡æœ¬é¢„è§ˆ: {text[:50]}{'...' if len(text) > 50 else ''}"
            )

            # ä½¿ç”¨ç»Ÿä¸€çš„å¤„ç†å™¨ç”Ÿæˆå‘é‡
            vector = await self.processor.generate_embedding(text)

            # éªŒè¯å‘é‡æœ‰æ•ˆæ€§
            if not vector or len(vector) == 0:
                raise AIError(f"å‘é‡ç”Ÿæˆå¤±è´¥ï¼šè¿”å›ç©ºå‘é‡ (context: {context})")

            # éªŒè¯å‘é‡ç»´åº¦æ˜¯å¦åˆç†
            if len(vector) < 100 or len(vector) > 10000:
                self.logger.warning(
                    f"å‘é‡ç»´åº¦å¼‚å¸¸: {len(vector)} (context: {context}), "
                    f"é€šå¸¸åº”åœ¨100-10000èŒƒå›´å†…"
                )

            # éªŒè¯å‘é‡æ˜¯å¦åŒ…å«æ— æ•ˆå€¼
            if not await self._is_valid_vector(vector):
                raise AIError(f"å‘é‡ç”Ÿæˆå¤±è´¥ï¼šå‘é‡åŒ…å«æ— æ•ˆå€¼ (context: {context})")

            self.logger.debug(
                f"å‘é‡ç”ŸæˆæˆåŠŸ - context: {context}, "
                f"ç»´åº¦: {len(vector)}"
            )

            return vector

        except AIError:
            # AIé”™è¯¯ç›´æ¥é‡æ–°æŠ›å‡º
            raise
        except Exception as e:
            error_msg = f"å‘é‡ç”Ÿæˆå¤±è´¥ - context: {context}, error: {e}"
            self.logger.error(error_msg, exc_info=True)
            raise AIError(error_msg) from e

    async def _is_valid_vector(self, vector: List[float]) -> bool:
        """
        éªŒè¯å‘é‡æ˜¯å¦æœ‰æ•ˆ

        Args:
            vector: å‘é‡æ•°ç»„

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        if not vector:
            return False

        try:
            import math
            return all(
                not math.isnan(x) and not math.isinf(x)
                for x in vector
            )
        except (TypeError, ValueError):
            return False

    async def _build_recall_clues(
        self,
        config: SearchConfig,
        key_query_related: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ„å»ºRecallé˜¶æ®µçš„çº¿ç´¢ï¼ˆquery â†’ entityï¼‰

        ä½¿ç”¨ç»Ÿä¸€çš„Trackeræ„å»ºï¼Œç¡®ä¿æ•°æ®ç»“æ„ä¸€è‡´æ€§

        Args:
            config: æœç´¢é…ç½®
            key_query_related: queryå¬å›çš„å®ä½“åˆ—è¡¨

        Returns:
            Recallé˜¶æ®µçš„çº¿ç´¢åˆ—è¡¨
        """
        from sag.modules.search.tracker import Tracker

        clues = []

        # query â†’ entityçº¿ç´¢
        for entity in key_query_related:
            # ç»Ÿä¸€ä½¿ç”¨similarityä½œä¸ºconfidence
            confidence = entity.get("similarity", 0.0)

            # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            entity_weight = entity.get("weight")
            metadata = {
                "similarity": entity.get("similarity", 0.0),
                "method": entity.get("method", "vector_search"),
                "source_attribute": entity.get("source_attribute")  # ğŸ†• æ·»åŠ æ¥æºå±æ€§
            }
            # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
            if entity_weight is not None:
                metadata["weight"] = entity_weight

            # ä½¿ç”¨ç»Ÿä¸€æ„å»ºå™¨åˆ›å»ºçº¿ç´¢
            clue = Tracker.build_recall_clue(
                config=config,
                entity=entity,
                confidence=confidence,
                metadata=metadata
            )
            clues.append(clue)

            # å°†toèŠ‚ç‚¹ï¼ˆentityèŠ‚ç‚¹ï¼‰å­˜å…¥ç¼“å­˜ï¼Œä¾›expandé˜¶æ®µä½¿ç”¨
            to_node = clue.get("to")
            if to_node and to_node.get("id"):
                config.entity_node_cache[to_node["id"]] = to_node

        return clues
