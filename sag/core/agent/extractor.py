"""
ExtractorAgent - äº‹é¡¹æå–Agent

å°†æ–‡ç« ç‰‡æ®µæˆ–å¯¹è¯æ¶ˆæ¯è½¬åŒ–ä¸ºç»“æ„åŒ–äº‹é¡¹
"""

from typing import Dict, List
from sag.core.agent.base import BaseAgent
from sag.core.prompt import get_prompt_manager
from sag.db import SourceChunk
from sag.utils import get_logger

logger = get_logger("agent.extractor")


class ExtractorAgent(BaseAgent):
    """
    æå–Agent - ä»chunkæå–ç»“æ„åŒ–äº‹é¡¹
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. memory: åŠ è½½æ–‡ç« /ä¼šè¯å…ƒæ•°æ®ï¼ˆèƒŒæ™¯ï¼‰
    2. database: åŠ è½½å®ä½“ç±»å‹å®šä¹‰ + å¾…å¤„ç†å†…å®¹
    3. todo: è®¾ç½®7æ­¥æå–ä»»åŠ¡
    4. æ‰§è¡Œ: è¿”å›JSONæ ¼å¼çš„events
    """
    
    def __init__(self, chunk_type: str, **kwargs):
        """
        Args:
            chunk_type: 'article' æˆ– 'conversation'
            **kwargs: ä¼ é€’ç»™BaseAgentï¼ˆå¦‚model_configï¼‰
        """
        # åŠ è½½extractor.json
        prompt_manager = get_prompt_manager()
        agent_config = prompt_manager.load_json_config("extractor")
        
        # ä»é…ç½®ä¸­è¯»å–å„éƒ¨åˆ†
        config_data = agent_config.get('config', {})
        
        kwargs.update({
            'database': config_data.get('database', []),
            'memory': config_data.get('memory', []),
            'todo': config_data.get('todo', []),
            'output': config_data.get('output', {}),
            'scenario': 'extract'
        })
        
        super().__init__(**kwargs)
        
        self.chunk_type = chunk_type
        logger.info(f"ExtractorAgentåˆå§‹åŒ–: type={chunk_type}")
    
    async def extract(
        self,
        content_items: List,
        metadata: Dict,
        entity_types: List[Dict],
        chunk: SourceChunk
    ) -> Dict:
        """
        æ‰§è¡Œæå–
        
        Args:
            content_items: å¾…å¤„ç†å†…å®¹ï¼ˆsectionsæˆ–messagesï¼‰
            metadata: æ–‡ç« /ä¼šè¯å…ƒæ•°æ®
            entity_types: å®ä½“ç±»å‹å®šä¹‰ï¼ˆå¿…ä¼ ï¼Œéç©ºï¼‰
            chunk: chunkå¯¹è±¡
        
        Returns:
            {"events": [...]}
        """
        logger.info(
            f"æå–: chunk={chunk.id}, type={self.chunk_type}, "
            f"items={len(content_items)}, entity_types={len(entity_types)}"
        )
        
        # 1. memory: èƒŒæ™¯ä¿¡æ¯
        self._load_background(metadata)
        
        # 2. database: å®ä½“ç±»å‹ + å¾…å¤„ç†å†…å®¹
        self._load_entity_types(entity_types)
        self._load_content_items(content_items)
        
        # 3. todo: 7æ­¥ä»»åŠ¡
        self._setup_tasks(len(content_items), len(entity_types))
        
        # 4. æ„å»ºæŸ¥è¯¢
        query = self._build_query(len(content_items), metadata.get('title', ''))
        
        # 5. åŠ¨æ€æ·»åŠ å®ä½“ç±»å‹çº¦æŸåˆ° schema
        from copy import deepcopy
        schema = deepcopy(self.output_config.get('schema'))
        
        if schema and entity_types:
            # æå–æ‰€æœ‰æœ‰æ•ˆçš„å®ä½“ç±»å‹
            valid_entity_types = [et['type'] for et in entity_types]
            
            # å¯¼èˆªåˆ° entities.type å­—æ®µå¹¶æ·»åŠ  enum çº¦æŸ
            try:
                entity_type_schema = (
                    schema
                    .get('properties', {})
                    .get('events', {})
                    .get('items', {})
                    .get('properties', {})
                    .get('entities', {})
                    .get('items', {})
                    .get('properties', {})
                    .get('type', {})
                )
                
                if entity_type_schema is not None:
                    # âœ… æ·»åŠ  enum çº¦æŸï¼Œå¼ºåˆ¶ LLM åªèƒ½ä½¿ç”¨é¢„å®šä¹‰çš„ç±»å‹
                    entity_type_schema['enum'] = valid_entity_types
                    logger.info(f"å·²æ·»åŠ å®ä½“ç±»å‹çº¦æŸ: {valid_entity_types}")
            except (KeyError, AttributeError, TypeError) as e:
                logger.warning(f"æ— æ³•æ·»åŠ å®ä½“ç±»å‹çº¦æŸ: {e}")
        
        # 6. æ‰§è¡Œï¼ˆä½¿ç”¨ä¿®æ”¹åçš„ schemaï¼‰
        result = await self.run(query=query, schema=schema)
        
        # schema æ¨¡å¼ä¸‹ï¼Œresult å·²ç»æ˜¯è§£æå¥½çš„ JSON å¯¹è±¡
        logger.info(f"æå–å®Œæˆ: events={len(result.get('events', []))}")
        return result
    
    def _load_background(self, metadata: Dict):
        """
        Memoryï¼šèƒŒæ™¯çŸ¥è¯†ï¼ˆè¾…åŠ©ç†è§£ï¼Œä¸å¹²æ‰°ï¼‰
        - åˆ†åŒº1ï¼šæºèƒŒæ™¯ï¼ˆArticle/Conversationè¡¨å­—æ®µï¼‰
        - åˆ†åŒº2ï¼šä¸Šæ–‡chunkå†…å®¹ï¼ˆå¦‚æœæœ‰ï¼Œæä¾›æ‰¿æ¥å…³ç³»ï¼‰
        """
        chunk_rank = metadata.get("chunk_rank", 0)
        chunk_heading = metadata.get("chunk_heading", "")
        
        if self.chunk_type == 'ARTICLE':
            # Memory åˆ†åŒº1ï¼šæ–‡ç« èƒŒæ™¯
            self.add_memory(
                data_type="æ–‡ç« èƒŒæ™¯",
                items=[{
                    "æ ‡é¢˜": metadata.get("title"),
                    "æ‘˜è¦": metadata.get("summary"),
                    "åˆ†ç±»": metadata.get("category"),
                    "æ ‡ç­¾": metadata.get("tags")
                }],
                description=f"ğŸ“„ Articleè¡¨å­—æ®µ - å½“å‰å¤„ç†ï¼šç¬¬{chunk_rank + 1}æ®µã€Š{chunk_heading}ã€‹"
            )
            
            # Memory åˆ†åŒº2ï¼šä¸Šæ–‡ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰
            previous = metadata.get("previous_chunk")
            if previous:
                self.add_memory(
                    data_type="ä¸Šæ–‡ç‰‡æ®µ",
                    items=[{
                        "æ ‡é¢˜": previous.get("heading"),
                        "å†…å®¹": previous.get("content")
                    }],
                    description="ğŸ“ å‰ä¸€ç‰‡æ®µçš„å®Œæ•´å†…å®¹ - æä¾›ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ç†è§£æ‰¿æ¥å…³ç³»"
                )
        
        else:  # CHAT
            # Memory åˆ†åŒº1ï¼šä¼šè¯èƒŒæ™¯
            self.add_memory(
                data_type="ä¼šè¯èƒŒæ™¯",
                items=[{
                    "å¯¹è¯ä¸»é¢˜": metadata.get("title"),
                    "å¹³å°": metadata.get("platform"),
                    "åœºæ™¯": metadata.get("scenario"),
                    "å‚ä¸è€…": metadata.get("participants"),
                    "æ¶ˆæ¯æ€»æ•°": metadata.get("messages_count"),
                    "å½“å‰æ—¶é—´æ®µ": metadata.get("time_range")
                }],
                description=f"ğŸ’¬ Conversationè¡¨å­—æ®µ - å½“å‰å¤„ç†ï¼šç¬¬{chunk_rank + 1}æ®µã€Š{chunk_heading}ã€‹"
            )
            
            # Memory åˆ†åŒº2ï¼šä¸Šæ–‡å¯¹è¯ï¼ˆå¦‚æœæœ‰ï¼‰
            previous = metadata.get("previous_chunk")
            if previous:
                self.add_memory(
                    data_type="ä¸Šæ–‡å¯¹è¯",
                    items=[{
                        "æ—¶é—´æ®µ": previous.get("heading"),
                        "å¯¹è¯å†…å®¹": previous.get("content")
                    }],
                    description="ğŸ“ å‰ä¸€æ—¶é—´æ®µçš„å®Œæ•´å¯¹è¯ - æä¾›ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ç†è§£å¯¹è¯è¿›å±•"
                )
    
    def _load_entity_types(self, entity_types: List[Dict]):
        """
        databaseç¬¬1åˆ†åŒº: å®ä½“ç±»å‹å®šä¹‰ï¼ˆå¿…ä¼ ï¼‰
        
        Args:
            entity_types: [{"type": "person", "name": "äººç‰©", "description": "...", ...}]
        """
        self.add_database(
            data_type="å®ä½“ç±»å‹å®šä¹‰",
            items=[{
                "type": et['type'],
                "name": et['name'],
                "description": et.get('description', ''),
                "weight": et.get('weight', 1.0),
                "examples": et.get('examples', [])
            } for et in entity_types],
            description=f"ğŸ“‹ å®ä½“ç±»å‹æ¸…å•ï¼ˆ{len(entity_types)}ç§ï¼‰- æå–entitiesæ—¶typeå¿…é¡»ä»è¿™é‡Œé€‰æ‹©"
        )
    
    def _load_content_items(self, items: List):
        """databaseç¬¬2åˆ†åŒº: å¾…å¤„ç†å†…å®¹"""
        if self.chunk_type == 'ARTICLE':
            self.add_database(
                data_type="å¾…å¤„ç†æ–‡ç« ç‰‡æ®µ",
                items=[{
                    "section_id": s.id,
                    "rank": s.rank,
                    "heading": s.heading,
                    "content": s.content
                } for s in items],
                description=f"ğŸ“„ å¾…å¤„ç†çš„{len(items)}ä¸ªæ–‡ç« ç‰‡æ®µ - æå–äº‹é¡¹å¹¶åœ¨referencesä¸­å¼•ç”¨section_id"
            )
        else:  # CHAT
            self.add_database(
                data_type="å¾…å¤„ç†å¯¹è¯æ¶ˆæ¯",
                items=[{
                    "message_id": m.id,
                    "timestamp": m.timestamp.isoformat() if m.timestamp else "",
                    "sender_name": m.sender_name or "æœªçŸ¥",
                    "sender_role": m.sender_role or "USER",
                    "content": m.content or ""
                } for m in items],
                description=f"ğŸ’¬ å¾…å¤„ç†çš„{len(items)}æ¡å¯¹è¯æ¶ˆæ¯ - æå–äº‹é¡¹å¹¶åœ¨referencesä¸­å¼•ç”¨message_id"
            )
    
    def _setup_tasks(self, items_count: int, types_count: int):
        """todoï¼š7æ­¥æå–æµç¨‹"""
        self.clear_todo()
        
        content_type = "æ–‡ç« ç‰‡æ®µ" if self.chunk_type == 'ARTICLE' else "å¯¹è¯æ¶ˆæ¯"
        id_field = "section_id" if self.chunk_type == 'ARTICLE' else "message_id"
        
        tasks = [
            ("understand", f"ç†è§£èƒŒæ™¯ï¼šé˜…è¯»memoryçš„æºä¿¡æ¯å’Œä¸Šæ–‡å†…å®¹ï¼ŒæŠŠæ¡ä¸»é¢˜å’Œæ‰¿æ¥å…³ç³»", 10),
            ("learn-types", f"å­¦ä¹ è§„åˆ™ï¼šè®°ä½{types_count}ç§å®ä½“ç±»å‹", 9),
            ("scan", f"æ‰«ææ•°æ®ï¼šæµè§ˆ{items_count}ä¸ª{content_type}ï¼Œè§„åˆ’æ‹†åˆ†", 8),
            ("extract", f"æå–äº‹é¡¹ï¼šæ‹†åˆ†å•å…ƒï¼Œè‡ªç„¶è§„æ•´ï¼ˆtitleæ¦‚æ‹¬ã€summaryè¯´æ˜ã€contentå®Œæ•´æè¿°ï¼‰", 7),
            ("entities", f"è¯†åˆ«å®ä½“ï¼šæå–å…³é”®å®ä½“ï¼Œä½¿ç”¨å®šä¹‰çš„type", 6),
            ("references", f"ç²¾ç¡®å¼•ç”¨ï¼šæ·»åŠ {id_field}ï¼Œè¦†ç›–æ‰€æœ‰ID", 5),
            ("verify", f"éªŒè¯è´¨é‡ï¼šæ£€æŸ¥å®Œæ•´æ€§å’Œå‡†ç¡®æ€§", 4)
        ]
        
        for task_id, desc, priority in tasks:
            self.add_todo(task_id=task_id, description=desc, status="pending", priority=priority)
    
    def _build_query(self, items_count: int, title: str) -> str:
        """æ„å»ºæå–æŸ¥è¯¢ï¼ˆç®€æ´æ˜äº†ï¼‰"""
        content_type = "æ–‡ç« ç‰‡æ®µ" if self.chunk_type == 'ARTICLE' else "å¯¹è¯æ¶ˆæ¯"
        
        return (
            f"ä»databaseæå–{items_count}ä¸ª{content_type}çš„ç»“æ„åŒ–äº‹é¡¹ã€‚\n"
            f"ä¸»é¢˜ï¼š{title}\n\n"
            f"å‚è€ƒmemoryç†è§£èƒŒæ™¯ï¼Œä¸¥æ ¼æŒ‰todoæ­¥éª¤æ‰§è¡Œã€‚"
        )

