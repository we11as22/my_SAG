"""
OpenAI LLMå®¢æˆ·ç«¯å®ç°

æ³¨æ„:
- æ”¯æŒæ ‡å‡†OpenAIæ¨¡å‹ (sophnet/Qwen3-30B-A3B-Thinking-2507, gpt-3.5-turboç­‰)
- æ”¯æŒæ€è€ƒæ¨¡å‹ (Thinking Models): æŸäº›æ¨¡å‹(å¦‚Qwen3-30B-A3B-Thinking)ä¼šå°†æ¨ç†è¿‡ç¨‹
  æ”¾åœ¨reasoning_contentå­—æ®µä¸­è€Œä¸æ˜¯contentå­—æ®µã€‚æœ¬å®ç°ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†è¿™ç§æƒ…å†µã€‚
"""

from typing import Any, AsyncIterator, Iterable, List, Optional, cast

from openai import (
    APIConnectionError,
    APIError,
    APITimeoutError,
    AsyncOpenAI,
    RateLimitError,
)
from openai.types.chat import ChatCompletionMessageParam

from sag.core.ai.base import BaseLLMClient
from sag.core.ai.models import ModelConfig, LLMMessage, LLMProvider, LLMResponse, LLMUsage
from sag.exceptions import LLMError, LLMRateLimitError, LLMTimeoutError
from sag.utils import get_logger

logger = get_logger("ai.openai")


class OpenAIClient(BaseLLMClient):
    """OpenAIå®¢æˆ·ç«¯å®ç°"""

    def __init__(self, config: ModelConfig) -> None:
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: LLMé…ç½®
        """
        super().__init__(config)

        # åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout,
        )

    async def chat(
        self,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs: Any,
    ) -> LLMResponse:
        """
        OpenAIèŠå¤©è¡¥å…¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå“åº”

        Raises:
            LLMError: è°ƒç”¨å¤±è´¥
            LLMTimeoutError: è°ƒç”¨è¶…æ—¶
            LLMRateLimitError: é€Ÿç‡é™åˆ¶
        """
        try:
            # å‡†å¤‡æ¶ˆæ¯
            api_messages = self._prepare_messages(messages)

            # è®°å½•ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
            logger.info(
                "ğŸ¤– è°ƒç”¨ LLM - æ¨¡å‹: %s, base_url: %s, temperature: %.2f, max_tokens: %s, timeout: %s",
                self.config.model,
                self.config.base_url,
                temperature or self.config.temperature,
                max_tokens or self.config.max_tokens or "æœªè®¾ç½®",
                self.config.timeout,
            )

            # è°ƒç”¨APIï¼ˆä½¿ç”¨ cast æ˜¾å¼ç±»å‹è½¬æ¢ï¼‰
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=cast(Iterable[ChatCompletionMessageParam], api_messages),
                temperature=temperature or self.config.temperature,
                max_tokens=max_tokens or self.config.max_tokens,  # ä»é…ç½®è¯»å–ï¼Œä¸ç¡¬ç¼–ç 
                **kwargs,
            )

            # è§£æå“åº”
            choice = response.choices[0]
            usage = response.usage

            # å¤„ç†å“åº”å†…å®¹
            content = choice.message.content
            reasoning = getattr(choice.message, "reasoning_content", None)

            logger.debug(
                "OpenAIå“åº”: content=%s, reasoning_content=%s, finish_reason=%s",
                choice.message.content,
                reasoning,
                choice.finish_reason,
            )

            return LLMResponse(
                content=content or "",
                model=response.model,
                usage=LLMUsage(
                    prompt_tokens=usage.prompt_tokens if usage else 0,
                    completion_tokens=usage.completion_tokens if usage else 0,
                    total_tokens=usage.total_tokens if usage else 0,
                ),
                finish_reason=choice.finish_reason or "stop",
            )

        except APITimeoutError as e:
            logger.error(
                "âŒ OpenAIè°ƒç”¨è¶…æ—¶ - æ¨¡å‹: %s, base_url: %s, timeout: %s, é”™è¯¯: %s",
                self.config.model,
                self.config.base_url,
                self.config.timeout,
                e,
            )
            raise LLMTimeoutError(f"OpenAIè°ƒç”¨è¶…æ—¶: {e}") from e
        except RateLimitError as e:
            logger.error(
                "âŒ OpenAIé€Ÿç‡é™åˆ¶ - æ¨¡å‹: %s, é”™è¯¯: %s",
                self.config.model,
                e,
            )
            raise LLMRateLimitError(f"OpenAIé€Ÿç‡é™åˆ¶: {e}") from e
        except (APIError, APIConnectionError) as e:
            logger.error(
                "âŒ OpenAIè°ƒç”¨å¤±è´¥ - æ¨¡å‹: %s, base_url: %s, é”™è¯¯: %s",
                self.config.model,
                self.config.base_url,
                e,
                exc_info=True,
            )
            raise LLMError(f"OpenAIè°ƒç”¨å¤±è´¥: {e}") from e
        except Exception as e:
            logger.error("æœªçŸ¥é”™è¯¯: %s", e, exc_info=True)
            raise LLMError(f"OpenAIè°ƒç”¨å¤±è´¥: {e}") from e

    async def chat_stream(
        self,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        include_reasoning: bool = False,
        **kwargs: Any,
    ) -> AsyncIterator[tuple[str, Optional[str]]]:
        """
        OpenAIæµå¼èŠå¤©è¡¥å…¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            include_reasoning: æ˜¯å¦è¿”å›æ¨ç†å†…å®¹ï¼ˆreasoning_contentï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            å…ƒç»„ (content, reasoning) - contentä¸ºå†…å®¹ç‰‡æ®µï¼Œreasoningä¸ºæ¨ç†ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰

        Raises:
            LLMError: è°ƒç”¨å¤±è´¥
        """
        try:
            # è®°å½•ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼ˆæ·»åŠ max_tokensï¼‰
            logger.info(
                "ğŸ¤– è°ƒç”¨æµå¼LLM - æ¨¡å‹: %s, base_url: %s, temperature: %.2f, max_tokens: %s, timeout: %s",
                self.config.model,
                self.config.base_url,
                temperature or self.config.temperature,
                max_tokens or self.config.max_tokens or "æœªè®¾ç½®",
                self.config.timeout,
            )

            # å‡†å¤‡æ¶ˆæ¯
            api_messages = self._prepare_messages(messages)

            # è°ƒç”¨æµå¼APIï¼ˆä½¿ç”¨ cast æ˜¾å¼ç±»å‹è½¬æ¢ï¼‰
            stream = await self.client.chat.completions.create(
                model=self.config.model,
                messages=cast(Iterable[ChatCompletionMessageParam], api_messages),
                temperature=temperature or self.config.temperature,
                max_tokens=max_tokens or self.config.max_tokens,
                stream=True,
                **kwargs,
            )

            # é€ä¸ªç”Ÿæˆå†…å®¹ç‰‡æ®µ
            async for chunk in stream:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    content = delta.content if delta.content else None
                    reasoning = None

                    # å¦‚æœéœ€è¦æ¨ç†å†…å®¹ï¼Œå°è¯•è·å–reasoning_content
                    if include_reasoning:
                        reasoning = getattr(delta, "reasoning_content", None)

                    # åªåœ¨æœ‰å†…å®¹æˆ–æ¨ç†æ—¶yield
                    if content or reasoning:
                        yield (content or "", reasoning)

        except APITimeoutError as e:
            logger.error("OpenAIæµå¼è°ƒç”¨è¶…æ—¶: %s", e)
            raise LLMTimeoutError(f"OpenAIæµå¼è°ƒç”¨è¶…æ—¶: {e}") from e
        except (APIError, APIConnectionError) as e:
            logger.error("OpenAIæµå¼è°ƒç”¨å¤±è´¥: %s", e, exc_info=True)
            raise LLMError(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {e}") from e
        except Exception as e:
            logger.error("æœªçŸ¥é”™è¯¯: %s", e, exc_info=True)
            raise LLMError(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {e}") from e


async def create_openai_client(  # pylint: disable=too-many-arguments,too-many-positional-arguments
    api_key: str,
    model: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    timeout: Optional[int] = None,
    max_retries: Optional[int] = None,
) -> OpenAIClient:
    """
    åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼ï¼‰

    Args:
        api_key: APIå¯†é’¥
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        base_url: åŸºç¡€URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    from sag.core.config.settings import get_settings
    settings = get_settings()
    
    config = ModelConfig(
        provider=LLMProvider.OPENAI,
        model=model or settings.llm_model,
        api_key=api_key,
        base_url=base_url or settings.llm_base_url,
        temperature=temperature or settings.llm_temperature,
        max_tokens=max_tokens or settings.llm_max_tokens,
        top_p=settings.llm_top_p,
        frequency_penalty=settings.llm_frequency_penalty,
        presence_penalty=settings.llm_presence_penalty,
        timeout=timeout or settings.llm_timeout,
        max_retries=max_retries or settings.llm_max_retries,
    )

    return OpenAIClient(config)
