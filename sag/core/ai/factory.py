"""
LLMå®¢æˆ·ç«¯å·¥å‚

æ ¹æ®é…ç½®åˆ›å»ºç›¸åº”çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒåœºæ™¯åŒ–é…ç½®
"""

import hashlib
import json
from typing import Any, Dict, Optional

from sag.core.ai.base import BaseLLMClient, LLMRetryClient
from sag.core.ai.models import ModelConfig, LLMProvider
from sag.core.ai.llm import OpenAIClient
from sag.core.config import get_settings
from sag.exceptions import ConfigError
from sag.utils import get_logger

logger = get_logger("ai.factory")


def _get_client_fingerprint(config: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆå®¢æˆ·ç«¯é…ç½®æŒ‡çº¹ï¼ˆé€šç”¨å‡½æ•°ï¼‰
    
    åªåŒ…å«å½±å“å®¢æˆ·ç«¯å®ä¾‹çš„æ ¸å¿ƒå‚æ•°ï¼š
    - model: æ¨¡å‹åç§°
    - api_key: APIå¯†é’¥
    - base_url: APIåœ°å€
    
    å…¶ä»–å‚æ•°ï¼ˆtemperature, dimensions, timeoutç­‰ï¼‰ä¸å½±å“å®¢æˆ·ç«¯å®ä¾‹æœ¬èº«
    
    Args:
        config: é…ç½®å­—å…¸
    
    Returns:
        é…ç½®æŒ‡çº¹ï¼ˆMD5 hashï¼‰
    """
    key_params = {
        'model': config.get('model'),
        'api_key': config.get('api_key'),
        'base_url': config.get('base_url'),
    }
    # ç”Ÿæˆé…ç½®çš„hashå€¼
    config_str = json.dumps(key_params, sort_keys=True)
    return hashlib.md5(config_str.encode()).hexdigest()


async def create_llm_client(
    scenario: str = 'general',
    model_config: Optional[Dict[str, Any]] = None,
    **kwargs: Any,
) -> BaseLLMClient | LLMRetryClient:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯ï¼ˆç»Ÿä¸€å…¥å£ï¼Œæ”¯æŒåœºæ™¯åŒ–é…ç½®ï¼‰

    é…ç½®ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
    1. model_config æ˜¾å¼ä¼ å…¥
    2. æ•°æ®åº“åœºæ™¯é…ç½® (if USE_DB_CONFIG=true)
    3. ç¯å¢ƒå˜é‡é…ç½® (å…œåº•)

    Args:
        scenario: åœºæ™¯æ ‡è¯†ï¼Œé»˜è®¤ 'general'
            - 'extract' : äº‹é¡¹æå–
            - 'search'  : æœç´¢
            - 'chat'    : å¯¹è¯
            - 'summary' : æ‘˜è¦
            - 'general' : é€šç”¨ï¼ˆé»˜è®¤ï¼‰
        
        model_config: LLMé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
            {
                'model': 'gpt-4',
                'api_key': 'sk-xxx',
                'base_url': 'https://api.302.ai',
                'temperature': 0.3,
                'max_tokens': 8000,
                ...
            }
            - å¦‚æœä¼ å…¥ï¼šç›´æ¥ä½¿ç”¨ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            - å¦‚æœä¸ä¼ ï¼šè‡ªåŠ¨ä»é…ç½®ç®¡ç†å™¨è·å–
        
        **kwargs: é›¶æ•£å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰

    Returns:
        LLMå®¢æˆ·ç«¯å®ä¾‹

    Raises:
        ConfigError: æ— æ³•è·å–æœ‰æ•ˆé…ç½®æ—¶æŠ›å‡º

    Examples:
        # æ–¹å¼1ï¼šåªä¼ åœºæ™¯ï¼Œè‡ªåŠ¨è·å–é…ç½®ï¼ˆæ¨èï¼‰
        >>> client = await create_llm_client(scenario='extract')
        
        # æ–¹å¼2ï¼šæ˜¾å¼ä¼ å…¥é…ç½®
        >>> client = await create_llm_client(
        ...     scenario='extract',
        ...     model_config={'model': 'gpt-4', 'temperature': 0.1}
        ... )
        
        # æ–¹å¼3ï¼šä½¿ç”¨é»˜è®¤é€šç”¨åœºæ™¯
        >>> client = await create_llm_client()

    è¯´æ˜ï¼š
    - ç»Ÿä¸€ä½¿ç”¨ OpenAIClientï¼ˆå…¼å®¹ OpenAI å®˜æ–¹ + 302.AI ä¸­è½¬ï¼‰
    - é€šè¿‡ base_url åŒºåˆ†ä¸åŒæœåŠ¡å•†
    """
    settings = get_settings()

    # ============ é…ç½®åˆå¹¶ï¼ˆä¸‰å±‚ä¼˜å…ˆçº§ï¼‰============
    
    # Layer 3: ç¯å¢ƒå˜é‡å…œåº•
    config = {
        'model': settings.llm_model,
        'api_key': settings.llm_api_key,
        'base_url': settings.llm_base_url,
        'temperature': settings.llm_temperature,
        'max_tokens': settings.llm_max_tokens,
        'top_p': settings.llm_top_p,
        'frequency_penalty': settings.llm_frequency_penalty,
        'presence_penalty': settings.llm_presence_penalty,
        'timeout': settings.llm_timeout,
        'max_retries': settings.llm_max_retries,
    }
    
    # Layer 2: æ•°æ®åº“é…ç½®ï¼ˆæŒ‡å®š type='llm'ï¼‰
    if settings.use_db_config:
        db_config = await _load_db_config(type='llm', scenario=scenario)
        if db_config:
            config.update(db_config)
            logger.info(f"ğŸ“Š ä½¿ç”¨æ•°æ®åº“LLMé…ç½®: scenario={scenario}, model={db_config.get('model')}")
        else:
            logger.debug(f"æ•°æ®åº“æ— LLMé…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡: scenario={scenario}")

    # Layer 1: æ˜¾å¼é…ç½®ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    if model_config:
        config.update(model_config)
        logger.info(f"ğŸ¯ ä½¿ç”¨æ˜¾å¼é…ç½®: scenario={scenario}")
    
    # å…¼å®¹é›¶æ•£å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
    if kwargs:
        config.update(kwargs)

    # ============ éªŒè¯å¿…éœ€å‚æ•° ============
    if not config.get('api_key'):
        raise ConfigError(
            f"âŒ LLMé…ç½®é”™è¯¯ï¼šç¼ºå°‘ API Keyï¼\n"
            f"åœºæ™¯: {scenario}\n"
            f"è¯·æ£€æŸ¥ï¼šæ•°æ®åº“é…ç½® æˆ– ç¯å¢ƒå˜é‡ LLM_API_KEY"
        )
    
    if not config.get('model'):
        raise ConfigError(f"âŒ LLMé…ç½®é”™è¯¯ï¼šç¼ºå°‘æ¨¡å‹åç§°ï¼åœºæ™¯: {scenario}")

    # ============ æ„å»ºé…ç½®å¯¹è±¡ ============
    model_config_obj = ModelConfig(
        provider=LLMProvider.OPENAI,  # ç»Ÿä¸€ä½¿ç”¨ OPENAIï¼ˆå…¼å®¹æ‰€æœ‰ä¸­è½¬æœåŠ¡ï¼‰
        model=config['model'],
        api_key=config['api_key'],
        base_url=config.get('base_url'),
        temperature=config['temperature'],
        max_tokens=config['max_tokens'],
        top_p=config['top_p'],
        frequency_penalty=config['frequency_penalty'],
        presence_penalty=config['presence_penalty'],
        timeout=config['timeout'],
        max_retries=config['max_retries'],
    )

    # ============ åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆç»Ÿä¸€ä½¿ç”¨OpenAIClientï¼‰============
    # OpenAIClient å…¼å®¹ï¼šOpenAI å®˜æ–¹ + 302.AI ä¸­è½¬ + å…¶ä»–å…¼å®¹æœåŠ¡
    base_client = OpenAIClient(model_config_obj)

    # åŒ…è£…é‡è¯•æœºåˆ¶
    with_retry = config.get('with_retry', True)
    if with_retry:
        logger.info(
            f"âœ… åˆ›å»ºLLMå®¢æˆ·ç«¯ï¼ˆå¸¦é‡è¯•ï¼‰: scenario={scenario}",
            extra={
                "scenario": scenario,
                "model": config['model'],
                "base_url": config.get('base_url') or 'OpenAIå®˜æ–¹',
                "max_retries": config['max_retries'],
            },
        )
        return LLMRetryClient(base_client)

    logger.info(
        f"âœ… åˆ›å»ºLLMå®¢æˆ·ç«¯: scenario={scenario}",
        extra={
            "scenario": scenario,
            "model": config['model'],
        },
    )
    return base_client


async def _load_db_config(
    type: str = 'llm',
    scenario: str = 'general'
) -> Optional[Dict[str, Any]]:
    """
    ä»æ•°æ®åº“åŠ è½½æ¨¡å‹é…ç½®ï¼ˆé€šç”¨å‡½æ•°ï¼‰
    
    é™çº§ç­–ç•¥ï¼ˆé’ˆå¯¹ LLMï¼‰ï¼š
    1. æŸ¥è¯¢ type + scenario çš„ä¸“ç”¨é…ç½®
    2. é™çº§åˆ° type + 'general'
    3. è¿”å› Noneï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡å…œåº•ï¼‰
    
    å¯¹äº Embedding/Rerank ç­‰ï¼š
    - ç›´æ¥æŸ¥ type + scenarioï¼ˆé€šå¸¸æ˜¯ generalï¼‰
    
    Args:
        type: æ¨¡å‹ç±»å‹ (llm/embedding/rerank)
        scenario: ä½¿ç”¨åœºæ™¯
        
    Returns:
        é…ç½®å­—å…¸æˆ–None
    """
    try:
        from sqlalchemy import select
        from sag.db import get_session_factory
        from sag.db.models import ModelConfig
        
        async with get_session_factory()() as session:
            # æŸ¥è¯¢æŒ‡å®šç±»å‹å’Œåœºæ™¯çš„é…ç½®
            result = await session.execute(
                select(ModelConfig)
                .where(
                    ModelConfig.type == type,
                    ModelConfig.scenario == scenario,
                    ModelConfig.is_active == True
                )
                .order_by(ModelConfig.priority.desc())
                .limit(1)
            )
            config = result.scalar_one_or_none()
            if config:
                logger.debug(f"æ‰¾åˆ°é…ç½®: type={type}, scenario={scenario}")
                return _db_model_to_dict(config)
            
            # é™çº§ç­–ç•¥ï¼šä»…å¯¹ LLM ä¸”é general åœºæ™¯ç”Ÿæ•ˆ
            if type == 'llm' and scenario != 'general':
                result = await session.execute(
                    select(ModelConfig)
                    .where(
                        ModelConfig.type == 'llm',
                        ModelConfig.scenario == 'general',
                        ModelConfig.is_active == True
                    )
                    .order_by(ModelConfig.priority.desc())
                    .limit(1)
                )
                config = result.scalar_one_or_none()
                if config:
                    logger.debug("é™çº§åˆ°é€šç”¨LLMé…ç½®")
                return _db_model_to_dict(config)
        
        return None
        
    except Exception as e:
        # æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œè¿”å› None ä½¿ç”¨ç¯å¢ƒå˜é‡å…œåº•
        logger.warning(f"æ•°æ®åº“é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None


def _db_model_to_dict(config) -> Dict[str, Any]:
    """
    æ•°æ®åº“æ¨¡å‹è½¬å­—å…¸
    
    Args:
        config: ModelConfig æ¨¡å‹å®ä¾‹
        
    Returns:
        é…ç½®å­—å…¸
    """
    result = {
        'model': config.model,
        'api_key': config.api_key,
        'base_url': config.base_url,
        'timeout': config.timeout,
        'max_retries': config.max_retries,
    }
    
    # LLM ä¸“ç”¨å‚æ•°
    if hasattr(config, 'temperature'):
        result['temperature'] = float(config.temperature)
    if hasattr(config, 'max_tokens'):
        result['max_tokens'] = config.max_tokens
    if hasattr(config, 'top_p'):
        result['top_p'] = float(config.top_p)
    if hasattr(config, 'frequency_penalty'):
        result['frequency_penalty'] = float(config.frequency_penalty)
    if hasattr(config, 'presence_penalty'):
        result['presence_penalty'] = float(config.presence_penalty)
    
    # æ‰©å±•æ•°æ®ï¼ˆå¦‚ embedding çš„ dimensionsï¼‰
    if hasattr(config, 'extra_data') and config.extra_data:
        result['extra_data'] = config.extra_data
    
    return result


# ============================================================
# è¯´æ˜ï¼š
# - LLM å®¢æˆ·ç«¯ï¼šæ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹ï¼Œå„æ¨¡å—è‡ªè¡Œç®¡ç†ï¼ˆextractor, searcher, agentç­‰ï¼‰
# - Embedding å®¢æˆ·ç«¯ï¼šå…¨å±€å•ä¾‹ï¼Œé…ç½®å˜æ›´è‡ªåŠ¨æ›¿æ¢
# ============================================================


# ============ Embedding å®¢æˆ·ç«¯å·¥å‚ ============

async def create_embedding_client(
    scenario: str = 'general',
    embedding_config: Optional[Dict[str, Any]] = None,
    **kwargs: Any,
) -> 'EmbeddingClient':
    """
    åˆ›å»ºEmbeddingå®¢æˆ·ç«¯ï¼ˆç»Ÿä¸€å…¥å£ï¼Œæ”¯æŒåˆ†å±‚é…ç½®ï¼‰

    é…ç½®ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š
    1. embedding_config æ˜¾å¼ä¼ å…¥
    2. æ•°æ®åº“é…ç½® (if USE_DB_CONFIG=true, model_type='embedding')
    3. ç¯å¢ƒå˜é‡é…ç½® (å…œåº•)

    Args:
        scenario: ä½¿ç”¨åœºæ™¯ï¼Œé»˜è®¤ 'general'ï¼ˆå½“å‰ embedding åªç”¨ generalï¼Œæœªæ¥å¯æ‰©å±•ï¼‰
        embedding_config: Embeddingé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
            {
                'model': 'Qwen/Qwen3-Embedding-0.6B',
                'api_key': 'sk-xxx',
                'base_url': 'https://api.302.ai',
                'dimensions': 1536,
                ...
            }
        **kwargs: é›¶æ•£å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰

    Returns:
        EmbeddingClientå®ä¾‹

    Raises:
        ConfigError: æ— æ³•è·å–æœ‰æ•ˆé…ç½®æ—¶æŠ›å‡º

    Examples:
        # æ–¹å¼1ï¼šè‡ªåŠ¨è·å–é…ç½®ï¼ˆæ¨èï¼‰
        >>> client = await create_embedding_client()
        
        # æ–¹å¼2ï¼šæ˜¾å¼ä¼ å…¥é…ç½®
        >>> client = await create_embedding_client(
        ...     embedding_config={'model': 'text-embedding-3-large'}
        ... )
    """
    settings = get_settings()

    # ============ é…ç½®åˆå¹¶ï¼ˆä¸‰å±‚ä¼˜å…ˆçº§ï¼‰============
    
    # Layer 3: ç¯å¢ƒå˜é‡å…œåº•
    config = {
        'model': settings.embedding_model_name,
        'api_key': settings.embedding_api_key or settings.llm_api_key,
        'base_url': settings.embedding_base_url or settings.llm_base_url,
        'dimensions': settings.embedding_dimensions,
        'timeout': 60,
        'max_retries': 3,
    }
    
    # Layer 2: æ•°æ®åº“é…ç½®ï¼ˆæŒ‡å®š type='embedding'ï¼‰
    if settings.use_db_config:
        db_config = await _load_db_config(type='embedding', scenario=scenario)
        if db_config:
            # æå– dimensionsï¼ˆå¯èƒ½åœ¨ extra_data ä¸­ï¼‰
            if 'extra_data' in db_config and db_config['extra_data']:
                if 'dimensions' in db_config['extra_data']:
                    db_config['dimensions'] = db_config['extra_data']['dimensions']
            config.update(db_config)
            logger.info(f"ğŸ“Š ä½¿ç”¨æ•°æ®åº“Embeddingé…ç½®: model={db_config.get('model')}")
        else:
            logger.debug("æ•°æ®åº“æ— Embeddingé…ç½®ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡")

    # Layer 1: æ˜¾å¼é…ç½®ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    if embedding_config:
        config.update(embedding_config)
        logger.info("ğŸ¯ ä½¿ç”¨æ˜¾å¼Embeddingé…ç½®")
    
    # å…¼å®¹é›¶æ•£å‚æ•°
    if kwargs:
        config.update(kwargs)

    # ============ éªŒè¯å¿…éœ€å‚æ•° ============
    if not config.get('api_key'):
        raise ConfigError(
            "âŒ Embeddingé…ç½®é”™è¯¯ï¼šç¼ºå°‘ API Keyï¼\n"
            f"åœºæ™¯: {scenario}\n"
            "è¯·æ£€æŸ¥ï¼šæ•°æ®åº“é…ç½® æˆ– ç¯å¢ƒå˜é‡ EMBEDDING_API_KEY/LLM_API_KEY"
        )
    
    if not config.get('model'):
        raise ConfigError(f"âŒ Embeddingé…ç½®é”™è¯¯ï¼šç¼ºå°‘æ¨¡å‹åç§°ï¼åœºæ™¯: {scenario}")

    # ============ åˆ›å»ºå®¢æˆ·ç«¯ ============
    from sag.core.ai.embedding import EmbeddingClient
    
    # âœ… æå–å‚æ•°åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆåŒ…å« api_keyï¼Œç¡®ä¿æ•°æ®åº“é…ç½®ç”Ÿæ•ˆï¼‰
    client = EmbeddingClient(
        model=config['model'],
        base_url=config.get('base_url'),
        api_key=config.get('api_key')
    )
    
    # å¦‚æœæœ‰ dimensions å‚æ•°ï¼Œéœ€è¦åœ¨ç”Ÿæˆæ—¶ä¼ é€’
    # TODO: æ›´æ–° EmbeddingClient.generate() æ”¯æŒ dimensions å‚æ•°

    logger.info(
        "âœ… åˆ›å»ºEmbeddingå®¢æˆ·ç«¯",
        extra={
            "scenario": scenario,
            "model": config['model'],
            "base_url": config.get('base_url') or 'OpenAIå®˜æ–¹',
            "dimensions": config.get('dimensions') or 'é»˜è®¤',
        },
    )
    return client


# å…¨å±€ Embedding å®¢æˆ·ç«¯å•ä¾‹ï¼ˆé…ç½®å˜æ›´æ—¶è‡ªåŠ¨æ›¿æ¢ï¼‰
_embedding_client: Optional['EmbeddingClient'] = None
_embedding_config_fingerprint: Optional[str] = None


async def get_embedding_client(scenario: str = 'general') -> 'EmbeddingClient':
    """
    è·å–Embeddingå®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼Œé…ç½®è‡ªåŠ¨æ›´æ–°ï¼‰
    
    å·¥ä½œåŸç†ï¼š
    - ç»´æŠ¤å…¨å±€å”¯ä¸€å®ä¾‹
    - æ¯æ¬¡è°ƒç”¨æ£€æµ‹é…ç½®æ˜¯å¦å˜åŒ–ï¼ˆåŸºäºæŒ‡çº¹ï¼‰
    - é…ç½®å˜åŒ–æ—¶è‡ªåŠ¨æ›¿æ¢ä¸ºæ–°å®ä¾‹
    - é…ç½®æœªå˜æ—¶å¤ç”¨ç°æœ‰å®ä¾‹
    
    æŒ‡çº¹å‚æ•°ï¼šmodel, api_key, base_urlï¼ˆé€šç”¨ä¸‰è¦ç´ ï¼‰
    
    Args:
        scenario: ä½¿ç”¨åœºæ™¯ï¼Œé»˜è®¤ 'general'
    
    Returns:
        EmbeddingClientå®ä¾‹
    """
    global _embedding_client, _embedding_config_fingerprint
    
    # 1. è·å–å®Œæ•´é…ç½®ï¼ˆåˆå¹¶ç¯å¢ƒå˜é‡ã€æ•°æ®åº“é…ç½®ç­‰ï¼‰
    settings = get_settings()
    config = {
        'model': settings.embedding_model_name,
        'api_key': settings.embedding_api_key or settings.llm_api_key,
        'base_url': settings.embedding_base_url or settings.llm_base_url,
        'dimensions': settings.embedding_dimensions,
        'timeout': 60,
        'max_retries': 3,
    }
    
    # 2. å°è¯•ä»æ•°æ®åº“åŠ è½½é…ç½®
    if settings.use_db_config:
        db_config = await _load_db_config(type='embedding', scenario=scenario)
        if db_config:
            # æå– dimensionsï¼ˆå¯èƒ½åœ¨ extra_data ä¸­ï¼‰
            if 'extra_data' in db_config and db_config['extra_data']:
                if 'dimensions' in db_config['extra_data']:
                    db_config['dimensions'] = db_config['extra_data']['dimensions']
            config.update(db_config)
            logger.debug(f"ä½¿ç”¨æ•°æ®åº“Embeddingé…ç½®: model={db_config.get('model')}")
    
    # 3. ç”Ÿæˆé…ç½®æŒ‡çº¹ï¼ˆåŸºäºå…³é”®å‚æ•°ï¼šmodel, api_key, base_urlï¼‰
    current_fingerprint = _get_client_fingerprint(config)
    
    # 4. æ£€æŸ¥é…ç½®æ˜¯å¦å˜åŒ–
    if _embedding_client is None or current_fingerprint != _embedding_config_fingerprint:
        # é…ç½®å˜åŒ–æˆ–é¦–æ¬¡åˆ›å»º
        action = 'æ›´æ–°' if _embedding_client else 'åˆ›å»º'
        
        from sag.core.ai.embedding import EmbeddingClient
        
        _embedding_client = EmbeddingClient(
            model=config['model'],
            base_url=config.get('base_url'),
            api_key=config.get('api_key')
        )
        _embedding_config_fingerprint = current_fingerprint
        
        logger.info(
            f"ğŸ”„ {action}Embeddingå®¢æˆ·ç«¯: model={config['model']}, "
            f"base_url={config.get('base_url') or 'é»˜è®¤'}, "
            f"fingerprint={current_fingerprint[:8]}..."
        )
    else:
        logger.debug(f"â™»ï¸ å¤ç”¨Embeddingå®¢æˆ·ç«¯ï¼ˆé…ç½®æœªå˜ï¼‰: {config['model']}")
    
    return _embedding_client


def reset_embedding_client() -> None:
    """é‡ç½®Embeddingå®¢æˆ·ç«¯å•ä¾‹"""
    global _embedding_client, _embedding_config_fingerprint
    _embedding_client = None
    _embedding_config_fingerprint = None
    logger.info("å·²é‡ç½®Embeddingå®¢æˆ·ç«¯")
