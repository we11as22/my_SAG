"""ç»Ÿä¸€æµç¨‹ API

æä¾› Load + Extract + Search çš„ç»Ÿä¸€è°ƒç”¨æ¥å£
"""

from typing import List, Optional

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession

from sag import SAGEngine, ExtractBaseConfig, DocumentLoadConfig, SearchBaseConfig
from sag.api.deps import get_db
from sag.api.schemas.common import SuccessResponse, TaskStatusResponse
from sag.api.schemas.pipeline import PipelineRequest, PipelineResponse
from sag.api.services.pipeline_service import PipelineService

router = APIRouter()


@router.post(
    "/pipeline/run",
    response_model=SuccessResponse[TaskStatusResponse],
    status_code=status.HTTP_202_ACCEPTED,
)
async def run_pipeline(
    request: PipelineRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
):
    """
    è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆå¼‚æ­¥ï¼‰

    **åŠŸèƒ½**ï¼š
    - ç»Ÿä¸€æ‰§è¡Œ Load â†’ Extract â†’ Search æµç¨‹
    - å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å› task_idï¼Œä¸é˜»å¡è¯·æ±‚
    - æ”¯æŒçµæ´»é…ç½®å„é˜¶æ®µå‚æ•°ï¼Œå¯é€‰æ‹©æ€§æ‰§è¡Œï¼š
      - åªæ‰§è¡Œ Loadï¼šè·å–æ–‡æ¡£æ®µè½
      - Load + Extractï¼šç”Ÿæˆäº‹é¡¹å’Œå®ä½“
      - å®Œæ•´æµç¨‹ï¼šåŒ…å«æœç´¢å’Œé‡æ’

    **è¯·æ±‚å‚æ•°**ï¼š
    ```json
    {
      "source_config_id": "source-001",
      "task_name": "å¤„ç†AIæ–‡æ¡£",
      "task_description": "å¯¹AIæŠ€æœ¯æ–‡æ¡£è¿›è¡Œåˆ†æå’Œæå–",
      "background": "è¿™æ˜¯AIæŠ€æœ¯æ–‡æ¡£é›†åˆï¼ŒåŒ…å«æ·±åº¦å­¦ä¹ ç›¸å…³å†…å®¹",

      "load": {
        "path": "./docs/ai_article.md",
        "max_tokens": 8000,
        "auto_vector": true,
        "min_content_length": 100,
        "merge_short_sections": true
      },

      "extract": {
        "max_concurrency": 10,
        "auto_vector": true,
        "custom_entity_types": [
          {
            "name": "ç®—æ³•",
            "description": "æœºå™¨å­¦ä¹ ç®—æ³•åç§°"
          }
        ]
      },

      "search": {
        "query": "æ·±åº¦å­¦ä¹ çš„æœ€æ–°è¿›å±•",
        "enable_query_rewrite": true,
        "return_type": "event",
        "recall": {
          "use_fast_mode": true,
          "vector_top_k": 15,
          "max_entities": 25,
          "max_events": 60
        },
        "expand": {
          "enabled": true,
          "max_hops": 3,
          "entities_per_hop": 10
        },
        "rerank": {
          "strategy": "rrf",
          "max_results": 10,
          "rrf_k": 60
        }
      }
    }
    ```

    **è¿”å›å€¼**ï¼š
    ```json
    {
      "success": true,
      "data": {
        "task_id": "task-uuid-xxx",
        "status": "pending",
        "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨æ‰§è¡Œä¸­..."
      },
      "message": "æµç¨‹å·²å¯åŠ¨"
    }
    ```

    **åç»­æ“ä½œ**ï¼š
    1. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼š`GET /tasks/{task_id}`
       - è¿”å›çŠ¶æ€ï¼špending, running, completed, failed
       - åŒ…å«è¿›åº¦ä¿¡æ¯å’Œæ‰§è¡Œç»“æœ

    2. åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡ï¼š`GET /tasks`
       - æ”¯æŒæŒ‰ source_config_idã€status ç­›é€‰
       - æ”¯æŒåˆ†é¡µæŸ¥è¯¢

    3. è·å–ä»»åŠ¡ç»Ÿè®¡ï¼š`GET /tasks/stats`
       - æŒ‰çŠ¶æ€å’Œç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯

    **ä½¿ç”¨åœºæ™¯**ï¼š
    - å¤§è§„æ¨¡æ–‡æ¡£å¤„ç†ï¼ˆæ¨èä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼‰
    - éœ€è¦è¿½è¸ªæ‰§è¡Œè¿›åº¦çš„é•¿æ—¶é—´ä»»åŠ¡
    - æ‰¹é‡æ–‡æ¡£çš„è‡ªåŠ¨åŒ–å¤„ç†æµç¨‹

    **æ³¨æ„äº‹é¡¹**ï¼š
    - å„é˜¶æ®µé…ç½®å‡ä¸ºå¯é€‰ï¼Œæœªé…ç½®åˆ™è·³è¿‡è¯¥é˜¶æ®µ
    - load.path å’Œ load.article_id äºŒé€‰ä¸€
    - background å¯åœ¨å…¨å±€è®¾ç½®ï¼Œä¹Ÿå¯åœ¨å„é˜¶æ®µå•ç‹¬è®¾ç½®
    - å»ºè®®å¤§è§„æ¨¡æ•°æ®å¤„ç†ä½¿ç”¨æ­¤æ¥å£ï¼Œå°è§„æ¨¡å¯ä½¿ç”¨ /pipeline/run-sync
    """
    service = PipelineService(db)

    # åˆ›å»ºä»»åŠ¡å¹¶åœ¨åå°æ‰§è¡Œ
    task_id = await service.create_task(request)
    background_tasks.add_task(service.execute_pipeline, task_id, request)

    return SuccessResponse(
        data=TaskStatusResponse(
            task_id=task_id,
            status="pending",
            message="ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨æ‰§è¡Œä¸­...",
        ),
        message="æµç¨‹å·²å¯åŠ¨",
    )


@router.post(
    "/pipeline/run-sync",
    response_model=SuccessResponse[PipelineResponse],
)
async def run_pipeline_sync(
    request: PipelineRequest,
    db: AsyncSession = Depends(get_db),
):
    """
    åŒæ­¥è¿è¡Œå®Œæ•´æµç¨‹

    **åŠŸèƒ½**ï¼š
    - åŒæ­¥æ‰§è¡Œï¼Œç­‰å¾…æ‰€æœ‰é˜¶æ®µå®Œæˆåè¿”å›ç»“æœ
    - é€‚åˆå°è§„æ¨¡æ•°æ®å¤„ç†
    - ä¸å»ºè®®ç”¨äºå¤§è§„æ¨¡æ–‡æ¡£å¤„ç†

    **æ³¨æ„**ï¼š
    - å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
    - å»ºè®®ä½¿ç”¨å¼‚æ­¥æ¥å£ /pipeline/run
    """
    service = PipelineService(db)
    result = await service.execute_pipeline_sync(request)
    return SuccessResponse(data=result, message="æµç¨‹æ‰§è¡Œå®Œæˆ")


@router.post(
    "/pipeline/load",
    response_model=SuccessResponse[dict],
)
async def run_load_only(
    source_config_id: str,
    path: str,
    background: Optional[str] = None,
    recursive: bool = True,
    pattern: str = "*.md",
    db: AsyncSession = Depends(get_db),
):
    """
    åªæ‰§è¡Œ Load é˜¶æ®µ

    **åŠŸèƒ½**ï¼š
    - åŠ è½½æ–‡æ¡£å¹¶ç”Ÿæˆæ‘˜è¦ã€åˆ‡å—
    - è¿”å› article_id ä¾›åç»­ä½¿ç”¨
    """
    # åˆ›å»ºå¼•æ“å¹¶æ‰§è¡Œ Load
    engine = SAGEngine(source_config_id=source_config_id)
    await engine.load_async(
        DocumentLoadConfig(
            source_config_id=source_config_id,
            path=path,
            recursive=recursive,
            pattern=pattern,
        )
    )

    result = engine.get_result()

    if result.is_success() and result.load_result:
        return SuccessResponse(
            data={
                "article_id": result.article_id,
                "sections": result.load_result.data_ids,
                "stats": result.load_result.stats,
            },
            message="Load é˜¶æ®µå®Œæˆ",
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=result.error or "Load å¤±è´¥",
        )


@router.post(
    "/pipeline/extract",
    response_model=SuccessResponse[dict],
)
async def run_extract_only(
    source_config_id: str,
    article_id: str,
    parallel: bool = True,
    max_concurrency: int = 10,
    db: AsyncSession = Depends(get_db),
):
    """
    åªæ‰§è¡Œ Extract é˜¶æ®µ

    **å‰æ**ï¼š
    - æ–‡æ¡£å·²é€šè¿‡ Load é˜¶æ®µå¤„ç†

    **å‚æ•°**ï¼š
    - source_config_id: ä¿¡æ¯æºID
    - article_id: æ–‡ç« ID
    """
    engine = SAGEngine(source_config_id=source_config_id)
    engine._article_id = article_id

    await engine.extract_async(
        ExtractBaseConfig(
            parallel=parallel,
            max_concurrency=max_concurrency,
        )
    )

    result = engine.get_result()

    if result.is_success() and result.extract_result:
        return SuccessResponse(
            data={
                "events": result.extract_result.data_ids,
                "stats": result.extract_result.stats,
            },
            message="Extract é˜¶æ®µå®Œæˆ",
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=result.error or "Extract å¤±è´¥",
        )


class SearchRequest(BaseModel):
    """æœç´¢è¯·æ±‚"""
    source_config_id: Optional[str] = None  # å•ä¸ªä¿¡æ¯æºIDï¼ˆå‘åå…¼å®¹ï¼‰
    source_config_ids: Optional[List[str]] = None  # å¤šä¸ªä¿¡æ¯æºIDåˆ—è¡¨
    query: str

    # === åŠŸèƒ½å¼€å…³ ===
    enable_query_rewrite: Optional[bool] = None
    use_fast_mode: Optional[bool] = None

    # === Recallå‚æ•° ===
    vector_top_k: Optional[int] = None
    vector_candidates: Optional[int] = None
    entity_similarity_threshold: Optional[float] = None
    event_similarity_threshold: Optional[float] = None
    max_entities: Optional[int] = None
    max_events: Optional[int] = None
    entity_weight_threshold: Optional[float] = None
    final_entity_count: Optional[int] = None

    # === Expandå‚æ•° ===
    expand_enabled: Optional[bool] = None
    max_hops: Optional[int] = None
    entities_per_hop: Optional[int] = None
    weight_change_threshold: Optional[float] = None
    expand_event_similarity_threshold: Optional[float] = None
    min_events_per_hop: Optional[int] = None
    max_events_per_hop: Optional[int] = None

    # === Rerankå‚æ•° ===
    use_pagerank: Optional[bool] = None  # æ˜¯å¦ä½¿ç”¨PageRankï¼ˆå¦åˆ™ä½¿ç”¨RRFï¼‰
    strategy: Optional[str] = None  # "pagerank" or "rrf"ï¼ˆä¼˜å…ˆçº§é«˜äºuse_pagerankï¼‰
    score_threshold: Optional[float] = None
    max_results: Optional[int] = None
    max_key_recall_results: Optional[int] = None  # Step1 Keyå¬å›çš„æœ€å¤§äº‹é¡¹/æ®µè½æ•°
    max_query_recall_results: Optional[int] = None  # Step2 Queryå¬å›çš„æœ€å¤§äº‹é¡¹/æ®µè½æ•°
    pagerank_section_top_k: Optional[int] = None
    pagerank_damping_factor: Optional[float] = None
    pagerank_max_iterations: Optional[int] = None
    rrf_k: Optional[int] = None


@router.post(
    "/pipeline/search",
    response_model=SuccessResponse[dict],
)
async def run_search_only(
    request: SearchRequest,
    db: AsyncSession = Depends(get_db),
):
    """
    åªæ‰§è¡Œ Search é˜¶æ®µ

    **å‰æ**ï¼š
    - å·²æœ‰äº‹é¡¹æ•°æ®ï¼ˆé€šè¿‡ Load + Extract ç”Ÿæˆï¼‰

    **å‚æ•°**ï¼š
    - source_config_id / source_config_ids: å•ä¸ªæˆ–å¤šä¸ªä¿¡æ¯æºID
    - query: æŸ¥è¯¢æ–‡æœ¬
    - ä»¥åŠ Recall/Expand/Rerank çš„é…ç½®å‚æ•°
    """
    # éªŒè¯ï¼šè‡³å°‘æä¾›ä¸€ä¸ª source_config_id æˆ– source_config_ids
    if not request.source_config_id and not request.source_config_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="å¿…é¡»æä¾› source_config_id æˆ– source_config_ids å‚æ•°"
        )

    # å…¼å®¹å¤„ç†ï¼šç»Ÿä¸€è½¬ä¸º source_config_ids
    source_config_ids = request.source_config_ids if request.source_config_ids else [
        request.source_config_id]
    primary_source_config_id = source_config_ids[0]  # ç”¨äºåˆ›å»ºå¼•æ“

    engine = SAGEngine(source_config_id=primary_source_config_id)

    # æ„å»ºæ–°çš„é…ç½®ç»“æ„
    from sag.modules.search.config import RecallConfig, ExpandConfig, RerankConfig, RerankStrategy, SearchConfig

    # Recallé…ç½®
    recall_dict = {}
    recall_mapping = {
        "use_fast_mode": "use_fast_mode",
        "vector_top_k": "vector_top_k",
        "vector_candidates": "vector_candidates",
        "entity_similarity_threshold": "entity_similarity_threshold",
        "event_similarity_threshold": "event_similarity_threshold",
        "max_entities": "max_entities",
        "max_events": "max_events",
        "entity_weight_threshold": "entity_weight_threshold",
        "final_entity_count": "final_entity_count",
    }
    for req_param, config_param in recall_mapping.items():
        value = getattr(request, req_param, None)
        if value is not None:
            recall_dict[config_param] = value

    # Expandé…ç½®
    expand_dict = {}
    expand_mapping = {
        "expand_enabled": "enabled",
        "max_hops": "max_hops",
        "entities_per_hop": "entities_per_hop",
        "weight_change_threshold": "weight_change_threshold",
        "expand_event_similarity_threshold": "event_similarity_threshold",
        "min_events_per_hop": "min_events_per_hop",
        "max_events_per_hop": "max_events_per_hop",
    }
    for req_param, config_param in expand_mapping.items():
        value = getattr(request, req_param, None)
        if value is not None:
            expand_dict[config_param] = value

    # Reranké…ç½®
    rerank_dict = {}

    # å¤„ç† use_pagerank åˆ° strategy çš„è½¬æ¢
    # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å®š strategyï¼Œåˆ™æ ¹æ® use_pagerank æ¥å†³å®š
    if request.strategy is None and request.use_pagerank is not None:
        request.strategy = "pagerank" if request.use_pagerank else "rrf"

    rerank_mapping = {
        "strategy": "strategy",
        "score_threshold": "score_threshold",
        "max_results": "max_results",
        "max_key_recall_results": "max_key_recall_results",
        "max_query_recall_results": "max_query_recall_results",
        "pagerank_section_top_k": "pagerank_section_top_k",
        "pagerank_damping_factor": "pagerank_damping_factor",
        "pagerank_max_iterations": "pagerank_max_iterations",
        "rrf_k": "rrf_k",
    }
    for req_param, config_param in rerank_mapping.items():
        value = getattr(request, req_param, None)
        if value is not None:
            # strategyéœ€è¦è½¬æ¢ä¸ºæšä¸¾
            if config_param == "strategy" and value:
                rerank_dict[config_param] = RerankStrategy(value)
            else:
                rerank_dict[config_param] = value

    # ç›´æ¥æ„å»ºå®Œæ•´çš„ SearchConfigï¼ˆåŒ…å« source_config_idsï¼‰
    search_config = SearchConfig(
        query=request.query,
        source_config_ids=source_config_ids,  # ä¼ é€’å¤šæºæ”¯æŒ
        enable_query_rewrite=request.enable_query_rewrite if request.enable_query_rewrite is not None else True,
        recall=RecallConfig(**recall_dict) if recall_dict else RecallConfig(),
        expand=ExpandConfig(**expand_dict) if expand_dict else ExpandConfig(),
        rerank=RerankConfig(**rerank_dict) if rerank_dict else RerankConfig(),
    )

    # ä½¿ç”¨ SearchBaseConfigï¼Œengine.search_async() ä¼šè‡ªåŠ¨æ·»åŠ  source_config_id è½¬æ¢ä¸º SearchConfig
    await engine.search_async(search_config)

    result = engine.get_result()

    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” æœç´¢ç»“æœè°ƒè¯•ä¿¡æ¯:")
    print(f"  - ä»»åŠ¡æ•´ä½“çŠ¶æ€: {result.status}")
    print(
        f"  - æœç´¢é˜¶æ®µçŠ¶æ€: {result.search_result.status if result.search_result else None}")

    # åªè¦æœç´¢é˜¶æ®µæˆåŠŸå°±è¿”å›ç»“æœï¼Œä¸ç®¡æ•´ä½“ä»»åŠ¡çŠ¶æ€
    if result.search_result and result.search_result.status == "success":
        try:
            print(f"  - data_full é•¿åº¦: {len(result.search_result.data_full)}")

            # æ‰©å±•äº‹é¡¹å†…å®¹ï¼ˆè¡¥å……å®ä½“å’ŒåŸæ–‡å¼•ç”¨ï¼‰
            from sag.modules.search.enricher import EventEnricher
            enricher = EventEnricher(db)
            events = await enricher.enrich_events(result.search_result.data_full)

            # ä»statsä¸­æå–cluesï¼ˆnow from-toæ ¼å¼çš„è¯¦ç»†çº¿ç´¢åˆ—è¡¨ï¼‰
            clues = result.search_result.stats.get(
                "clues", []) if result.search_result.stats else []

            print(f"âœ… æˆåŠŸå¤„ç† {len(events)} ä¸ªäº‹é¡¹ï¼ˆå·²æ‰©å±•å®ä½“å’Œå¼•ç”¨ï¼‰")
            print(f"ğŸ“‹ Cluesä¿¡æ¯: æ€»å…± {len(clues)} æ¡çº¿ç´¢"
                  f" (recall={len([c for c in clues if c.get('stage') == 'recall'])}, "
                  f"expand={len([c for c in clues if c.get('stage') == 'expand'])}, "
                  f"rerank={len([c for c in clues if c.get('stage') == 'rerank'])})")

            return SuccessResponse(
                data={
                    "events": events,
                    "clues": clues,  # è¿”å› from-to æ ¼å¼çš„è¯¦ç»†çº¿ç´¢
                    "stats": {
                        "matched_count": result.search_result.stats.get("matched_count", len(events))
                    },
                },
                message="Search å®Œæˆ",
            )
        except Exception as e:
            print(f"âŒ æ•°æ®å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}",
            )
    else:
        error_msg = result.search_result.error if result.search_result else "Search å¤±è´¥"
        print(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=error_msg,
        )


class SummarizeRequest(BaseModel):
    """AIæ€»ç»“è¯·æ±‚"""
    source_config_id: str
    query: str
    event_ids: List[str]


@router.post("/pipeline/summarize")
async def summarize_search_results(
    request: SummarizeRequest,
    db: AsyncSession = Depends(get_db),
):
    """
    AIæ™ºèƒ½æ€»ç»“æœç´¢ç»“æœï¼ˆæµå¼è¾“å‡ºï¼‰

    æ ¹æ®æœç´¢ç»“æœçš„äº‹é¡¹IDï¼ŒæŸ¥è¯¢å®Œæ•´æ•°æ®å¹¶ä½¿ç”¨AIè¿›è¡Œæ€»ç»“

    Args:
        source_config_id: ä¿¡æ¯æºID
        query: ç”¨æˆ·æŸ¥è¯¢
        event_ids: äº‹é¡¹IDåˆ—è¡¨

    Returns:
        Server-Sent Events æµå¼å“åº”
    """
    from sag.core.agent import SummarizerAgent
    from sag.db.models import SourceEvent
    from sqlalchemy import select

    try:
        # 1. ä»æ•°æ®åº“æŸ¥è¯¢äº‹é¡¹å®Œæ•´æ•°æ®
        events_data = []

        for event_id in request.event_ids:
            try:
                # æŸ¥è¯¢äº‹é¡¹
                result = await db.execute(
                    select(SourceEvent).where(SourceEvent.id == event_id)
                )
                event = result.scalar_one_or_none()

                if not event:
                    print(f"âš ï¸  äº‹é¡¹ä¸å­˜åœ¨: {event_id}")
                    continue

                # æ„å»ºäº‹é¡¹æ•°æ®ï¼ˆreferences å·²ç»åœ¨ event.references JSONå­—æ®µä¸­ï¼‰
                event_data = {
                    "id": event.id,
                    "title": event.title,
                    "summary": event.summary,
                    "content": event.content,
                    "references": event.references or [],  # references æ˜¯ JSON å­—æ®µ
                }
                events_data.append(event_data)

            except Exception as e:
                print(f"âš ï¸  æŸ¥è¯¢äº‹é¡¹å¤±è´¥ {event_id}: {e}")
                import traceback
                traceback.print_exc()
                continue

        if not events_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="æœªæ‰¾åˆ°æœ‰æ•ˆçš„äº‹é¡¹æ•°æ®"
            )

        print(f"âœ… æˆåŠŸåŠ è½½ {len(events_data)} æ¡äº‹é¡¹ç”¨äºæ€»ç»“")

        # 2. åˆ›å»º SummarizerAgent å¹¶æ‰§è¡Œ
        agent = SummarizerAgent(events=events_data)

        # 3. æµå¼ç”Ÿæˆå“åº”
        async def generate():
            try:
                async for chunk in agent.run(request.query):
                    # SSE æ ¼å¼
                    if chunk.get("reasoning"):
                        yield f"event: thinking\ndata: {chunk['reasoning']}\n\n"
                    if chunk.get("content"):
                        yield f"event: content\ndata: {chunk['content']}\n\n"

            except Exception as e:
                print(f"âŒ æ€»ç»“å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                yield f"event: error\ndata: {str(e)}\n\n"
            finally:
                yield "event: done\ndata: \n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ€»ç»“è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ€»ç»“è¯·æ±‚å¤±è´¥: {str(e)}"
        )
